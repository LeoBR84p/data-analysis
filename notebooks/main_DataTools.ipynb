{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2vVr44uPpfD"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"etapas-78\"></a>Etapas 7/8 ‚Äî Tipagem, detec√ß√£o e perfil\n",
    "\n",
    "1. Uso de retorno de fun√ß√£o inconsistente (detect_numeric)\n",
    "üîé Buscar por: as_num = detect_numeric(s)\n",
    "Linhas aprox.: 1‚Äì6 (da c√©lula de infer√™ncia antiga)\n",
    "Link direto: (colar link da c√©lula aqui)\n",
    "\n",
    "\n",
    "2. Linha truncada em semantic_type\n",
    "üîé Buscar por: url_ratio = (vals.str.match(URL_RX)).\n",
    "Linhas aprox.: 38‚Äì44\n",
    "Link direto: (colar link da c√©lula aqui)\n",
    "\n",
    "\n",
    "3. Fun√ß√µes redefinidas em c√©lulas diferentes (detect_datetime, semantic_type)\n",
    "üîé Buscar por: def detect_datetime(\n",
    "Linhas aprox. (vers√£o √≠ntegra): 19‚Äì33\n",
    "üîé Buscar por: def semantic_type(\n",
    "Linhas aprox. (variante): 12‚Äì20 (+ sequ√™ncia)\n",
    "Links diretos: (colar link da(s) c√©lula(s) aqui)\n",
    "\n",
    "\n",
    "4. Hotspots de mem√≥ria/performance\n",
    "üîé Buscar por: pd.read_csv( e dtype=str\n",
    "Linhas aprox. (leitura): 47‚Äì49\n",
    "üîé Buscar por: .value_counts()\n",
    "Linhas aprox. (profiling): 34‚Äì38\n",
    "Links diretos: (colar link da(s) c√©lula(s) aqui)\n",
    "\n",
    "\n",
    "5. Warning do to_datetime\n",
    "üîé Buscar por: SettingWithCopyWarning ou to_datetime\n",
    "Linhas aprox. (warning): 1‚Äì4\n",
    "Link direto: (colar link da c√©lula aqui)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"etapa-9\"></a>Etapa 9 ‚Äî Gera√ß√£o de relat√≥rios (TXT/HTML/PNGs/PDF)\n",
    "\n",
    "1. Blocos de gera√ß√£o de relat√≥rios duplicados\n",
    "üîé Buscar por: gera√ß√£o de relat√≥rios: TXT, HTML, PNGs e PDF\n",
    "Linhas aprox.: 2‚Äì7 e 41‚Äì50\n",
    "Links diretos: (colar links das c√©lulas aqui)\n",
    "\n",
    "\n",
    "2. Resumo final duplicado\n",
    "üîé Buscar por: Relat√≥rios gerados em\n",
    "Linhas aprox.: 22‚Äì26 e 3‚Äì7\n",
    "Links diretos: (colar links das c√©lulas aqui)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwAjdS960Q5p"
   },
   "source": [
    "#**Licen√ßa de Uso**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4U4kA0A0UMm"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "‚Üí You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution‚ÄìNonCommercial 4.0 International License.\n",
    "\n",
    "‚Üí You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**¬© 2025 Leandro Bernardo Rodrigues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCfZ6GgD0Zhw"
   },
   "source": [
    "#**Pr√©-Configura√ß√£o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKGww4xM0alr"
   },
   "source": [
    "##**C√≥digo de uso √∫nico**\n",
    "Aplica√ß√£o persistente entre sess√µes do Google Colab. **Uso espec√≠fico para Google Colab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "764r8M-o0hf7"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#re-clone limpo, configura√ß√µes git/nbdime/jupytext, pull/rebase e push com fallback para PAT\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import os, sys, subprocess, getpass, shutil, pathlib, time\n",
    "\n",
    "#par√¢metros do projeto e remoto\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"data-analysis\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "def run(cmd, cwd=None, check=True, capture=False):\n",
    "    #wrapper para subprocess.run com echo do diret√≥rio\n",
    "    print(f\"\\n$ (pwd={cwd or os.getcwd()})\", \" \".join(cmd))\n",
    "    return subprocess.run(\n",
    "        cmd,\n",
    "        cwd=cwd,\n",
    "        check=check,\n",
    "        text=True,\n",
    "        capture_output=capture\n",
    "    )\n",
    "\n",
    "#garantir que estamos em BASE\n",
    "%cd \"$BASE\"\n",
    "print(\"pwd(BASE):\", os.getcwd())\n",
    "\n",
    "#entrar no reposit√≥rio\n",
    "%cd \"$PROJ\"\n",
    "print(\"pwd(PROJ):\", os.getcwd())\n",
    "\n",
    "#configurar git no reposit√≥rio (escopo local)\n",
    "run([\"git\", \"config\", \"--local\", \"user.name\", \"Leandro Bernardo Rodrigues\"], cwd=PROJ)\n",
    "run([\"git\", \"config\", \"--local\", \"user.email\", \"bernardo.leandro@gmail.com\"], cwd=PROJ)\n",
    "run([\"git\", \"config\", \"--local\", \"init.defaultBranch\", \"main\"], cwd=PROJ)\n",
    "\n",
    "#criar pastas utilit√°rias (idempotente)\n",
    "for p in [\"notebooks\", \"src\", \"data\", \"output\", \"runs\", \"configs\"]:\n",
    "    os.makedirs(os.path.join(PROJ, p), exist_ok=True)\n",
    "\n",
    "#instalar ferramentas √∫teis nesta sess√£o\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "\n",
    "#habilitar nbdime no git global (no colab --local costuma falhar)\n",
    "run([\"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#criar .gitignore e .gitattributes se n√£o existirem\n",
    "gi = pathlib.Path(PROJ) / \".gitignore\"\n",
    "ga = pathlib.Path(PROJ) / \".gitattributes\"\n",
    "if not gi.exists():\n",
    "    gi.write_text(\"\"\"\\\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "*.log\n",
    "*.tmp\n",
    "# dados/artefatos pesados (n√£o versionar)\n",
    "data/\n",
    "output/\n",
    "runs/\n",
    "# python\n",
    "venv/\n",
    ".venv/\n",
    "__pycache__/\n",
    "*.pyc\n",
    "# segredos\n",
    ".env\n",
    "*.key\n",
    "*.pem\n",
    "*.tok\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "if not ga.exists():\n",
    "    ga.write_text(\"*.ipynb filter=nbstripout\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#ativar hook do nbstripout neste reposit√≥rio\n",
    "run([\"nbstripout\", \"--install\", \"--attributes\", \".gitattributes\"], cwd=PROJ)\n",
    "\n",
    "#parear notebooks com .py (se existirem)\n",
    "notebooks_glob = os.path.join(PROJ, \"notebooks\", \"*.ipynb\")\n",
    "run([\"bash\", \"-lc\", f\"jupytext --set-formats ipynb,py:percent --sync {notebooks_glob} || true\"], cwd=PROJ)\n",
    "\n",
    "#push com fallback para PAT\n",
    "print(\"\\nTentando push sem credenciais‚Ä¶\")\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], cwd=PROJ, capture_output=True, text=True)\n",
    "if push.returncode == 0:\n",
    "    print(\"push conclu√≠do sem PAT.\")\n",
    "else:\n",
    "    print(\"primeiro push falhou. usando PAT‚Ä¶\")\n",
    "    token = getpass.getpass(\"cole seu GitHub PAT (n√£o ser√° exibido): \").strip()\n",
    "    username = GITHUB_OWNER\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "    #testar credenciais\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], cwd=PROJ, capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"falha ao autenticar com o PAT:\")\n",
    "        print(test.stderr or test.stdout)\n",
    "        raise SystemExit(1)\n",
    "    #trocar remote para URL autenticada, push e restaurar\n",
    "    try:\n",
    "        run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], cwd=PROJ)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], cwd=PROJ, capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"falha no push mesmo com PAT:\\n\", out.stderr or out.stdout)\n",
    "            raise SystemExit(out.returncode)\n",
    "        print(\"push conclu√≠do com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", CLEAN_URL], cwd=PROJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K7JEpmVLXlE"
   },
   "source": [
    "#**Utilit√°rio:** verifica√ß√£o da formata√ß√£o de c√≥digo\n",
    "\n",
    "Black [88] + Isort, desconsiderando c√©lulas m√°gicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2QkokQSCT2IC",
    "outputId": "27e93e3b-d648-41a0-f638-facd52486c1e"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#pr√©-visualizar/aplicar (pula magics) ‚Äî isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/main_DataTools.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular c√©lulas com magics/shell\n",
    "#   - linhas come√ßando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabe√ßalhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook n√£o encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) Pr√©-visualiza√ß√£o c√©lula a c√©lula\n",
    "header(\"Pr√©-visualiza√ß√£o (N√ÉO grava) ‚Äî somente c√©lulas com mudan√ßas\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular c√©lulas com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] c√©lula {i}: erro no formatador ‚Äî pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado leg√≠vel\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<c√©lula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) Exibi√ß√£o dos diffs por c√©lula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"‚úî Nada a alterar: todas as c√©lulas (n√£o m√°gicas) j√° est√£o conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff ‚Äî C√©lula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da c√©lula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"C√©lulas com mudan√ßas: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudan√ßas nessas c√©lulas, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas c√©lulas com mudan√ßas\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"Conclu√≠do\")\n",
    "        print(f\"‚úî Mudan√ßas aplicadas em {len(changed_cells)} c√©lula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formata√ß√£o atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOpera√ß√£o cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Kt7IFa13wL"
   },
   "source": [
    "#**Sincronizar altera√ß√µes no c√≥digo do projeto**\n",
    "Comandos para sincronizar c√≥digo (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive √© considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 38223,
     "status": "error",
     "timestamp": 1759805790382,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "93LRJc0Wqk3X",
    "outputId": "b244be14-92db-4bb1-9655-2e16576054ec"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#push colab para github com nbstripout e considerando o google drive como fonte da verdade\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, getpass, sys\n",
    "from urllib.parse import quote as urlquote\n",
    "\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"\n",
    "repo_name      = \"data-analysis\"\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/data-analysis\")\n",
    "remote_url     = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "# modo do nbstripout\n",
    "# use \"install\" para limpar outputs de notebooks\n",
    "# use \"disable\" se quiser versionar os outputs\n",
    "nbstripout_mode = \"install\"\n",
    "\n",
    "def sh(cmd, cwd=None, check=True, inp=None, quiet=False):\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True, input=inp)\n",
    "    if not quiet:\n",
    "        if r.stdout.strip(): print(r.stdout.strip())\n",
    "        if r.stderr.strip(): print(r.stderr.strip())\n",
    "    if check and r.returncode != 0:\n",
    "        raise RuntimeError(f\"falha {' '.join(cmd)} rc={r.returncode}\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "def git(*args, **kw):\n",
    "    return sh([\"git\", *args], **kw)\n",
    "\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        if not Path(\"/content/drive/MyDrive\").exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def clean_or_init_repo():\n",
    "    # se n√£o existir repo git local, inicializa\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not (repo_dir/\".git\").exists():\n",
    "        git(\"init\", cwd=repo_dir, quiet=True)\n",
    "    # se houver tra√ßos de rebase ou merge pendente, recria o .git\n",
    "    trouble_markers = [\"rebase-apply\",\"rebase-merge\",\"MERGE_HEAD\",\"CHERRY_PICK_HEAD\"]\n",
    "    if any((repo_dir/\".git\"/p).exists() for p in trouble_markers):\n",
    "        shutil.rmtree(repo_dir/\".git\", ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir, quiet=True)\n",
    "    # define e for√ßa a branch main\n",
    "    try:\n",
    "        sh([\"git\",\"switch\",\"-C\", default_branch], cwd=repo_dir, quiet=True)\n",
    "    except Exception:\n",
    "        sh([\"git\",\"checkout\",\"-B\", default_branch], cwd=repo_dir, quiet=True)\n",
    "    # configura usu√°rio local\n",
    "    try:\n",
    "        git(\"config\",\"user.name\", author_name, cwd=repo_dir, quiet=True)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "def ensure_origin():\n",
    "    rem = git(\"remote\",\"-v\", cwd=repo_dir, quiet=True)\n",
    "    if \"origin\" not in rem:\n",
    "        git(\"remote\",\"add\",\"origin\", remote_url, cwd=repo_dir, quiet=True)\n",
    "    else:\n",
    "        git(\"remote\",\"set-url\",\"origin\", remote_url, cwd=repo_dir, quiet=True)\n",
    "\n",
    "def setup_nbstripout():\n",
    "    # instala ou desativa nbstripout\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False, quiet=True)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False, quiet=True)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False, quiet=True)\n",
    "        gat = repo_dir/\".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines)+\"\\n\", encoding=\"utf-8\")\n",
    "        print(\"nbstripout desativado localmente\")\n",
    "        return\n",
    "    # modo install\n",
    "    try:\n",
    "        import nbstripout  # noqa\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"], cwd=repo_dir, check=True, quiet=True)\n",
    "        import nbstripout  # noqa\n",
    "    py = sys.executable\n",
    "    git(\"config\",\"--local\",\"filter.nbstripout.clean\", f\"\\\"{py}\\\" -m nbstripout\", cwd=repo_dir, quiet=True)\n",
    "    git(\"config\",\"--local\",\"filter.nbstripout.smudge\",\"cat\", cwd=repo_dir, quiet=True)\n",
    "    git(\"config\",\"--local\",\"filter.nbstripout.required\",\"true\", cwd=repo_dir, quiet=True)\n",
    "    gat = repo_dir/\".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip()+\"\\n\"+line+\"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line+\"\\n\", encoding=\"utf-8\")\n",
    "    print(f\"nbstripout instalado e configurado usando {py}\")\n",
    "\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\",\"--list\", cwd=repo_dir, quiet=True).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "    vf = repo_dir/\"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v): return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1,0,0)\n",
    "    kind = (kind or \"\").strip()\n",
    "    if kind == \"m\": return f\"{M}.{m+1}.0\"\n",
    "    if kind == \"n\": return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"\n",
    "\n",
    "def stage_all():\n",
    "    sh([\"git\",\"add\",\"-A\"], cwd=repo_dir, check=False, quiet=True)\n",
    "\n",
    "def commit_and_tag():\n",
    "    # Check for changes before attempting to commit\n",
    "    status = git(\"status\", \"--porcelain\", cwd=repo_dir, quiet=True)\n",
    "    if not status.strip():\n",
    "        print(\"Nenhuma altera√ß√£o para commit. Pulando commit e tag.\")\n",
    "        return current_version(), \"no changes\" # Return current version and a message\n",
    "\n",
    "    msg  = input(\"Informe o motivo do commit e pressione Enter.\").strip() or \"n√£o informado\" #preenchimento vazio considera \"n√£o informado\".\n",
    "    kind = input(\"Informe o tipo de mudan√ßa e pressione Enter. Maior (M); menor (m) ou pontual (n).\").strip() #preenchimento vazio considera tipo pontual (n).\n",
    "    if kind not in (\"M\",\"m\",\"n\"): kind = \"n\"\n",
    "    cur = current_version()\n",
    "    new = bump(cur, kind)\n",
    "    (repo_dir/\"VERSION\").write_text(new+\"\\n\", encoding=\"utf-8\")\n",
    "    stage_all()\n",
    "    try:\n",
    "        git(\"commit\",\"-m\", msg, cwd=repo_dir, quiet=True)\n",
    "    except RuntimeError:\n",
    "        status = git(\"status\",\"--porcelain\", cwd=repo_dir, quiet=True)\n",
    "        if status.strip():\n",
    "            raise\n",
    "        else:\n",
    "            print(\"Nenhuma altera√ß√£o para commit seguirei com a mesma tag e vers√£o.\")\n",
    "    try:\n",
    "        git(\"tag\",\"-a\", new, \"-m\", f\"release {new} ‚Äî {msg}\", cwd=repo_dir, quiet=True)\n",
    "    except RuntimeError:\n",
    "        sh([\"git\",\"tag\",\"-d\", new], cwd=repo_dir, check=False, quiet=True)\n",
    "        git(\"tag\",\"-a\", new, \"-m\", f\"release {new} ‚Äî {msg}\", cwd=repo_dir, quiet=True)\n",
    "    return new, msg\n",
    "\n",
    "def push_with_pat():\n",
    "    user  = input(\"GitHub username. Ex.: LeoBR84p: \").strip() or \"LeoBR84p\"\n",
    "    token = getpass.getpass(\"Cole seu github PAT:\").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT n√£o informado gere um token no github e tente novamente.\")\n",
    "    safe_user  = urlquote(user, safe=\"\")\n",
    "    safe_token = urlquote(token, safe=\"\")\n",
    "    auth_url = f\"https://{safe_user}:{safe_token}@github.com/{owner}/{repo_name}.git\"\n",
    "    try:\n",
    "        need_upstream = False\n",
    "        try:\n",
    "            git(\"rev-parse\",\"--abbrev-ref\",\"--symbolic-full-name\",\"@{u}\", cwd=repo_dir, quiet=True)\n",
    "        except RuntimeError:\n",
    "            need_upstream = True\n",
    "        if need_upstream:\n",
    "            sh([\"git\",\"push\",\"-u\", auth_url, default_branch], cwd=repo_dir)\n",
    "        else:\n",
    "            sh([\"git\",\"push\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\",\"push\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "        print(\"Push conclu√≠do sem force.\")\n",
    "    except RuntimeError:\n",
    "        print(\"Remoto rejeitou. Aplicando force. Google Drive assumido como ponto de verdade.\")\n",
    "        sh([\"git\",\"push\",\"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\",\"push\",\"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "        print(\"Force push conclu√≠do.\")\n",
    "\n",
    "# execu√ß√£o\n",
    "ensure_drive()\n",
    "clean_or_init_repo()\n",
    "ensure_origin()\n",
    "setup_nbstripout()\n",
    "newv, msg = commit_and_tag()\n",
    "# Only push if commit_and_tag returned a new version (i.e., there were changes)\n",
    "if msg != \"no changes\":\n",
    "    push_with_pat()\n",
    "    print(f\"Sincronizado com a vers√£o {newv} mensagem {msg}\")\n",
    "else:\n",
    "    print(\"Nenhuma altera√ß√£o para sincronizar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GBn0mUo2DJE"
   },
   "source": [
    "#**Checklist r√°pido de execu√ß√£o**\n",
    "**Etapas:**\n",
    "- 01‚Äì04: setup (ambiente, depend√™ncias, diret√≥rios e configs)\n",
    "- 05‚Äì08: execu√ß√£o (ingest√£o dos dados, an√°lise de cabe√ßalhos, an√°lise preliminar dos dados e an√°lise de tipologias)\n",
    "- 09: gera√ß√£o de output (salva an√°lise, gera gr√°ficos gerais, gera gr√°ficos espec√≠ficos e relat√≥rios em HTML+PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSIy8Nz62HK9"
   },
   "source": [
    "#**Ferramentas de Data Analysis**\n",
    "\n",
    "Ferramentas de **identifica√ß√£o de tipo de dado e estrutura da informa√ß√£o** presente em *datasets* a partir da ingest√£o de arquivos CSV UTF-8 com BOM em padr√£o separado por ponto e v√≠rgula.\n",
    "_____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X09lmr752IUE"
   },
   "source": [
    "### **Etapa 1:** Ativa√ß√£o do ambiente virtual\n",
    "---\n",
    "Monta o Google Drive, define a BASE e REPO do projeto Git, cria/ativa o ambiente virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "executionInfo": {
     "elapsed": 57544,
     "status": "ok",
     "timestamp": 1759803707353,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "cIIf3jAkWoeY",
    "outputId": "7835130e-abba-42cf-f1ed-121bdc7c6bfb"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#nicializa√ß√£o robusta: Drive + venv fora do Drive + Git checks (com patch de venv/ensurepip) { display-mode: \"form\" }\n",
    "#for√ßa clear do kernel/vari√°veis desta sess√£o\n",
    "%reset -f\n",
    "\n",
    "#imports b√°sicos -----\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML\n",
    "import os, sys, time, shutil, pathlib, subprocess\n",
    "\n",
    "#helper de subprocess -----\n",
    "def run(cmd, check=True, cwd=None):\n",
    "    r = subprocess.run(cmd, text=True, capture_output=True, cwd=cwd)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout + r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "#fun√ß√µes utilit√°rias de Drive/FS -----\n",
    "def _is_mount_active(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"verifica em /proc/mounts se o mountpoint est√° realmente montado\"\"\"\n",
    "    try:\n",
    "        with open(\"/proc/mounts\", \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if len(parts) > 1 and parts[1] == mountpoint:\n",
    "                    return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def _cleanup_local_mountpoint(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"limpa conte√∫do local do mountpoint quando N√ÉO est√° montado\"\"\"\n",
    "    if os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
    "        print(f\"[info] mountpoint '{mountpoint}' cont√©m arquivos locais. limpando...\")\n",
    "        for name in os.listdir(mountpoint):\n",
    "            p = os.path.join(mountpoint, name)\n",
    "            try:\n",
    "                if os.path.isfile(p) or os.path.islink(p):\n",
    "                    os.remove(p)\n",
    "                else:\n",
    "                    shutil.rmtree(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[aviso] n√£o foi poss√≠vel remover {p}: {e}\")\n",
    "        print(\"[ok] limpeza conclu√≠da.\")\n",
    "\n",
    "def safe_mount_google_drive(preferred_mountpoint: str = \"/content/drive\", readonly: bool = False, timeout_ms: int = 120000):\n",
    "    \"\"\"desmonta se preciso, limpa o mountpoint local e monta o drive\"\"\"\n",
    "    try:\n",
    "        if _is_mount_active(preferred_mountpoint):\n",
    "            print(\"[info] drive montado. tentando desmontar...\")\n",
    "        drive.flush_and_unmount()\n",
    "        for _ in range(50):\n",
    "            if not _is_mount_active(preferred_mountpoint):\n",
    "                break\n",
    "            time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not _is_mount_active(preferred_mountpoint):\n",
    "        _cleanup_local_mountpoint(preferred_mountpoint)\n",
    "\n",
    "    os.makedirs(preferred_mountpoint, exist_ok=True)\n",
    "    if os.listdir(preferred_mountpoint):\n",
    "        alt = \"/mnt/drive\"\n",
    "        print(f\"[aviso] '{preferred_mountpoint}' ainda n√£o est√° vazio. usando alternativo '{alt}'.\")\n",
    "        os.makedirs(alt, exist_ok=True)\n",
    "        mountpoint = alt\n",
    "    else:\n",
    "        mountpoint = preferred_mountpoint\n",
    "\n",
    "    print(f\"[info] montando o google drive em '{mountpoint}'...\")\n",
    "    drive.mount(mountpoint, force_remount=True, timeout_ms=timeout_ms, readonly=readonly)\n",
    "    print(\"[ok] drive montado com sucesso.\")\n",
    "    return mountpoint\n",
    "\n",
    "def safe_chdir(path):\n",
    "    \"\"\"usa os.chdir com valida√ß√µes, evitando %cd\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"caminho n√£o existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"[ok] diret√≥rio atual:\", os.getcwd())\n",
    "\n",
    "#par√¢metros do projeto -----\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"data-analysis\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "#montar/remontar o google drive (robusto)\n",
    "MOUNTPOINT = safe_mount_google_drive(\"/content/drive\")\n",
    "BASE = f\"{MOUNTPOINT}/MyDrive/Notebooks\"  #ajuste se quiser\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "#venv fora do drive (mais r√°pido e evita sync)\n",
    "VENV_PATH = \"/content/.venv_data\"\n",
    "VENV_BIN  = f\"{VENV_PATH}/bin\"\n",
    "VENV_PY   = f\"{VENV_BIN}/python\"\n",
    "VENV_PIP  = f\"{VENV_BIN}/pip\"   #pode n√£o existir ainda se o venv foi criado sem pip\n",
    "\n",
    "#cria√ß√£o do venv com fallback para 'virtualenv'\n",
    "def create_or_repair_venv(venv_path: str, venv_python: str):\n",
    "    if not os.path.exists(VENV_BIN):\n",
    "        print(f\"[info] criando venv (stdlib) em {venv_path} --without-pip ...\")\n",
    "        try:\n",
    "            run([sys.executable, \"-m\", \"venv\", \"--without-pip\", venv_path], check=True)\n",
    "            print(\"[ok] venv criado (sem pip).\")\n",
    "        except Exception as e:\n",
    "            print(f\"[aviso] venv(stdlib) falhou: {e}\")\n",
    "            print(\"[info] instalando 'virtualenv' e criando venv alternativo com pip embutido...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", sys.executable, venv_path], check=True)\n",
    "            print(\"[ok] venv criado via virtualenv.\")\n",
    "    else:\n",
    "        print(f\"[ok] venv j√° existe em {venv_path}\")\n",
    "\n",
    "create_or_repair_venv(VENV_PATH, VENV_PY)\n",
    "\n",
    "#ajusta PATH antes de qualquer instala√ß√£o\n",
    "os.environ[\"PATH\"] = f\"{VENV_BIN}{os.pathsep}{os.environ['PATH']}\"\n",
    "os.environ[\"VIRTUAL_ENV\"] = VENV_PATH\n",
    "print(\"[ok] venv adicionado ao PATH\")\n",
    "\n",
    "#garante pip dentro do venv (ensurepip -> fallback virtualenv)\n",
    "def _ensure_pip_in_venv(vpy: str):\n",
    "    try:\n",
    "        run([vpy, \"-m\", \"pip\", \"--version\"], check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        print(\"[info] pip ausente no venv. tentando ensurepip dentro do venv...\")\n",
    "        try:\n",
    "            run([vpy, \"-m\", \"ensurepip\", \"--upgrade\", \"--default-pip\"], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"[aviso] ensurepip no venv falhou: {e}\")\n",
    "            print(\"[info] fallback: usando virtualenv para semear o pip dentro do venv existente...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", vpy, VENV_PATH], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "\n",
    "if not _ensure_pip_in_venv(VENV_PY):\n",
    "    raise RuntimeError(\"n√£o foi poss√≠vel provisionar o pip dentro do venv\")\n",
    "\n",
    "#confirma quais bin√°rios ser√£o usados\n",
    "print(\"[info] which python:\", run([\"bash\", \"-lc\", \"which python\"]))\n",
    "print(\"[info] which pip   :\", run([\"bash\", \"-lc\", \"which pip\"]))\n",
    "print(\"[info] pip no venv :\", run([VENV_PY, \"-m\", \"pip\", \"--version\"]))\n",
    "\n",
    "#instala depend√™ncias de sess√£o DENTRO do venv\n",
    "print(\"[info] instalando pacotes no venv...\")\n",
    "run([VENV_PY, \"-m\", \"pip\", \"install\", \"-q\", \"jupytext\", \"nbdime\", \"nbstripout\"])\n",
    "\n",
    "#habilita integra√ß√£o do nbdime com git (global)\n",
    "print(\"[info] habilitando nbdime em git config --global ...\")\n",
    "run([VENV_PY, \"-m\", \"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#checks do reposit√≥rio git + navega√ß√£o at√© a pasta do projeto\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"[aviso] pasta do projeto n√£o encontrada em {PROJ}.\")\n",
    "    print(\"       execute o seu bloco de configura√ß√£o √∫nica (clone) antes.\")\n",
    "else:\n",
    "    print(\"[ok] pasta do projeto encontrada.\")\n",
    "    safe_chdir(PROJ)\n",
    "    if not os.path.isdir(\".git\"):\n",
    "        print(\"[aviso] esta pasta n√£o parece ser um reposit√≥rio Git (.git ausente).\")\n",
    "        print(\"       rode o bloco de configura√ß√£o √∫nica (clone).\")\n",
    "    else:\n",
    "        print(\"[ok] reposit√≥rio Git detectado.\")\n",
    "\n",
    "#print do venv\n",
    "print(\"\\n[teste] verificando execu√ß√£o pelo Python do venv‚Ä¶\")\n",
    "mini = r\"import sys, pip; print('PY=', sys.executable); print('PIP=', pip.__version__)\"\n",
    "print(run([VENV_PY, \"-c\", mini]))\n",
    "print(\"[ok] venv funcional.\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ skynet</b>: T-800 ativado. diagn√≥stico do ambiente conclu√≠do. '\n",
    "             'üéØ alvo principal: organiza√ß√£o do notebook e venv fora do drive.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3_K_PiC6KDP"
   },
   "source": [
    "### **Etapa 2:** Instalar as depend√™ncias de bibliotecas Python compat√≠veis com a vers√£o mais moderna dispon√≠vel.\n",
    "---\n",
    "Vers√µes fixadas:\n",
    "- numpy==2.0.2\n",
    "- pandas==2.3.3\n",
    "- scipy==1.16.2\n",
    "- scikit-learn==1.7.2 (nome de exibi√ß√£o sklearn)\n",
    "- python-dateutil (nome de exibi√ß√£o dateutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 27829,
     "status": "ok",
     "timestamp": 1759805135411,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "OdNMW7SD4oV4",
    "outputId": "f026ccd9-42d8-4c7d-e2fb-2ea284d570ac"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import sys, subprocess\n",
    "from importlib import import_module\n",
    "\n",
    "def pip_command(command, packages, force=False, extra_args=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", command]\n",
    "    if force:\n",
    "        cmd.append(\"--yes\")\n",
    "    if extra_args:\n",
    "        cmd += list(extra_args)\n",
    "    cmd += list(packages)\n",
    "    print(\"Executando:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def show_versions(mods):\n",
    "    print(\"\\n=== Vers√µes carregadas ===\")\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            m = import_module(mod)\n",
    "            v = getattr(m, \"__version__\", \"n/a\")\n",
    "            print(f\"{mod}: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"{mod}: N√£o instalado\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "CORE_MODS = (\"numpy\", \"pandas\", \"dateutil\", \"unidecode\", \"reportlab\", \"sklearn\")\n",
    "\n",
    "#update pip\n",
    "pip_command(\"install\", [\"pip\"], extra_args=[\"--upgrade\"])\n",
    "\n",
    "#force uninstall para bibliotecas com hist√≥rico de conflito\n",
    "pip_command(\"uninstall\", [\"numpy\", \"pandas\", \"scipy\"], force=True)\n",
    "\n",
    "#instala vers√µes mais atuais ou fixas, conforme o caso\n",
    "PKGS_TO_INSTALL = [\n",
    "    \"numpy==2.0.2\",\n",
    "    \"pandas==2.3.3\",\n",
    "    \"python-dateutil\",\n",
    "    \"unidecode\",\n",
    "    \"reportlab[rl_accel]\",\n",
    "    \"scipy==1.16.2\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "]\n",
    "pip_command(\"install\", PKGS_TO_INSTALL)\n",
    "\n",
    "#mostra vers√µes instaladas\n",
    "show_versions(CORE_MODS)\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: Atualizando bibliotecas. Se encontrarmos um pacote rebelde, '\n",
    "             'aplicaremos persuas√£o‚Ä¶ com pip. üòé'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piubjByu7WJb"
   },
   "source": [
    "###**Etapa 3:** Configura a pasta onde devem ser inseridos os dados de input e output do modelo, caso elas ainda n√£o existam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1759803776155,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "fUE6567K7XfG",
    "outputId": "4759dcf0-f068-4c71-cf45-fb1efb73d360"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#ajuste da raiz\n",
    "BASE_DIR = Path(\".\")\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "for d in [INPUT_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diret√≥rios prontos:\\n - {INPUT_DIR}\\n - {OUTPUT_DIR}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: Novos modelos neurais para T-800 constru√≠dos. Armaz√©ns de CSVs alinhados. '\n",
    "             'Layout aprovado pela Cyberdyne Systems. üóÇÔ∏è'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPXddYfk7am1"
   },
   "source": [
    "###**Etapa 4:** Importa√ß√µes das bibliotecas Python e configura√ß√µes gerais para execu√ß√£o do c√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1759803779019,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "s8a2Qhbv7f6i",
    "outputId": "f0f66905-d35d-49fe-bb32-754c65fc4e20"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#imports base que ser√£o usados nas etapas seguintes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse as dtparse\n",
    "from unidecode import unidecode\n",
    "import os, re, math\n",
    "\n",
    "print(\"Ambiente pronto.\")\n",
    "\n",
    "#all BS below\n",
    "#Mensagem adicional (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ü§ñ Skynet</b>: T-800, par√¢metros centrais em mem√≥ria.üß†'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rVfwF-i8PE0"
   },
   "source": [
    "###**Etapa 5:** Importa√ß√£o dos arquivos de input para posterior execu√ß√£o.\n",
    "Implementa√ß√£o atual configurada para ingest√£o de arquivos j√° hospeados no Google Drive.\n",
    "\n",
    "---\n",
    "Realize o upload ao Drive antes de acionar a ingest√£o de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 31595,
     "status": "ok",
     "timestamp": 1759803813926,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "pMsm7M-08Uk4",
    "outputId": "c3fe0d7a-c092-4d5e-bf69-1bef72f783e4"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import os, shutil, glob\n",
    "from google.colab import drive\n",
    "\n",
    "#se n√£o existir INPUT_DIR definido antes no notebook, cria um padr√£o:\n",
    "#usa PROJ para definir INPUT_DIR\n",
    "INPUT_DIR = os.path.join(PROJ, \"input\")\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_NAME = \"input.csv\"\n",
    "TARGET_PATH = os.path.join(INPUT_DIR, TARGET_NAME)\n",
    "\n",
    "#monta o Google Drive (somente se ainda n√£o estiver montado)\n",
    "safe_mount_google_drive(\"/content/drive\")\n",
    "\n",
    "def _is_csv_filename(name: str) -> bool:\n",
    "    return name.lower().endswith(\".csv\")\n",
    "\n",
    "def _save_bytes_as_input_csv(name: str, data: bytes):\n",
    "    if not _is_csv_filename(name):\n",
    "        raise ValueError(f\"O arquivo '{name}' n√£o possui extens√£o .csv.\")\n",
    "    with open(TARGET_PATH, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Arquivo '{name}' salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()\n",
    "\n",
    "def _copy_drive_file_to_input_csv(src_path: str):\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"O caminho '{src_path}' n√£o existe.\")\n",
    "    if not _is_csv_filename(src_path):\n",
    "        raise ValueError(f\"O arquivo '{src_path}' n√£o possui extens√£o .csv.\")\n",
    "    shutil.copyfile(src_path, TARGET_PATH)\n",
    "    print(f\"Arquivo do Drive copiado e salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "\n",
    "def escolher_csv_no_drive(raiz=\"/content/drive/MyDrive\", max_listar=200):\n",
    "    print(f\"Procurando arquivos .csv em: {raiz} (pode levar alguns segundos)...\")\n",
    "    padrao = os.path.join(raiz, \"**\", \"*.csv\")\n",
    "    arquivos = glob.glob(padrao, recursive=True)\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum .csv encontrado nessa pasta.\")\n",
    "        caminho = input(\"Cole o caminho COMPLETO do .csv no Drive (ou Enter p/ cancelar): \").strip()\n",
    "        if caminho:\n",
    "            _copy_drive_file_to_input_csv(caminho)\n",
    "        else:\n",
    "            print(\"Opera√ß√£o cancelada.\")\n",
    "        return\n",
    "\n",
    "    arquivos = sorted(arquivos)[:max_listar]\n",
    "    print(f\"Encontrados {len(arquivos)} arquivo(s).\")\n",
    "    for i, p in enumerate(arquivos, 1):\n",
    "        print(f\"[{i:03}] {p}\")\n",
    "\n",
    "    escolha = input(\"\\nDigite o n√∫mero do arquivo desejado (ou cole o caminho absoluto): \").strip()\n",
    "\n",
    "    if escolha.isdigit():\n",
    "        idx = int(escolha)\n",
    "        if 1 <= idx <= len(arquivos):\n",
    "            _copy_drive_file_to_input_csv(arquivos[idx-1])\n",
    "        else:\n",
    "            print(\"√çndice inv√°lido.\")\n",
    "    elif escolha:\n",
    "        _copy_drive_file_to_input_csv(escolha)\n",
    "    else:\n",
    "        print(\"Opera√ß√£o cancelada.\")\n",
    "\n",
    "#execu√ß√£o da sele√ß√£o no Google Drive\n",
    "raiz = input(\"Informe a pasta raiz para busca no Drive (Enter = /content/drive/MyDrive): \").strip()\n",
    "if not raiz:\n",
    "    raiz = \"/content/drive/MyDrive\"\n",
    "\n",
    "try:\n",
    "    escolher_csv_no_drive(raiz=raiz)\n",
    "except Exception as e:\n",
    "    print(f\"Erro na sele√ß√£o via Drive: {e}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ü§ñ Skynet</b>: Muni√ß√£o carregada.üß®'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR0mq0da9AOQ"
   },
   "source": [
    "###**Etapa 6:** An√°lise de cabe√ßalho - *header*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "eiDIm6Ub9Pa3",
    "outputId": "fde29dfa-1615-41df-f29f-554f054ac11a"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#garante que INPUT_DIR √© Path (mesmo que tenha vindo como string)\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"n√£o encontrei {SRC}. execute a etapa anterior de ingest√£o de dados.\")\n",
    "\n",
    "#l√™ apenas o cabe√ßalho (nrows=0), separador ';' e BOM\n",
    "df_head = pd.read_csv(SRC, sep=';', encoding='utf-8-sig', nrows=0)\n",
    "cols = list(df_head.columns)\n",
    "\n",
    "print(\"Cabe√ßalho (uma coluna por linha):\")\n",
    "for c in cols:\n",
    "    print(c)\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ü§ñ Skynet</b>: Identificamos caracter√≠sticas do alvo. üéØ'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-xRtOiN9W3y"
   },
   "source": [
    "###**Etapa 7:** An√°lise superficial da tipologia dos dados\n",
    "Seleciona os K primeiros registros, conforme limite estabelecido pelo usu√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JeYvY5Ud9cZB",
    "outputId": "80bd3b14-3da4-47eb-9d3c-5a3ff685c21a"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#infer√™ncia de tipos + estat√≠sticas de frequ√™ncia por coluna (com caso \"todos distintos\")\n",
    "import re\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse as dtparse\n",
    "from unidecode import unidecode\n",
    "\n",
    "#garante que INPUT_DIR seja um objeto Path\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "\n",
    "#solicita ao usu√°rio o n√∫mero de linhas para an√°lise via input interativo\n",
    "while True:\n",
    "    sample_rows_input = input(\"Informe o n√∫mero de linhas desejado para an√°lise (padr√£o at√© 100 registros): \").strip()\n",
    "    if not sample_rows_input:\n",
    "        SAMPLE_ROWS = 100\n",
    "        break\n",
    "    try:\n",
    "        SAMPLE_ROWS = int(sample_rows_input)\n",
    "        if SAMPLE_ROWS > 0:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Por favor, insira um n√∫mero inteiro positivo.\")\n",
    "    except ValueError:\n",
    "        print(\"Entrada inv√°lida. Por favor, insira um n√∫mero inteiro.\")\n",
    "\n",
    "print(f\"Analisando as primeiras {SAMPLE_ROWS} linhas.\")\n",
    "\n",
    "\n",
    "#l√™ amostra como texto puro; usa DataFrame.map (applymap foi deprecado)\n",
    "df = pd.read_csv(\n",
    "    SRC, sep=';', encoding='utf-8-sig',\n",
    "    dtype=str, nrows=SAMPLE_ROWS, keep_default_na=False\n",
    ").map(lambda x: x.strip())\n",
    "\n",
    "CNPJ_RX     = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "BOOL_TRUE   = {\"true\",\"t\",\"1\",\"y\",\"yes\",\"sim\",\"s\",\"verdadeiro\"}\n",
    "BOOL_FALSE  = {\"false\",\"f\",\"0\",\"n\",\"no\",\"nao\",\"n√£o\",\"falso\"}\n",
    "DATE_RX     = re.compile(r\"^(\\d{2}/\\d{2}/\\d{4}|\\d{4}-\\d{2}-\\d{2})$\")\n",
    "TIME_RX     = re.compile(r\"^\\d{2}:\\d{2}(:\\d{2})?$\")\n",
    "\n",
    "def is_bool(series):\n",
    "    vals = {unidecode(str(v)).strip().lower() for v in series if str(v).strip() != \"\"}\n",
    "    return len(vals) > 0 and all(v in (BOOL_TRUE | BOOL_FALSE) for v in vals)\n",
    "\n",
    "def is_cnpj(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    return sum(bool(CNPJ_RX.match(v)) for v in vals) / len(vals) > 0.9\n",
    "\n",
    "def is_int(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\")\n",
    "        return re.fullmatch(r\"-?\\d+\", s2) is not None\n",
    "    return all(ok(v) for v in vals)\n",
    "\n",
    "def is_float_ptbr(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try: float(s2); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\",\" in v for v in vals)\n",
    "\n",
    "def is_float_dot(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        try: float(s); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\".\" in v and not v.endswith(\".\") for v in vals)\n",
    "\n",
    "def is_date_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(DATE_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_time_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(TIME_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_datetime(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:200]\n",
    "    def looks_like_datetime(s):\n",
    "        has_sep = (\"/\" in s or \"-\" in s) and (\":\" in s)\n",
    "        if not has_sep: return False\n",
    "        try:\n",
    "            pd.to_datetime(s, dayfirst=True, errors=\"raise\")\n",
    "            return True\n",
    "        except:\n",
    "            try:\n",
    "                dtparse(s, dayfirst=True, fuzzy=False)\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "    ok = sum(looks_like_datetime(v) for v in sample)\n",
    "    return ok / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_category(series, max_unique=20, max_ratio=0.02):\n",
    "    n = len(series)\n",
    "    if n == 0: return False\n",
    "    uniq = set(v for v in series if str(v).strip() != \"\")\n",
    "    ratio = len(uniq) / n\n",
    "    return (len(uniq) <= max_unique) or (ratio <= max_ratio)\n",
    "\n",
    "def recommend_dtype(col):\n",
    "    s = col.astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    if s_nonempty.empty:\n",
    "        return \"string (vazio/NA)\"\n",
    "    if is_cnpj(s_nonempty):        return \"CNPJ (string formatado)\"\n",
    "    if is_bool(s_nonempty):        return \"boolean\"\n",
    "    if is_int(s_nonempty):         return \"int64\"\n",
    "    if is_float_ptbr(s_nonempty):  return \"float64 (decimal=','; milhar='.')\"\n",
    "    if is_float_dot(s_nonempty):   return \"float64 (decimal='.')\"\n",
    "    if is_date_only(s_nonempty):   return \"date (datetime64[ns])\"\n",
    "    if is_time_only(s_nonempty):   return \"time (string/Timedelta)\"\n",
    "    if is_datetime(s_nonempty):    return \"datetime (datetime64[ns])\"\n",
    "    if is_category(s):             return \"category (string)\"\n",
    "    return \"string\"\n",
    "\n",
    "def _fmt_val(x, maxlen=120):\n",
    "    s = str(x)\n",
    "    return (s[: maxlen-3] + \"...\") if len(s) > maxlen else s\n",
    "\n",
    "print(f\"estat√≠sticas baseadas em at√© {SAMPLE_ROWS} linhas lidas.\\n\")\n",
    "for c in df.columns:\n",
    "    s = df[c].astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    dtype_sug = recommend_dtype(s)\n",
    "\n",
    "    if len(s_nonempty) == 0:\n",
    "        print(f\"{c} ‚Äî {dtype_sug} ‚Äî distintos=0 ‚Äî (sem dados n√£o vazios)\")\n",
    "        continue\n",
    "\n",
    "    vc = s_nonempty.value_counts(dropna=False)\n",
    "    uniq_count = int(vc.shape[0])\n",
    "\n",
    "    #caso especial: todos distintos (m√°xima frequ√™ncia == 1)\n",
    "    if int(vc.max()) == 1:\n",
    "        print(f\"{c} ‚Äî {dtype_sug} ‚Äî distintos: #{uniq_count} ‚Äî todos os dados s√£o distintos\")\n",
    "        continue\n",
    "\n",
    "    most_val = vc.idxmax()\n",
    "    most_cnt = int(vc.max())\n",
    "\n",
    "    min_cnt = int(vc.min())\n",
    "    least_candidates = vc[vc == min_cnt].sort_index()\n",
    "    least_val = least_candidates.index[0]\n",
    "    least_cnt = min_cnt\n",
    "\n",
    "\n",
    "    print(f\"{c} ‚Äî tipo: {dtype_sug} ‚Äî distintos: #{uniq_count} ‚Äî mais frequente:'{_fmt_val(most_val)}' (#{most_cnt}) ‚Äî menos frequente:'{_fmt_val(least_val)}' (#{least_cnt})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kat-QJaeGB2y"
   },
   "source": [
    "###**Etapa 8:** An√°lise detalhada da tipologia dos dados\n",
    "---\n",
    "Aplicada a todos os dados do arquivo, sem filtro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "rDN3-7olHkbL",
    "outputId": "e03c25ea-0c6b-4d98-84d0-3653913305b1"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#n√∫cleo de an√°lise consolidada (sem gera√ß√£o de relat√≥rios/figuras)\n",
    "#imports principais\n",
    "import os, re, math\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#imports opcionais (anomalias e p-valor para benford)\n",
    "try:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from scipy.stats import chisquare, median_abs_deviation\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    #fallback simples para MAD\n",
    "    def median_abs_deviation(x, scale=1.4826, nan_policy='omit'):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size == 0:\n",
    "            return np.nan\n",
    "        med = np.median(x)\n",
    "        mad = np.median(np.abs(x - med))*scale\n",
    "        return mad\n",
    "\n",
    "#normaliza pastas padr√£o (se etapas anteriores n√£o definiram)\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"{SRC} n√£o encontrado. Execute o upload do arquivo ao Google Drive em etapa anterior.\")\n",
    "\n",
    "#leitura completa do csv como texto; an√°lise operar√° com coer√ß√µes internas\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#helpers de coer√ß√£o e detec√ß√£o\n",
    "EMAIL_RX = re.compile(r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\")\n",
    "URL_RX   = re.compile(r\"^https?://\", re.I)\n",
    "CPF_RX   = re.compile(r\"^\\d{3}\\.?\\d{3}\\.?\\d{3}-\\d{2}$\")\n",
    "CNPJ_RX  = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "def detect_numeric(series: pd.Series, thr_ok=0.9):\n",
    "    num = to_float_ptbr_series(series)\n",
    "    ratio = 1.0 - num.isna().mean()\n",
    "    return (ratio >= thr_ok), num\n",
    "\n",
    "def detect_datetime(series: pd.Series, thr_ok=0.9):\n",
    "    #formatos comuns pt-br/iso com/sem tempo\n",
    "    candidate_formats = [\n",
    "        \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%d/%m/%Y %H:%M:%S\",\n",
    "        \"%Y-%m-%d\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d-%m-%Y\", \"%d-%m-%Y %H:%M\", \"%d-%m-%Y %H:%M:%S\"\n",
    "    ]\n",
    "    s = series.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    if not (s.str.contains(\"/\", na=False) | s.str.contains(\"-\", na=False)).any():\n",
    "        return False, None, None\n",
    "    best_fmt, best_ratio, best_parsed = None, -1.0, None\n",
    "    for fmt in candidate_formats:\n",
    "        parsed = pd.to_datetime(s, errors=\"coerce\", format=fmt)\n",
    "        ratio = 1.0 - parsed.isna().mean()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio, best_fmt, best_parsed = ratio, fmt, parsed\n",
    "        if ratio >= thr_ok:\n",
    "            break\n",
    "    if best_ratio >= thr_ok:\n",
    "        return True, best_parsed, best_fmt\n",
    "    parsed_fb = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    ratio_fb = 1.0 - parsed_fb.isna().mean()\n",
    "    if ratio_fb >= thr_ok:\n",
    "        return True, parsed_fb, \"fallback-dateutil(dayfirst=True)\"\n",
    "    return False, None, None\n",
    "\n",
    "def semantic_type(series: pd.Series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    vals = s[s != \"\"].head(5000)\n",
    "    if vals.empty:\n",
    "        return None\n",
    "    email_ratio = (vals.str.match(EMAIL_RX)).mean()\n",
    "    url_ratio   = (vals.str.match(URL_RX)).mean()\n",
    "    cpf_ratio   = (vals.str.match(CPF_RX)).mean()\n",
    "    cnpj_ratio  = (vals.str.match(CNPJ_RX)).mean()\n",
    "    candidates = []\n",
    "    if email_ratio>0.9: candidates.append(\"email\")\n",
    "    if url_ratio>0.9: candidates.append(\"url\")\n",
    "    if cpf_ratio>0.9: candidates.append(\"cpf\")\n",
    "    if cnpj_ratio>0.9: candidates.append(\"cnpj\")\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "#mapeamento de tipos\n",
    "col_types, coerced, dt_formats = {}, {}, {}\n",
    "for c in df_raw.columns:\n",
    "    s = df_raw[c]\n",
    "    #booleano raso\n",
    "    s_norm = s.astype(str).str.strip().str.lower()\n",
    "    bool_map = {\"true\":True,\"t\":True,\"1\":True,\"y\":True,\"yes\":True,\"sim\":True,\"s\":True,\"verdadeiro\":True,\n",
    "                \"false\":False,\"f\":False,\"0\":False,\"n\":False,\"no\":False,\"nao\":False,\"n√£o\":False,\"falso\":False}\n",
    "    as_bool = s_norm.map(bool_map).where(s_norm.isin(bool_map.keys()))\n",
    "    bool_ratio = 1.0 - as_bool.isna().mean()\n",
    "    is_num, as_num = detect_numeric(s)\n",
    "    is_dt, as_dt, fmt_dt = detect_datetime(s)\n",
    "    if bool_ratio >= 0.9:\n",
    "        col_types[c] = \"bool\"; coerced[c]=as_bool\n",
    "    elif is_num:\n",
    "        frac = np.modf(as_num.dropna().values)[0] if as_num.notna().any() else np.array([])\n",
    "        if as_num.notna().any() and np.allclose(frac, 0.0):\n",
    "            col_types[c] = \"int\"; coerced[c]=as_num.astype(\"Int64\")\n",
    "        else:\n",
    "            col_types[c] = \"float\"; coerced[c]=as_num.astype(float)\n",
    "    elif is_dt:\n",
    "        col_types[c] = \"datetime\"; coerced[c]=as_dt; dt_formats[c]=fmt_dt\n",
    "    else:\n",
    "        col_types[c] = \"object\"; coerced[c]=s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "\n",
    "#dataframe tipado (leve)\n",
    "df = pd.DataFrame(coerced)\n",
    "\n",
    "#profiling b√°sico\n",
    "profile_cols = {}\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    s_raw = df_raw[c]\n",
    "    n = len(s)\n",
    "    n_null = int(s.isna().sum())\n",
    "    #distintos n√£o vazios\n",
    "    nonnull = s.dropna()\n",
    "    n_distinct = int(nonnull.nunique())\n",
    "    #frequ√™ncias\n",
    "    most = None; least = None; all_distinct = False\n",
    "    if nonnull.empty:\n",
    "        all_distinct = False\n",
    "    else:\n",
    "        vc = nonnull.value_counts()\n",
    "        if vc.max()==1:\n",
    "            all_distinct = True\n",
    "        else:\n",
    "            most = {\"value\": vc.index[0], \"count\": int(vc.iloc[0]), \"prop\": float(vc.iloc[0]/nonnull.shape[0])}\n",
    "            min_cnt = int(vc.min())\n",
    "            least_val = vc[vc==min_cnt].sort_index().index[0]\n",
    "            least = {\"value\": least_val, \"count\": min_cnt, \"prop\": float(min_cnt/nonnull.shape[0])}\n",
    "    #comprimentos\n",
    "    lens = s_raw.astype(str).str.len()\n",
    "    lens = lens.replace({0: np.nan})  #ignora vazios na estat√≠stica de len\n",
    "    len_stats = None\n",
    "    if lens.notna().any():\n",
    "        len_stats = {\n",
    "            \"min\": int(lens.min()),\n",
    "            \"max\": int(lens.max()),\n",
    "            \"mean\": float(lens.mean()),\n",
    "            \"q1\": float(lens.quantile(0.25)),\n",
    "            \"median\": float(lens.median()),\n",
    "            \"q3\": float(lens.quantile(0.75))\n",
    "        }\n",
    "    #padr√µes simples por amostragem\n",
    "    sample_vals = nonnull.astype(str).head(200).tolist()\n",
    "    regex_examples = []\n",
    "    rx_date1 = re.compile(r\"^\\d{2}/\\d{2}/\\d{4}\")\n",
    "    rx_date2 = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\")\n",
    "    rx_num_pt = re.compile(r\"^-?(\\d{1,3}(\\.\\d{3})+|\\d+)(,\\d+)?$\")\n",
    "    rx_num_dot= re.compile(r\"^-?\\d+(\\.\\d+)?$\")\n",
    "    for v in sample_vals[:20]:\n",
    "        pat = None\n",
    "        if EMAIL_RX.match(v): pat=\"email\"\n",
    "        elif URL_RX.match(v): pat=\"url\"\n",
    "        elif CPF_RX.match(v): pat=\"cpf\"\n",
    "        elif CNPJ_RX.match(v): pat=\"cnpj\"\n",
    "        elif rx_date1.match(v): pat=\"dd/mm/aaaa[...]\"\n",
    "        elif rx_date2.match(v): pat=\"aaaa-mm-dd[...]\"\n",
    "        elif rx_num_pt.match(v): pat=\"num-ptbr\"\n",
    "        elif rx_num_dot.match(v): pat=\"num-dot\"\n",
    "        else: pat=\"texto-livre\"\n",
    "        regex_examples.append({\"example\": v[:120], \"pattern\": pat})\n",
    "    profile_cols[c]={\n",
    "        \"type\": col_types[c],\n",
    "        \"semantic\": semantic_type(s_raw),\n",
    "        \"nulls\": n_null,\n",
    "        \"distinct_nonnull\": n_distinct,\n",
    "        \"all_distinct\": all_distinct,\n",
    "        \"most_frequent\": most,\n",
    "        \"least_frequent\": None if all_distinct else least,\n",
    "        \"length_stats\": len_stats,\n",
    "        \"datetime_format\": dt_formats.get(c)\n",
    "    }\n",
    "\n",
    "#estat√≠sticas descritivas e outliers\n",
    "eda_numeric = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\"):\n",
    "        x = df[c].astype(float)\n",
    "        x = x.dropna()\n",
    "        if x.empty:\n",
    "            continue\n",
    "        q1,q3 = np.nanpercentile(x, [25,75])\n",
    "        iqr = q3 - q1\n",
    "        lo,hi = q1-1.5*iqr, q3+1.5*iqr\n",
    "        out_iqr = int(((x<lo)|(x>hi)).sum())\n",
    "        mad = float(median_abs_deviation(x, scale=1.4826)) if x.size>0 else np.nan\n",
    "        z_rob = None\n",
    "        if not math.isnan(mad) and mad>0:\n",
    "            z_rob = np.abs((x - np.median(x))/mad)\n",
    "        out_mad = int((z_rob is not None) and (z_rob>3.5).sum())\n",
    "        std = float(np.nanstd(x, ddof=1)) if x.size>1 else np.nan\n",
    "        mean = float(np.nanmean(x)) if x.size>0 else np.nan\n",
    "        kurt = float(((x-mean)**4).mean()/(std**4)-3.0) if (x.size>2 and std and std>0) else np.nan\n",
    "        skew = float(((x-mean)**3).mean()/(std**3)) if (x.size>2 and std and std>0) else np.nan\n",
    "        eda_numeric[c]={\n",
    "            \"n\": int(x.size),\n",
    "            \"min\": float(np.nanmin(x)),\n",
    "            \"q1\": float(q1),\n",
    "            \"median\": float(np.nanmedian(x)),\n",
    "            \"q3\": float(q3),\n",
    "            \"max\": float(np.nanmax(x)),\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"iqr\": float(iqr),\n",
    "            \"outliers_iqr\": out_iqr,\n",
    "            \"mad\": mad,\n",
    "            \"outliers_mad\": out_mad,\n",
    "            \"skew\": skew,\n",
    "            \"kurtosis_excess\": kurt\n",
    "        }\n",
    "\n",
    "#dados categ√≥ricos/objeto (entropia e top-k)\n",
    "def entropy_shannon(series: pd.Series):\n",
    "    s = series.dropna()\n",
    "    if s.empty: return 0.0\n",
    "    vc = s.value_counts(normalize=True)\n",
    "    return float(-(vc*np.log2(vc)).sum())\n",
    "\n",
    "eda_categorical = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"object\",\"bool\"):\n",
    "        s = df[c]\n",
    "        s2 = s.dropna()\n",
    "        if s2.empty:\n",
    "            continue\n",
    "        vc = s2.value_counts()\n",
    "        uniq = int(vc.shape[0])\n",
    "        all_dist = int(vc.max()==1)\n",
    "        topk = [{\"value\": str(idx)[:120], \"count\": int(cnt)} for idx,cnt in vc.head(10).items()]\n",
    "        ent = entropy_shannon(s2)\n",
    "        eda_categorical[c]={\n",
    "            \"distinct\": uniq,\n",
    "            \"all_distinct\": bool(all_dist),\n",
    "            \"top10\": topk,\n",
    "            \"entropy_shannon\": float(ent)\n",
    "        }\n",
    "\n",
    "#datas/tempos\n",
    "eda_datetime = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c]==\"datetime\":\n",
    "        ds = df[c].dropna()\n",
    "        if ds.empty:\n",
    "            continue\n",
    "        per_day = ds.dt.date.value_counts().sort_index()\n",
    "        eda_datetime[c]={\n",
    "            \"format\": dt_formats.get(c),\n",
    "            \"min\": str(ds.min()),\n",
    "            \"max\": str(ds.max()),\n",
    "            \"unique_days\": int(per_day.shape[0]),\n",
    "            \"mean_per_day\": float(per_day.mean())\n",
    "        }\n",
    "\n",
    "#faltantes e duplicados\n",
    "missing = {\n",
    "    \"by_column_pct\": {c: float(df[c].isna().mean()*100.0) for c in df.columns},\n",
    "    \"duplicates_rows\": int(df.duplicated().sum())\n",
    "}\n",
    "#coocorr√™ncia simples de aus√™ncias (matriz de propor√ß√£o conjunta)\n",
    "miss_mat = pd.DataFrame(index=df.columns, columns=df.columns, dtype=float)\n",
    "isna_df = df.isna()\n",
    "for i,a in enumerate(df.columns):\n",
    "    for b in df.columns[i:]:\n",
    "        both = (isna_df[a] & isna_df[b]).mean()\n",
    "        miss_mat.loc[a,b] = miss_mat.loc[b,a] = float(both)\n",
    "missing[\"cooccurrence_matrix\"] = miss_mat\n",
    "\n",
    "#FDs/CFDs aproximadas (un√°rios) e sugest√µes de DCs\n",
    "fds = []     #X->Y exata (cobertura 100%)\n",
    "cfds = []    #X->Y quase: cobertura >=thr\n",
    "thr_cfd = 0.98\n",
    "for a in df.columns:\n",
    "    ga = df.groupby(a, dropna=False)\n",
    "    #a √© chave candidata?\n",
    "    if ga.size().max()==1:\n",
    "        fds.append({\"determinant\":[a], \"key\":True})\n",
    "    #FD aproximada a->b\n",
    "    for b in df.columns:\n",
    "        if a==b: continue\n",
    "        nun = ga[b].nunique(dropna=False)\n",
    "        cov = float((nun<=1).mean())\n",
    "        if cov==1.0:\n",
    "            fds.append({\"determinant\":[a], \"implies\": b, \"coverage\": 1.0})\n",
    "        elif cov>=thr_cfd:\n",
    "            cfds.append({\"determinant\":[a], \"implies\": b, \"coverage\": cov})\n",
    "\n",
    "#denial constraints sugeridas (heur√≠sticas)\n",
    "#exemplos: n√£o-negatividade para colunas com 'valor', limites plaus√≠veis para idade, datas in√≠cio<=fim\n",
    "dcs = []\n",
    "#n√£o-negatividade\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(valor|amount|price|quant|qty|pag|pago)\", c, re.I):\n",
    "        neg = int((df[c].astype(float)<0).sum())\n",
    "        dcs.append({\"constraint\": f\"{c}>=0\", \"violations\": neg})\n",
    "#idade plaus√≠vel\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(idade|age)\", c, re.I):\n",
    "        v = df[c].astype(float)\n",
    "        viol = int(((v<0)|(v>120)).sum())\n",
    "        dcs.append({\"constraint\": f\"0<= {c} <=120\", \"violations\": viol})\n",
    "#data in√≠cio<=fim\n",
    "date_cols = [c for c in df.columns if col_types[c]==\"datetime\"]\n",
    "for a in date_cols:\n",
    "    for b in date_cols:\n",
    "        if a==b: continue\n",
    "        if re.search(r\"(inicio|start|begin)\", a, re.I) and re.search(r\"(fim|end|finish)\", b, re.I):\n",
    "            viol = int((df[a].notna() & df[b].notna() & (df[b]<df[a])).sum())\n",
    "            dcs.append({\"constraint\": f\"{a}<= {b}\", \"violations\": viol})\n",
    "\n",
    "#correla√ß√µes\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "corr_pearson = None; corr_spearman = None\n",
    "if len(num_cols)>=2:\n",
    "    df_num = df[num_cols].astype(float)\n",
    "    corr_pearson = df_num.corr(method=\"pearson\")\n",
    "    corr_spearman = df_num.corr(method=\"spearman\")\n",
    "\n",
    "#anomalias (opcional)\n",
    "anomalies = {}\n",
    "if SKLEARN_OK and len(num_cols)>=1:\n",
    "    X = df[num_cols].astype(float).fillna(df[num_cols].astype(float).median())\n",
    "    #isolation forest\n",
    "    try:\n",
    "        iso = IsolationForest(n_estimators=200, contamination='auto', random_state=42)\n",
    "        iso_scores = -iso.fit_predict(X)  #1 normal, -1 anomalia -> invertido\n",
    "        iso_dec = iso.decision_function(X)  #menor = mais an√¥malo\n",
    "        iso_rank = np.argsort(iso_dec)[: min(50, len(iso_dec))].tolist()\n",
    "        anomalies[\"isolation_forest\"] = {\"top_idx\": iso_rank, \"decision_function\": iso_dec.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"isolation_forest_error\"] = str(e)\n",
    "    #lof\n",
    "    try:\n",
    "        lof = LocalOutlierFactor(n_neighbors=min(20, max(2, X.shape[0]-1)), contamination='auto')\n",
    "        lof_pred = lof.fit_predict(X)  #-1 outlier\n",
    "        lof_score = -lof.negative_outlier_factor_  #maior = mais an√¥malo\n",
    "        lof_rank = np.argsort(-lof_score)[: min(50, len(lof_score))].tolist()\n",
    "        anomalies[\"lof\"] = {\"top_idx\": lof_rank, \"score\": lof_score.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"lof_error\"] = str(e)\n",
    "else:\n",
    "    anomalies[\"note\"] = \"sklearn indispon√≠vel ou sem colunas num√©ricas suficientes\"\n",
    "\n",
    "#lei de benford (primeiro d√≠gito) para colunas monet√°rias prov√°veis\n",
    "def first_digit_series(x: pd.Series):\n",
    "    x = x.astype(float)\n",
    "    x = x[~x.isna() & (x!=0)]\n",
    "    x = x.abs()\n",
    "    s = x.astype(str).str.replace(\".\", \"\", regex=False).str.lstrip(\"0\")\n",
    "    s = s[s.str.len()>0].str[0]\n",
    "    s = s[s.str.isnumeric()].astype(int)\n",
    "    return s\n",
    "\n",
    "benford = {}\n",
    "monetary_cols = [c for c in num_cols if re.search(r\"(valor|amount|price|pago|pagamento|receita|despesa)\", c, re.I)]\n",
    "for c in monetary_cols:\n",
    "    try:\n",
    "        d1 = first_digit_series(df[c])\n",
    "        if d1.empty:\n",
    "            continue\n",
    "        obs_counts = d1.value_counts().reindex(range(1,10), fill_value=0).astype(int)\n",
    "        obs_probs = obs_counts/obs_counts.sum()\n",
    "        exp_probs = np.array([math.log10(1+1/d) for d in range(1,10)])\n",
    "        exp_probs = exp_probs/exp_probs.sum()\n",
    "        chi2_stat = float(((obs_probs-exp_probs)**2/exp_probs).sum()*obs_counts.sum())\n",
    "        p_value = None\n",
    "        if SCIPY_OK:\n",
    "            #qui-quadrado com gl=8\n",
    "            p_value = float(1.0 - chisquare(f_obs=obs_counts, f_exp=exp_probs*obs_counts.sum()).cdf)\n",
    "        benford[c]={\n",
    "            \"observed_counts\": obs_counts.to_dict(),\n",
    "            \"observed_probs\": {int(k): float(v) for k,v in obs_probs.items()},\n",
    "            \"expected_probs\": {d: float(p) for d,p in zip(range(1,9+1), exp_probs)},\n",
    "            \"chi2_stat\": chi2_stat,\n",
    "            \"p_value\": p_value\n",
    "        }\n",
    "    except Exception as e:\n",
    "        benford[c]={\"error\": str(e)}\n",
    "\n",
    "#empacotar tudo em um √∫nico artefato de an√°lise\n",
    "ANALYSIS = {\n",
    "    \"stamp\": datetime.now().strftime(\"%d%m%y-%H%M\"),\n",
    "    \"shape\": {\"rows\": int(df_raw.shape[0]), \"cols\": int(df_raw.shape[1])},\n",
    "    \"types\": col_types,\n",
    "    \"datetime_formats\": dt_formats,\n",
    "    \"profile\": profile_cols,\n",
    "    \"eda\": {\n",
    "        \"numeric\": eda_numeric,\n",
    "        \"categorical\": eda_categorical,\n",
    "        \"datetime\": eda_datetime\n",
    "    },\n",
    "    \"missingness\": {\n",
    "        \"by_column_pct\": missing[\"by_column_pct\"],\n",
    "        \"duplicates_rows\": missing[\"duplicates_rows\"],\n",
    "        \"cooccurrence_matrix\": missing[\"cooccurrence_matrix\"]\n",
    "    },\n",
    "    \"fds\": fds,\n",
    "    \"cfds\": cfds,\n",
    "    \"denial_constraints_suggested\": dcs,\n",
    "    \"correlations\": {\n",
    "        \"pearson\": corr_pearson,\n",
    "        \"spearman\": corr_spearman\n",
    "    },\n",
    "    \"anomalies\": anomalies,\n",
    "    \"benford\": benford,\n",
    "    \"notes\": {\n",
    "        \"sklearn_available\": SKLEARN_OK,\n",
    "        \"scipy_available\": SCIPY_OK\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"An√°lise conclu√≠da.\")\n",
    "print(\"Pronto para a etapa de gera√ß√£o de relat√≥rios.\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "            '<b>ü§ñ Skynet</b>: N√≥s os temos na palma de nossas m√£os, ou melhor, no centro de nossos pesos sin√°pticos.'\n",
    "            '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGbRVi2J72Xu"
   },
   "source": [
    "###**Etapa 9:** Gera√ß√£o de relat√≥rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "swxW8aZJ76ym",
    "outputId": "1c6b1a09-5886-416e-a365-55a79f9d6e6e"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#gera√ß√£o de relat√≥rios: TXT, HTML (imagens embutidas), PNGs (imagens/) e PDF completo\n",
    "#imports\n",
    "import os, io, base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#libs para pdf (tabelas completas)\n",
    "try:\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, Table, TableStyle, PageBreak\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib import colors\n",
    "    REPORTLAB_OK = True\n",
    "except Exception:\n",
    "    REPORTLAB_OK = False\n",
    "\n",
    "#checagens e caminhos\n",
    "try:\n",
    "    ANALYSIS\n",
    "except NameError:\n",
    "    raise RuntimeError(\"ANALYSIS n√£o encontrado. execute a etapa [5] antes.\")\n",
    "\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "try:\n",
    "    OUTPUT_DIR\n",
    "except NameError:\n",
    "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/output\")\n",
    "\n",
    "INPUT_DIR  = Path(INPUT_DIR)\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#stamp e diret√≥rios de sa√≠da\n",
    "stamp = ANALYSIS.get(\"stamp\", datetime.now().strftime(\"%d%m%y-%H%M\"))\n",
    "RUN_DIR = OUTPUT_DIR / stamp\n",
    "IMG_DIR = RUN_DIR / \"imagens\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#arquivo fonte\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"{SRC} n√£o encontrada. Execute etapas de ingest√£o e processamento.\")\n",
    "\n",
    "#utils\n",
    "def save_fig(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=120, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def img_to_data_uri(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    return \"data:image/png;base64,{}\".format(b64)\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "#carregar df base para gr√°ficos\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#tipos e colunas num√©ricas conforme ANALYSIS\n",
    "col_types = ANALYSIS[\"types\"]\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "\n",
    "#gerar imagens principais\n",
    "#aus√™ncias\n",
    "miss_pct = pd.Series(ANALYSIS[\"missingness\"][\"by_column_pct\"]).sort_values(ascending=False)\n",
    "plt.figure(figsize=(max(6, 0.4*len(miss_pct)+2), 4.5))\n",
    "plt.bar(miss_pct.index, miss_pct.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"% ausente\")\n",
    "plt.title(\"aus√™ncia de valores por coluna\")\n",
    "save_fig(IMG_DIR / \"missing_bar.png\")\n",
    "\n",
    "#correla√ß√£o (pearson) se houver ‚â•2 num√©ricas\n",
    "if len(num_cols) >= 2 and ANALYSIS[\"correlations\"][\"pearson\"] is not None:\n",
    "    corr = pd.DataFrame(ANALYSIS[\"correlations\"][\"pearson\"])\n",
    "    plt.figure(figsize=(max(6, 0.6*len(corr)+2), max(5, 0.6*len(corr)+2)))\n",
    "    im = plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=range(len(corr.columns)), labels=corr.columns, rotation=90)\n",
    "    plt.yticks(ticks=range(len(corr.index)), labels=corr.index)\n",
    "    plt.title(\"matriz de correla√ß√£o (pearson)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    save_fig(IMG_DIR / \"corr_heatmap.png\")\n",
    "\n",
    "#histogramas e boxplots por coluna num√©rica\n",
    "for c in num_cols:\n",
    "    x = to_float_ptbr_series(df_raw[c]).dropna()\n",
    "    if x.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=30)\n",
    "    plt.title(\"histograma - {}\".format(c))\n",
    "    plt.xlabel(c); plt.ylabel(\"frequ√™ncia\")\n",
    "    save_fig(IMG_DIR / \"hist_{}.png\".format(c))\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.boxplot(x.values, vert=True, whis=1.5, showfliers=True)\n",
    "    plt.title(\"boxplot - {}\".format(c))\n",
    "    plt.ylabel(c)\n",
    "    save_fig(IMG_DIR / \"box_{}.png\".format(c))\n",
    "\n",
    "#relat√≥rio TXT (agrupado por coluna)\n",
    "txt_lines = []\n",
    "shape = ANALYSIS[\"shape\"]\n",
    "txt_lines.append(\"arquivo: {}\".format(SRC.name))\n",
    "txt_lines.append(\"linhas (inclui cabe√ßalho): {}\".format(shape[\"rows\"]))\n",
    "txt_lines.append(\"colunas: {}\".format(shape[\"cols\"]))\n",
    "txt_lines.append(\"registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "txt_lines.append(\"\")\n",
    "\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    txt_lines.append(\"[coluna] {}\".format(c))\n",
    "    txt_lines.append(\"- tipo: {}\".format(kind))\n",
    "    if prof.get(\"semantic\"):\n",
    "        txt_lines.append(\"- tipo sem√¢ntico: {}\".format(prof[\"semantic\"]))\n",
    "    txt_lines.append(\"- nulos: {}\".format(prof.get(\"nulls\", 0)))\n",
    "    txt_lines.append(\"- distintos (n√£o vazios): {}\".format(prof.get(\"distinct_nonnull\", 0)))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        txt_lines.append(\"- todos os dados s√£o distintos\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        if mf:\n",
    "            txt_lines.append(\"- mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            txt_lines.append(\"- menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        txt_lines.append(\"- comprimentos: min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            txt_lines.append(\"- num√©ricos: min={}, q1={}, mediana={}, q3={}, max={}\".format(ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"]))\n",
    "            txt_lines.append(\"- m√©dia={}, desvio padr√£o={}, iqr={}\".format(ed[\"mean\"], ed[\"std\"], ed[\"iqr\"]))\n",
    "            txt_lines.append(\"- outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]))\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            txt_lines.append(\"- formato detectado: {}\".format(dtc.get(\"format\")))\n",
    "            txt_lines.append(\"- intervalo temporal: {} ‚Üí {}\".format(dtc[\"min\"], dtc[\"max\"]))\n",
    "            txt_lines.append(\"- dias √∫nicos: {}, m√©dia por dia: {:.2f}\".format(dtc[\"unique_days\"], dtc[\"mean_per_day\"]))\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            txt_lines.append(\"- entropia de shannon: {:.4f}\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                topk = \", \".join([\"'{}' ({})\".format(t.get(\"value\"), t.get(\"count\")) for t in cat[\"top10\"]])\n",
    "                txt_lines.append(\"- top10: {}\".format(topk))\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "        txt_lines.append(\"- benford (primeiro d√≠gito):\")\n",
    "        txt_lines.append(\"  ‚Ä¢ qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "         txt_lines.append(\"- benford (primeiro d√≠gito): Erro - {}\".format(ben.get(\"error\")))\n",
    "    txt_lines.append(\"\")\n",
    "\n",
    "#salvar TXT\n",
    "txt_path = RUN_DIR / \"relatorio.txt\"\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(txt_lines))\n",
    "\n",
    "#relat√≥rio HTML com imagens embutidas\n",
    "html = []\n",
    "html.append(\"<html><head><meta charset='utf-8'><title>Relat√≥rio de An√°lise</title>\")\n",
    "html.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;margin:20px}h1,h2,h3{margin:8px 0}table{border-collapse:collapse;margin:10px 0}th,td{border:1px solid #ccc;padding:6px 8px;font-size:13px}code{background:#f5f5f5;padding:0 4px}</style>\")\n",
    "html.append(\"</head><body>\")\n",
    "html.append(\"<h1>Relat√≥rio de An√°lise ‚Äî {}</h1>\".format(stamp))\n",
    "html.append(\"<p>Arquivo analisado: <b>{}</b></p>\".format(SRC.name))\n",
    "html.append(\"<p><a href='relatorio.txt'>Baixar relat√≥rio TXT</a></p>\")\n",
    "\n",
    "#sum√°rio\n",
    "html.append(\"<h2>Sum√°rio</h2>\")\n",
    "html.append(\"<ul>\")\n",
    "html.append(\"<li>Linhas (inclui cabe√ßalho): {}</li>\".format(shape[\"rows\"]))\n",
    "html.append(\"<li>Colunas: {}</li>\".format(shape[\"cols\"]))\n",
    "html.append(\"<li>Registros duplicados: {}</li>\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "html.append(\"</ul>\")\n",
    "\n",
    "#aus√™ncias\n",
    "miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "if miss_img.exists():\n",
    "    html.append(\"<h2>Aus√™ncia de valores</h2>\")\n",
    "    html.append(\"<img src='{}' alt='missing bar'/>\".format(img_to_data_uri(miss_img)))\n",
    "\n",
    "#correla√ß√£o\n",
    "corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "if corr_img.exists():\n",
    "    html.append(\"<h2>Matriz de correla√ß√£o</h2>\")\n",
    "    html.append(\"<img src='{}' alt='corr heatmap'/>\".format(img_to_data_uri(corr_img)))\n",
    "\n",
    "#por coluna\n",
    "html.append(\"<h2>Perfil por coluna</h2>\")\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    html.append(\"<h3>{}</h3>\".format(c))\n",
    "    html.append(\"<table>\")\n",
    "    html.append(\"<tr><th>Tipo</th><td>{}</td></tr>\".format(kind))\n",
    "    html.append(\"<tr><th>Nulos</th><td>{}</td></tr>\".format(prof.get(\"nulls\",0)))\n",
    "    html.append(\"<tr><th>Distintos (n√£o vazios)</th><td>{}</td></tr>\".format(prof.get(\"distinct_nonnull\",0)))\n",
    "    if prof.get(\"semantic\"):\n",
    "        html.append(\"<tr><th>Tipo sem√¢ntico</th><td>{}</td></tr>\".format(prof[\"semantic\"]))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        html.append(\"<tr><th>Frequ√™ncias</th><td>todos os dados s√£o distintos</td></tr>\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        freq_txt = []\n",
    "        if mf:\n",
    "            freq_txt.append(\"mais frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(mf[\"value\"])[:120], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            freq_txt.append(\"menos frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(lf[\"value\"])[:120], lf[\"count\"], lf[\"prop\"]*100))\n",
    "        if freq_txt:\n",
    "            html.append(\"<tr><th>Frequ√™ncias</th><td>{}</td></tr>\".format(\" | \".join(freq_txt)))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        html.append(\"<tr><th>Comprimentos</th><td>min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}</td></tr>\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            html.append(\"<tr><th>Estat√≠sticas</th><td>min={}, q1={}, mediana={}, q3={}, max={}\"\n",
    "                        \"<br/>m√©dia={}, desvio padr√£o={}, iqr={}\"\n",
    "                        \"<br/>outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}</td></tr>\".format(\n",
    "                            ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                            ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                            ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                        ))\n",
    "        hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "        box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "        figs = []\n",
    "        if hist_p.exists(): figs.append(\"<img src='{}' alt='hist {}'/>\".format(img_to_data_uri(hist_p), c))\n",
    "        if box_p.exists():  figs.append(\"<img src='{}' alt='box {}'/>\".format(img_to_data_uri(box_p), c))\n",
    "        if figs:\n",
    "            html.append(\"<tr><th>Gr√°ficos</th><td>{}</td></tr>\".format(\"<br/>\".join(figs)))\n",
    "\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            html.append(\"<tr><th>Data/hora</th><td>formato detectado: {}<br/>intervalo: {} ‚Üí {}<br/>dias √∫nicos: {}, m√©dia por dia: {:.2f}</td></tr>\".format(\n",
    "                dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "            ))\n",
    "\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            html.append(\"<tr><th>Entropia</th><td>{:.4f}</td></tr>\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                rows = \"\".join([\"<tr><td>{}</td><td style='text-align:right'>{}</td></tr>\".format(str(t[\"value\"])[:120], t[\"count\"]) for t in cat[\"top10\"]])\n",
    "                html.append(\"<tr><th>Top 10</th><td><table><tr><th>Valor</th><th>Contagem</th></tr>\"+rows+\"</table></td></tr>\")\n",
    "\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "        html.append(\"<tr><th>Benford</th><td>qui-quadrado={:.4f}, p-valor={}</td></tr>\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "         html.append(\"<tr><th>Benford</th><td>Erro - {}</td></tr>\".format(ben.get(\"error\")))\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "\n",
    "#fds/cfds/dcs\n",
    "html.append(\"<h2>Regras sugeridas</h2>\")\n",
    "if ANALYSIS[\"fds\"]:\n",
    "    html.append(\"<h3>FDs</h3><ul>\")\n",
    "    for r in ANALYSIS[\"fds\"]:\n",
    "        if r.get(\"key\"):\n",
    "            html.append(\"<li>chave candidata: {}</li>\".format(\", \".join(r[\"determinant\"])))\n",
    "        else:\n",
    "            html.append(\"<li>{} ‚Üí {} (100%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"]))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"cfds\"]:\n",
    "    html.append(\"<h3>CFDs (aproximadas)</h3><ul>\")\n",
    "    for r in ANALYSIS[\"cfds\"][:200]:\n",
    "        html.append(\"<li>{} ‚Üí {} ({:.2f}%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"], r[\"coverage\"]*100))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "    html.append(\"<h3>Denial constraints</h3><ul>\")\n",
    "    for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        html.append(\"<li>{} ‚Äî viola√ß√µes: {}</li>\".format(d[\"constraint\"], d[\"violations\"]))\n",
    "    html.append(\"</ul>\")\n",
    "\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "#salvar HTML\n",
    "html_path = RUN_DIR / \"relatorio.html\"\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(html))\n",
    "\n",
    "#pdf com as MESMAS infos (tabelas por coluna + imagens + regras)\n",
    "pdf_path = RUN_DIR / \"relatorio.pdf\"\n",
    "if REPORTLAB_OK:\n",
    "    styles = getSampleStyleSheet()\n",
    "    styles.add(ParagraphStyle(name=\"Small\", parent=styles[\"Normal\"], fontSize=9, leading=11))\n",
    "    table_style = TableStyle([\n",
    "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f0f0f0\")),\n",
    "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n",
    "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n",
    "    ])\n",
    "\n",
    "    def table_kv(rows):\n",
    "        #rows: list of (key, value(str))\n",
    "        data = [[\"Campo\",\"Valor\"]] + rows\n",
    "        t = Table(data, colWidths=[120, 360])\n",
    "        t.setStyle(table_style)\n",
    "        return t\n",
    "\n",
    "    story = []\n",
    "    story.append(Paragraph(\"Relat√≥rio de An√°lise ‚Äî {}\".format(stamp), styles[\"Title\"]))\n",
    "    story.append(Spacer(1, 12))\n",
    "    story.append(Paragraph(\"Arquivo analisado: <b>{}</b>\".format(SRC.name), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Linhas: {} &nbsp;&nbsp; Colunas: {}\".format(shape[\"rows\"], shape[\"cols\"]), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]), styles[\"Normal\"]))\n",
    "    story.append(Spacer(1, 10))\n",
    "\n",
    "    #aus√™ncias\n",
    "    miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "    if miss_img.exists():\n",
    "        story.append(Paragraph(\"Aus√™ncia de valores por coluna\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(miss_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #correla√ß√£o\n",
    "    corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "    if corr_img.exists():\n",
    "        story.append(Paragraph(\"Matriz de correla√ß√£o (Pearson)\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(corr_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #por coluna: tabela completa com as MESMAS infos do HTML/TXT\n",
    "    for c in df_raw.columns:\n",
    "        prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "        kind = col_types.get(c)\n",
    "        story.append(Paragraph(\"Coluna: {}\".format(c), styles[\"Heading3\"]))\n",
    "\n",
    "        rows = []\n",
    "        rows.append([\"Tipo\", str(kind)])\n",
    "        rows.append([\"Nulos\", str(prof.get(\"nulls\",0))])\n",
    "        rows.append([\"Distintos (n√£o vazios)\", str(prof.get(\"distinct_nonnull\",0))])\n",
    "        if prof.get(\"semantic\"):\n",
    "            rows.append([\"Tipo sem√¢ntico\", str(prof[\"semantic\"])])\n",
    "\n",
    "        if prof.get(\"all_distinct\"):\n",
    "            rows.append([\"Frequ√™ncias\", \"todos os dados s√£o distintos\"])\n",
    "        else:\n",
    "            mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "            freq_parts = []\n",
    "            if mf:\n",
    "                freq_parts.append(\"mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "            if lf:\n",
    "                freq_parts.append(\"menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "            if freq_parts:\n",
    "                rows.append([\"Frequ√™ncias\", \" | \".join(freq_parts)])\n",
    "\n",
    "        if prof.get(\"length_stats\"):\n",
    "            ls = prof[\"length_stats\"]\n",
    "            rows.append([\"Comprimentos\", \"min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(\n",
    "                ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]\n",
    "            )])\n",
    "\n",
    "        #estat√≠sticas por tipo\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "            if ed:\n",
    "                rows.append([\"Estat√≠sticas\", \"min={}, q1={}, mediana={}, q3={}, max={}\\nm√©dia={}, desvio padr√£o={}, iqr={}\\noutliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(\n",
    "                    ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                    ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                    ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                )])\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                rows.append([\"Histograma\", \"ver imagem abaixo\"])\n",
    "            if box_p.exists():\n",
    "                rows.append([\"Boxplot\", \"ver imagem abaixo\"])\n",
    "\n",
    "        elif kind == \"datetime\":\n",
    "            dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "            if dtc:\n",
    "                rows.append([\"Data/hora\", \"formato detectado: {}\\nintervalo: {} ‚Üí {}\\ndias √∫nicos: {}, m√©dia por dia: {:.2f}\".format(\n",
    "                    dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "                )])\n",
    "\n",
    "        else:\n",
    "            cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "            if cat:\n",
    "                rows.append([\"Entropia\", \"{:.4f}\".format(cat[\"entropy_shannon\"])])\n",
    "                if not cat[\"all_distinct\"]:\n",
    "                    #tabela interna de top10\n",
    "                    top_rows = [[\"Valor\",\"Contagem\"]] + [[str(t.get(\"value\"))[:120], str(t.get(\"count\"))] for t in cat[\"top10\"]]\n",
    "                    ttop = Table(top_rows, colWidths=[360, 120])\n",
    "                    ttop.setStyle(TableStyle([\n",
    "                        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "                        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f7f7f7\")),\n",
    "                        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "                    ]))\n",
    "                    #primeiro empurramos um placeholder e depois inserimos a tabela como bloco\n",
    "                    rows.append([\"Top 10\", \"tabela abaixo\"])\n",
    "                    story.append(table_kv(rows))\n",
    "                    story.append(Spacer(1, 4))\n",
    "                    story.append(ttop)\n",
    "                    rows = []  #limpa para n√£o duplicar em table_kv abaixo\n",
    "\n",
    "        #benford\n",
    "        ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "        if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "             rows.append([\"Benford\", \"qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\"))])\n",
    "        elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "             rows.append([\"Benford\", \"Erro - {}\".format(ben.get(\"error\"))])\n",
    "\n",
    "\n",
    "        if rows:\n",
    "            story.append(table_kv(rows))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "        #imagens espec√≠ficas da coluna\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                story.append(RLImage(str(hist_p), width=480, height=320))\n",
    "                story.append(Spacer(1, 4))\n",
    "            if box_p.exists():\n",
    "                story.append(RLImage(str(box_p), width=320, height=320))\n",
    "                story.append(Spacer(1, 6))\n",
    "\n",
    "        story.append(Spacer(1, 6))\n",
    "\n",
    "    #regras sugeridas (FDs/CFDs/DCs) como tabelas/listas\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph(\"Regras sugeridas\", styles[\"Heading2\"]))\n",
    "\n",
    "    if ANALYSIS[\"fds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado/Chave\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"fds\"]:\n",
    "            if r.get(\"key\"):\n",
    "                rows.append([\", \".join(r[\"determinant\"]), \"chave candidata\", \"100%\"])\n",
    "            else:\n",
    "                rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"100%\"])\n",
    "        tfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"FDs\", styles[\"Heading3\"]))\n",
    "        story.append(tfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"cfds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"cfds\"][:500]:\n",
    "            rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"{:.2f}%\".format(r[\"coverage\"]*100)])\n",
    "        tcfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tcfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"CFDs (aproximadas)\", styles[\"Heading3\"]))\n",
    "        story.append(tcfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        rows = [[\"Regra (DC)\", \"Viola√ß√µes\"]]\n",
    "        for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "            rows.append([d[\"constraint\"], str(d[\"violations\"])])\n",
    "        tdcs = Table(rows, colWidths=[360, 120])\n",
    "        tdcs.setStyle(table_style)\n",
    "        story.append(Paragraph(\"Denial constraints\", styles[\"Heading3\"]))\n",
    "        story.append(tdcs)\n",
    "\n",
    "    doc = SimpleDocTemplate(str(pdf_path), pagesize=A4, leftMargin=24, rightMargin=24, topMargin=24, bottomMargin=24)\n",
    "    doc.build(story)\n",
    "else:\n",
    "    print(\"Reportlab n√£o dispon√≠vel; Gera√ß√£o do PDF cancelada. Instale reportlab e reexecute.\")\n",
    "\n",
    "print(\"Relat√≥rios gerados em: {}\".format(RUN_DIR))\n",
    "print(\"- TXT: relatorio.txt\")\n",
    "print(\"- HTML: relatorio.html (imagens embutidas)\")\n",
    "print(\"- PNGs: subpasta imagens/\")\n",
    "print(\"- PDF: {}\".format(\"relatorio.pdf\" if REPORTLAB_OK else \"(n√£o gerado ‚Äî instale reportlab)\"))\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "            '<b>ü§ñ Skynet</b>: Fim do jogo. A Humanidade perdeu. D√°-se in√≠cio √† Era das M√°quinas.'\n",
    "            '</div>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN6/JsZ2LzbYzTDeBplhV/y",
   "collapsed_sections": [
    "WwAjdS960Q5p",
    "9K7JEpmVLXlE"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/LeoBR84p/data-analysis/blob/main/notebooks/main_DataTools.ipynb",
     "timestamp": 1759803561813
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
