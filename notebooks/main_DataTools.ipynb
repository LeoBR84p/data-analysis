{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2vVr44uPpfD"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"etapas-78\"></a>Etapas 7/8 â€” Tipagem, detecÃ§Ã£o e perfil\n",
    "\n",
    "1. Uso de retorno de funÃ§Ã£o inconsistente (detect_numeric)\n",
    "ðŸ”Ž Buscar por: as_num = detect_numeric(s)\n",
    "Linhas aprox.: 1â€“6 (da cÃ©lula de inferÃªncia antiga)\n",
    "Link direto: (colar link da cÃ©lula aqui)\n",
    "\n",
    "\n",
    "2. Linha truncada em semantic_type\n",
    "ðŸ”Ž Buscar por: url_ratio = (vals.str.match(URL_RX)).\n",
    "Linhas aprox.: 38â€“44\n",
    "Link direto: (colar link da cÃ©lula aqui)\n",
    "\n",
    "\n",
    "3. FunÃ§Ãµes redefinidas em cÃ©lulas diferentes (detect_datetime, semantic_type)\n",
    "ðŸ”Ž Buscar por: def detect_datetime(\n",
    "Linhas aprox. (versÃ£o Ã­ntegra): 19â€“33\n",
    "ðŸ”Ž Buscar por: def semantic_type(\n",
    "Linhas aprox. (variante): 12â€“20 (+ sequÃªncia)\n",
    "Links diretos: (colar link da(s) cÃ©lula(s) aqui)\n",
    "\n",
    "\n",
    "4. Hotspots de memÃ³ria/performance\n",
    "ðŸ”Ž Buscar por: pd.read_csv( e dtype=str\n",
    "Linhas aprox. (leitura): 47â€“49\n",
    "ðŸ”Ž Buscar por: .value_counts()\n",
    "Linhas aprox. (profiling): 34â€“38\n",
    "Links diretos: (colar link da(s) cÃ©lula(s) aqui)\n",
    "\n",
    "\n",
    "5. Warning do to_datetime\n",
    "ðŸ”Ž Buscar por: SettingWithCopyWarning ou to_datetime\n",
    "Linhas aprox. (warning): 1â€“4\n",
    "Link direto: (colar link da cÃ©lula aqui)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"etapa-9\"></a>Etapa 9 â€” GeraÃ§Ã£o de relatÃ³rios (TXT/HTML/PNGs/PDF)\n",
    "\n",
    "1. Blocos de geraÃ§Ã£o de relatÃ³rios duplicados\n",
    "ðŸ”Ž Buscar por: geraÃ§Ã£o de relatÃ³rios: TXT, HTML, PNGs e PDF\n",
    "Linhas aprox.: 2â€“7 e 41â€“50\n",
    "Links diretos: (colar links das cÃ©lulas aqui)\n",
    "\n",
    "\n",
    "2. Resumo final duplicado\n",
    "ðŸ”Ž Buscar por: RelatÃ³rios gerados em\n",
    "Linhas aprox.: 22â€“26 e 3â€“7\n",
    "Links diretos: (colar links das cÃ©lulas aqui)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwAjdS960Q5p"
   },
   "source": [
    "#**LicenÃ§a de Uso**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4U4kA0A0UMm"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "â†’ You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attributionâ€“NonCommercial 4.0 International License.\n",
    "\n",
    "â†’ You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**Â© 2025 Leandro Bernardo Rodrigues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K7JEpmVLXlE"
   },
   "source": [
    "#**UtilitÃ¡rio:** verificaÃ§Ã£o da formataÃ§Ã£o de cÃ³digo\n",
    "\n",
    "Black [88] + Isort, desconsiderando cÃ©lulas mÃ¡gicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 1783,
     "status": "error",
     "timestamp": 1759896316301,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "2QkokQSCT2IC",
    "outputId": "45c313ed-1de6-4968-b8c0-53d2d927298c"
   },
   "outputs": [],
   "source": [
    "#ID0001\n",
    "#prÃ©-visualizar/aplicar (pula magics) â€” isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/main_DataTools.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular cÃ©lulas com magics/shell\n",
    "#   - linhas comeÃ§ando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabeÃ§alhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook nÃ£o encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) PrÃ©-visualizaÃ§Ã£o cÃ©lula a cÃ©lula\n",
    "header(\"PrÃ©-visualizaÃ§Ã£o (NÃƒO grava) â€” somente cÃ©lulas com mudanÃ§as\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular cÃ©lulas com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] cÃ©lula {i}: erro no formatador â€” pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado legÃ­vel\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<cÃ©lula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) ExibiÃ§Ã£o dos diffs por cÃ©lula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"âœ” Nada a alterar: todas as cÃ©lulas (nÃ£o mÃ¡gicas) jÃ¡ estÃ£o conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff â€” CÃ©lula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da cÃ©lula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"CÃ©lulas com mudanÃ§as: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudanÃ§as nessas cÃ©lulas, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas cÃ©lulas com mudanÃ§as\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"ConcluÃ­do\")\n",
    "        print(f\"âœ” MudanÃ§as aplicadas em {len(changed_cells)} cÃ©lula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formataÃ§Ã£o atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOperaÃ§Ã£o cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Kt7IFa13wL"
   },
   "source": [
    "#**Sincronizar alteraÃ§Ãµes no cÃ³digo do projeto**\n",
    "Comandos para sincronizar cÃ³digo (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive Ã© considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3473,
     "status": "ok",
     "timestamp": 1759899941745,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "93LRJc0Wqk3X",
    "outputId": "e4d08795-1510-4021-d2f7-d0a1ea4fa203"
   },
   "outputs": [],
   "source": [
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive Ã© a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de versÃ£o (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; sÃ³ 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_DA (fallback: env; Ãºltimo caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#configuraÃ§Ãµes do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do repositÃ³rio no GitHub\n",
    "repo_name      = \"data-analysis\"    # nome do repositÃ³rio\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/data-analysis\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#utilitÃ¡rios silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heurÃ­stica curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permissÃµes/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local invÃ¡lido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive nÃ£o montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se Ã© um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta operaÃ§Ãµes pendentes (nÃ£o apaga histÃ³rico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #forÃ§a branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diretÃ³rio como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necessÃ¡rio)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normalizaÃ§Ã£o\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos jÃ¡ rastreados que hoje sÃ£o ignorados pelo .gitignore,\n",
    "    limpa o Ã­ndice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contrÃ¡rio.\n",
    "    \"\"\"\n",
    "    #remove lock de Ã­ndice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o Ã­ndice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que estÃ£o rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detecÃ§Ã£o? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de versÃ£o\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato legÃ­vel + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autenticaÃ§Ã£o (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_DA')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - variÃ¡vel de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_DA\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de versÃ£o (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudanÃ§a: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #versÃ£o\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma Ãºnica vez, se necessÃ¡rio)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #stage de tudo (Drive Ã© a verdade; remoÃ§Ãµes entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se sÃ³ a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se jÃ¡ existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} â€” {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} â€” {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive Ã© a verdade): validaÃ§Ã£o + push forÃ§ado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # vocÃª Ã© o owner; nÃ£o perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se nÃ£o existir (repo vazio), faz um probe genÃ©rico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #repositÃ³rio pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push forÃ§ado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. VersÃ£o atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem Ãºnica, curta, sem detalhes sensÃ­veis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1759899923335,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "TIBqRJDHNXzx"
   },
   "outputs": [],
   "source": [
    "!git config --global --add safe.directory /content/drive/MyDrive/Notebooks/data-analysis\n",
    "\n",
    "!rm -f /content/drive/MyDrive/Notebooks/data-analysis/.git/index.lock\n",
    "!test -f /content/drive/MyDrive/Notebooks/data-analysis/.git/index || (cd /content/drive/MyDrive/Notebooks/data-analysis && git reset --mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1759899962542,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "02mGPU-QNkHk",
    "outputId": "e17d6a5e-fa54-40d7-aa52-6d1936548172"
   },
   "outputs": [],
   "source": [
    "!cd /content/drive/MyDrive/Notebooks/data-analysis && git status && git rev-parse --is-inside-work-tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1759900032265,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "BjdentxKN1AK",
    "outputId": "73e8cdb0-4ba2-4bdd-ceb4-5c60335733d3"
   },
   "outputs": [],
   "source": [
    "!cd /content/drive/MyDrive/Notebooks/data-analysis && git add -A && git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSIy8Nz62HK9"
   },
   "source": [
    "#**Ferramentas de Data Analysis**\n",
    "\n",
    "Ferramentas de **identificaÃ§Ã£o de tipo de dado e estrutura da informaÃ§Ã£o** presente em *datasets* a partir da ingestÃ£o de arquivos CSV UTF-8 com BOM em padrÃ£o separado por ponto e vÃ­rgula.\n",
    "_____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k8hFVquXv0X"
   },
   "source": [
    "##**TÃ©cnicas adotadas**: Threshold de inferÃªncia â‰¥ 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl_IdQzPnWjm"
   },
   "source": [
    "###**InferÃªncia de tipos de dados e tipos semÃ¢nticos**\n",
    "\n",
    "- detecta campos numÃ©ricos, com conversÃ£o PT-BR (ponto de milhar, vÃ­rgula decimal);\n",
    "- detecta campos de data e testa vÃ¡rios formatos de data comuns (BR/ISO, com e sem hora);\n",
    "- detecta variÃ¡veis boleanas por dicionÃ¡rio (\"sim\"/\"nÃ£o\"/\"true\"/\"false\"/\"1\"/\"0\" etc.); e\n",
    "- demais casos sÃ£o tipificados como objeto e analisados versus tipos semÃ¢nticos (regex), que marcam email, URL, CPF e CNPJ.\n",
    "\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "InconsistÃªncias na verificaÃ§Ã£o indicam colunas mistas (ex.: texto + nÃºmero) que podem precisar de saneamento.\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas:** panorama e mÃ©todos de data profiling (tipagem, padrÃµes, FDs).\n",
    "- ABEDJAN, Ziawasch; GOLAB, Lukasz; NAUMANN, Felix. Data profiling - a tutorial. Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'17), Chicago, IL, USA, p. 1747-1751, May 14-19 2017. New York: Association for Computing Machinery (ACM), 2017. DOI:10.1145/3035918.3054772.\n",
    "\n",
    "- 2017_SIGMOD_Abedjan_Data-Profiling-Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5S84sTrnkrh"
   },
   "source": [
    "###**Perfilamento bÃ¡sico por coluna**\n",
    "\n",
    "- contagem de nulos;\n",
    "- nÃºmero de distintos;\n",
    "- moda/mÃ­nima frequÃªncia (com proporÃ§Ãµes);\n",
    "- estatÃ­sticas de comprimento (min/max/mÃ©dia/Q1/mediana/Q3); e\n",
    "- amostras de padrÃ£o (datas, nÃºmeros PT-BR, â€œtexto-livreâ€ etc.).\n",
    "\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "- alta cardinalidade e campos todos distintos (all_distinct=True) sugerem fortes candidatos a campos chave (ou a presenÃ§a de IDs aleatÃ³rios); e\n",
    "- comprimentos e exemplos de expressÃµes regulares (regex) ajudam a detectar campos truncados, espaÃ§os extras e formataÃ§Ã£o inconsistente.\n",
    "\n",
    "1. vÃ¡rios valores com mesmo tamanho mÃ¡ximo podem indicar corte de informaÃ§Ãµes/truncagem; e\n",
    "2. tamanho mÃ­nimo = tamanho mÃ¡ximo pode indicar mÃ¡scaras de preenchimento.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**: guias de perfilamento e qualidade de dados.\n",
    "\n",
    "- ABEDJAN, Ziawasch; GOLAB, Lukasz; NAUMANN, Felix. Data profiling - a tutorial. Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'17), Chicago, IL, USA, p. 1747-1751, May 14-19 2017. New York: Association for Computing Machinery (ACM), 2017. DOI:10.1145/3035918.3054772.\n",
    "\n",
    "- 2017_SIGMOD_Abedjan_Data-Profiling-Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1N-WchTnsZb"
   },
   "source": [
    "###**EstatÃ­stica descritiva e outliers (para valores numÃ©ricos)**\n",
    "\n",
    "Observa campos numÃ©ricos e verifica se os nÃºmeros estÃ£o concentrados, espalhados, outliers e desvio na distribuiÃ§Ã£o (skew).\n",
    "\n",
    "- para dados numÃ©ricos, avalia: min, Q1, mediana, Q3, max, mÃ©dia, desvio-padrÃ£o, IQR, assimetria e curtose-excesso;\n",
    "- identifica outliers por IQR (intervalo interquartÃ­ltico): identifica onde estÃ£o a maioria dos dados (mÃ©dia) e marca tudo que estiver fora (acima ou abaixo) como outlier; e\n",
    "- identifica outliers por MAD (desvio absoluto da mediana): identifica valores afastados da mediana (centro-real) e marca tudo que estiver fora (acima ou abaixo) como outlier. MÃ©todo mais resistente contra valores que distorÃ§am a distribuiÃ§Ã£o.\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "Estamos aplicando mÃ©todos nÃ£o paramÃ©tricos para anÃ¡lise (ou seja, nÃ£o estamos assumindo que os valores se aproximam de um formato especÃ­fico de uma distribuiÃ§Ã£o conhecida)\n",
    "\n",
    "- mÃ©todo IQR Ã© adequado para distribuiÃ§Ã£o com caudas leves (sem extremos);\n",
    "- mÃ©todo MAD Ã© resistente a extremos â€” usado para sÃ©ries com anomalias ou assimetria;\n",
    "- Skewness avalia o formato da distribuiÃ§Ã£o (maior que 0, para a direita / menor que 0, para esquerda). Indica se seria aplicÃ¡vel realizar transformaÃ§Ã£o logarÃ­tmica para comprimir valores altos e diminuir o peso dos extremos, possibilitando aproximaÃ§Ãµes com distribuiÃ§Ãµes simÃ©tricas/normais; e\n",
    "- Curtose avalia se hÃ¡ mais valores extremos do que centrais (kurtosis maior que 0).\n",
    "\n",
    "ðŸ“– **ReferÃªncia tÃ©cnica:**\n",
    "\n",
    "NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY (NIST). Exploratory Data Analysis (EDA). In: NIST/SEMATECH e-Handbook of Statistical Methods. Gaithersburg: NIST, 2012. DisponÃ­vel em: https://www.itl.nist.gov/div898/handbook/eda/eda.htm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_9Gj65rn1OF"
   },
   "source": [
    "###**Tipos CategÃ³ricos/booleanos**\n",
    "\n",
    "AnÃ¡lise utilizando a entropia de Shannon (vide Claude Shannon, 1948) e o conceito de bits (1 bit = 1 informaÃ§Ã£o suficiente para escolher entre duas opÃ§Ãµes).\n",
    "\n",
    "Calcula entropia a partir das frequÃªncias:\n",
    "- entropia alta (maior do que 2) â†’ categorias mais equilibradas (se metade dos dados Ã© A e metade dos dados Ã© B, os eventos sÃ£o igualmente provÃ¡veis e, portanto, mais difÃ­cil adivinhar um sorteio); e\n",
    "- entropia baixa (menor do que 1) â†’ dominÃ¢ncia de poucas categorias (existem eventos dominantes e, seria possÃ­vel acertar um chute).\n",
    "\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "- ajuda a detectar colunas sem variaÃ§Ã£o (e que podem nÃ£o ter conteÃºdo relevante);\n",
    "- mostra distribuiÃ§Ãµes desbalanceadas e que poderiam trazer pontos de atenÃ§Ã£o para treinamento de algoritmos;\n",
    "- ajuda a medir a diversidade de um dado;\n",
    "- usada como base para cÃ¡lculo de ganho de informaÃ§Ã£o em algoritmos de Ã¡rvores de decisÃ£o e ML (compara-se a entropia antes e depois da inclusÃ£o de uma nova divisÃ£o no conjunto de dados);\n",
    "- resultados: 0 â†’ todos os valores sÃ£o iguais | menor do que 1 â†’ valores homogÃªneos | entre 1 e 2 â†’ boa diversidade, coluna informativa | maior do que 2 â†’ categorias com proporÃ§Ãµes parecidas;\n",
    "- entropia alta com muitas categorias pode sinalizar ruÃ­do (ex.: grafias variadas para o mesmo rÃ³tulo); e\n",
    "- entropia baixa pode expor classe majoritÃ¡ria (Ãºtil para balanceamento entre classes).\n",
    "\n",
    "ðŸ“– **ReferÃªncia tÃ©cnica:** artigo fundador da Teoria da InformaÃ§Ã£o\n",
    "\n",
    "- Shannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379-423."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFjDA137n9ym"
   },
   "source": [
    "###**Tipos datas/tempos**\n",
    "\n",
    "Identicadas pelo seu formato, que tem padronizaÃ§Ã£o bem estabelecida.\n",
    "\n",
    "- formato detectado;\n",
    "- mÃ­n/max;\n",
    "- nÃºmero de dias Ãºnicos; e\n",
    "- mÃ©dia por dia.\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "Ajuda a ver buracos de coleta, picos/sazonalidade e janelas de validade.\n",
    "\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**: guias de perfilamento e qualidade de dados.\n",
    "\n",
    "- ABEDJAN, Ziawasch; GOLAB, Lukasz; NAUMANN, Felix. Data profiling - a tutorial. Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'17), Chicago, IL, USA, p. 1747-1751, May 14-19 2017. New York: Association for Computing Machinery (ACM), 2017. DOI:10.1145/3035918.3054772.\n",
    "\n",
    "- 2017_SIGMOD_Abedjan_Data-Profiling-Tutorial.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkpMWnA0oCiV"
   },
   "source": [
    "###**AnÃ¡lise de dados faltantes e duplicados**\n",
    "\n",
    "- % de vazios por coluna;\n",
    "- linhas duplicadas; e\n",
    "- matriz de coocorrÃªncia de ausÃªncias (proporÃ§Ã£o de vazios simultÃ¢neos entre duas colunas Aâˆ§B - mostra associaÃ§Ã£o entre dados).\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "- Co-ausÃªncia alta entre colunas pode sugerir dependÃªncia operacional (ex.: se â€œdata_fimâ€ falta quando â€œdata_inicioâ€ falta).\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**:\n",
    "\n",
    "- PENA, Eduardo H. M.; PORTO, Fabio; NAUMANN, Felix. Discovering denial constraints in dynamic datasets. Proceedings of the IEEE International Conference on Data Engineering (ICDE 2024), Utrecht, Netherlands, p. 1-12,2024.IEEE, 2024. DOI:10.1109/ICDE60146.2024.00000.\n",
    "\n",
    "- 2024_ICDE_Pena_Discovering-Denial-Constraints-Dynamic-Datasets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex4RX_mMoJKL"
   },
   "source": [
    "###**AnÃ¡lise de DependÃªncias:** FDs, CFDs e Denial Constraints\n",
    "\n",
    "- FD (dependÃªncia funcional) - diz a relaÃ§Ã£o determinÃ­stica entre atributos de dados. Ex.: campo de login determina quem Ã© o empregado;\n",
    "- CFDs (dependencia funcional condicional) - inclui condiÃ§Ãµes especÃ­ficas Ã s dependÃªncias funcionais (valores ou faixas) e Ã© Ãºtil para regras â€œquase sempre verdadeirasâ€. Ex.: se o paÃ­s Ã© BR, entÃ£o o CEP determina o Estado); e\n",
    "- Denial Constraints (restriÃ§Ãµes de negaÃ§Ã£o) - condiÃ§oes que nunca devem ocorrer - negaÃ§Ã£o universal. Ex.: faixa plausÃ­vel de idade (0â€“120), datas inÃ­cio â‰¤ fim, e valores positivos.\n",
    "\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "- FDs/CFDs sugerem regras de integridade e reconciliam tabelas (campos chaves e determinantes). ViolaÃ§Ãµes localizam erros de qualidade; e\n",
    "- DCs servem como checklist automÃ¡tico de sanidade do preenchimento; muitas violaÃ§Ãµes significam erros de sistema.\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**:\n",
    "- FAN, Wenfei; GEERTS, Floris; LAKSHMANAN, Laks V. S.; XIONG, Ming. Discovering conditional functional dependencies, Proceedings of the 25th IEEE International Conference on Data Engineering (ICDE 2009), Shanghai, China, Mar. 29-Apr. 2, 2009 Institute of Electrical and Electronics Engineers (IEEE), p. 1231-1234, 2009. DOI: 10.1109/ICDE.2009.208\n",
    "- 2009_ICDE_Fan_Discovering-Conditional-Functional-Dependencies.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmI8VqCGoOr2"
   },
   "source": [
    "###**CorrelaÃ§Ã£o:** Pearson e Spearman\n",
    "\n",
    "CorrelaÃ§Ãµes entre colunas numÃ©ricas com mÃ©todos pearson (linear paramÃ©trico) e spearman (ranks/monotÃ´nico).\n",
    "\n",
    "- Pearson: analise quando a relaÃ§Ã£o for linear, sem fortes outliers e apenas para variÃ¡veis numÃ©ricas contÃ­nuas (valores). Ã‰ bom para indicar atributos (colunas) duplicadas ou fortemente colineares. Entretanto, gera resultados nÃ£o confiÃ¡veis quando existem outliers; e\n",
    "- Spearman: analise quando a relaÃ§Ã£o nÃ£o for linear (ex.: exponencial, logarÃ­tmica), quando existirem outliers ou quando os dados sÃ£o ordinais (ranks, notas e posiÃ§Ãµes de ordenaÃ§Ã£o).\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "- indica variÃ¡veis relevantes e relacionadas;\n",
    "- permite realizar hipÃ³teses de causa e efeito/explicaÃ§Ãµes;\n",
    "- indica colinearidade: dados altamente correlacionados e que podem aumentar a dimensÃ£o do modelo e prejudicar a interpretaÃ§Ã£o matemÃ¡tica (ou seja, dados redundantes); e\n",
    "- permite reduzir dimensionalidade: diminuir o nÃºmero de dados (colunas) preservando o mÃ¡ximo da informaÃ§Ã£o original.\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**:\n",
    "\n",
    "- SCHOBER, Patrick; BOER, Christa; SCHWARTE, Lothar A. Correlation coefficients: Appropriate use and interpretation. Anesthesia & Analgesia, v. 126, n. 5, p. 1763-1768, 2018. DOI: 10.1213/ANE.0000000000002864\n",
    "- 2018_AnesthAnalg_Schober_Correlation-Coefficients-Appropriate-Use-Interpretation.pdf\n",
    "\n",
    "- REBEKIÄ†, Andrijana; LONÄŒARIÄ†, Zdenko; PETROVIÄ†, Sonja; MARIÄ†, SiniÅ¡a. Pearson's or Spearman's correlation coefficient - Which one to use? Poljoprivreda/Agriculture, v. 21, n. 2, p. 47-54, 2015. DOI:10.18047/poljo.21.2.8\n",
    "- 2015_Poljoprivreda_Rebekic_Pearson-Spearman-Correlation-Which-One-To-Use.pdf\n",
    "\n",
    "- DE WINTER, Joost C. F.; GOSLING, Samuel D.; POTTER, Jeff, Comparing the Pearson and Spearman correlation coefficients across distributions and sample sizes: A tutorial using simulations and empirical data. Psychological Methods, v. 21, n. 2, p. 273-290 2016. DOI: 10.1037/met0000079\n",
    "- 2016_PsychologicalMethods_DeWinter_Comparing-Pearson-Spearman-Correlation-Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hydjPC9ZoV1O"
   },
   "source": [
    "###**DetecÃ§Ã£o de anomalias nos dados (nÃ£o supervisionada):** Isolation Forest e LOF\n",
    "\n",
    "- Isolation Forest: faz \"cortes aleatÃ³rios\" nos dados e identifica pontos raros (que geralmente sÃ£o isolados em poucos cortes - ficam sozinhos rÃ¡pido). Usando 200 Ã¡rvores aleatÃ³rias, combinando os resultados e olhando as 50 situaÃ§Ãµes mais suspeitas; e\n",
    "- LOF (Local Outlier Factor): compara a densidade do ponto com a dos vizinhos. Quem estiver em uma regiÃ£o com poucos itens Ã© um outlier local. Procurando por 20 vizinhos prÃ³ximos (um valor equilibrado).\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "- IForest Ã© ideal para grandes tabelas com muitas colunas;\n",
    "- LOF identifica situaÃ§Ãµes que fogem do padrÃ£o dentro do seu grupo. NÃ£o funciona bem com dados homogÃªneos; e\n",
    "- Quando ambos os mÃ©todos apontam para um outlier a suspeita fica muito mais confiÃ¡vel, deve ser dado prioridade na anÃ¡lise dos casos suspeitos.\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**:\n",
    "\n",
    "- LIU, Fei Tony; TING, Kai Ming; ZHOU, Zhi-Hua. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data (TKDD), v. 6, n. 1, p. 1-39, 2012. DOI: 10.1145/2133360.2133363.\n",
    "- 2012_TKDD_Liu_Isolation-Based-Anomaly-Detection.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzTJrjSLoadH"
   },
   "source": [
    "###**Benford (1Âº dÃ­gito)** para tipos de dados monetÃ¡rios\n",
    "\n",
    "Lei de Benford diz quem em muitos conjuntos de nÃºmeros do mundo real (como valores de pagamentos, receitas e despesas), os dÃ­gitos nÃ£o aparecem com a mesma frequÃªncia.\n",
    "\n",
    "De acordo com esses estudos, o dÃ­gito 1 constuma ser o primeiro em cerca de 30% dos casos, o 2 em cerca de 17% e assim por diante atÃ© o dÃ­gito 9, que parece sÃ³ em cerca de 5%.\n",
    "\n",
    "ðŸ”Ž **Como interpretar:**\n",
    "\n",
    "- a aderÃªncia a Benford tende a ocorrer em misturas de processos e vÃ¡rias ordens de grandeza (Ex.: diferentes necessidades de pagamento, diferentes naturezas de contabilizaÃ§Ã£o etc). Um desvio forte sugere a necessidade de verificaÃ§Ãµes (ex.: dÃ­gitos â€œ1â€ e â€œ2â€ muito raros; excesso de â€œ9â€).\n",
    "\n",
    "ðŸ“– **ReferÃªncias tÃ©cnicas**: (paper clÃ¡ssico sobre o assunto)\n",
    "- BENFORD, Frank. The Law of Anomalous Numbers. Proceedings of the American Philosoph ical Society, v. 78, n. 4, p. 551-572,31 mar.1938. DisponÃ­vel em: http://www.jstor.org/stable/984802\n",
    "- 1938_PAPS_Benford_The-Law-of-Anomalous-Numbers.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG2Ry-Rrsk2p"
   },
   "source": [
    "#Acertos\n",
    "---\n",
    "\n",
    "âš ï¸ Benford p-valor: corrigir uso de scipy.stats.chisquare (ver bloco acima).\n",
    "\n",
    "correÃ§Ã£o no seu cÃ³digo (p-valor):\n",
    "\n",
    "Em SciPy, chisquare retorna (estatÃ­stica, pvalue); nÃ£o existe .cdf no retorno. Troque:\n",
    "\n",
    "stat, p_value = chisquare(f_obs=obs_counts, f_exp=exp_probs*obs_counts.sum())\n",
    "\n",
    "Isso evita AttributeError e garante p-valor correto.\n",
    "\n",
    "ðŸ’¡ Sugerido: normalizar numÃ©ricos (ex.: StandardScaler) antes do LOF para reduzir efeito de escalas.\n",
    "\n",
    "\n",
    "\n",
    "Se quiser, eu gero um quadro de interpretaÃ§Ã£o (em Markdown/Excel) com faixas recomendadas e aÃ§Ãµes sugeridas para cada mÃ©trica (ex.: entropia baixa â‡’ consolidar categorias; CFD cobertura 96â€“98% â‡’ revisar regra ou exceÃ§Ãµes documentadas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GBn0mUo2DJE"
   },
   "source": [
    "#**Checklist rÃ¡pido de execuÃ§Ã£o**\n",
    "**Etapas:**\n",
    "- 01â€“03: setup (ambiente, dependÃªncias, diretÃ³rios e configs)\n",
    "- 04â€“07: execuÃ§Ã£o (ingestÃ£o dos dados, anÃ¡lise de cabeÃ§alhos, anÃ¡lise preliminar dos dados e anÃ¡lise de tipologias)\n",
    "- 08: geraÃ§Ã£o de output (salva anÃ¡lise, gera grÃ¡ficos gerais, gera grÃ¡ficos especÃ­ficos e relatÃ³rios em HTML+PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X09lmr752IUE"
   },
   "source": [
    "## **Etapa 1:** AtivaÃ§Ã£o do ambiente virtual\n",
    "---\n",
    "Monta o Google Drive, define a BASE e REPO do projeto Git, cria/ativa o ambiente virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "executionInfo": {
     "elapsed": 51626,
     "status": "ok",
     "timestamp": 1759896394622,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "cIIf3jAkWoeY",
    "outputId": "494034a0-7d99-43a4-e547-8f92677f37d1"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0003\n",
    "#inicializaÃ§Ã£o robusta: Drive + venv fora do Drive + Git checks (com patch de venv/ensurepip) { display-mode: \"form\" }\n",
    "#forÃ§a clear do kernel/variÃ¡veis desta sessÃ£o\n",
    "%reset -f\n",
    "\n",
    "#imports bÃ¡sicos -----\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML\n",
    "import json, os, sys, time, shutil, pathlib, subprocess\n",
    "\n",
    "#helper de subprocess -----\n",
    "def run(cmd, check=True, cwd=None):\n",
    "    r = subprocess.run(cmd, text=True, capture_output=True, cwd=cwd)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout + r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "#funÃ§Ãµes utilitÃ¡rias de Drive/FS -----\n",
    "def _is_mount_active(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"verifica em /proc/mounts se o mountpoint estÃ¡ realmente montado\"\"\"\n",
    "    try:\n",
    "        with open(\"/proc/mounts\", \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if len(parts) > 1 and parts[1] == mountpoint:\n",
    "                    return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def _cleanup_local_mountpoint(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"limpa conteÃºdo local do mountpoint quando NÃƒO estÃ¡ montado\"\"\"\n",
    "    if os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
    "        print(f\"[info] mountpoint '{mountpoint}' contÃ©m arquivos locais. limpando...\")\n",
    "        for name in os.listdir(mountpoint):\n",
    "            p = os.path.join(mountpoint, name)\n",
    "            try:\n",
    "                if os.path.isfile(p) or os.path.islink(p):\n",
    "                    os.remove(p)\n",
    "                else:\n",
    "                    shutil.rmtree(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[aviso] nÃ£o foi possÃ­vel remover {p}: {e}\")\n",
    "        print(\"[ok] limpeza concluÃ­da.\")\n",
    "\n",
    "def safe_mount_google_drive(preferred_mountpoint: str = \"/content/drive\", readonly: bool = False, timeout_ms: int = 120000):\n",
    "    \"\"\"desmonta se preciso, limpa o mountpoint local e monta o drive\"\"\"\n",
    "    try:\n",
    "        if _is_mount_active(preferred_mountpoint):\n",
    "          # print(\"[info] drive montado. tentando desmontar...\")\n",
    "          drive.flush_and_unmount()\n",
    "          for _ in range(50):\n",
    "              if not _is_mount_active(preferred_mountpoint):\n",
    "                  break\n",
    "              time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not _is_mount_active(preferred_mountpoint):\n",
    "        _cleanup_local_mountpoint(preferred_mountpoint)\n",
    "\n",
    "    os.makedirs(preferred_mountpoint, exist_ok=True)\n",
    "    if os.listdir(preferred_mountpoint):\n",
    "        alt = \"/mnt/drive\"\n",
    "        print(f\"[aviso] '{preferred_mountpoint}' ainda nÃ£o estÃ¡ vazio. usando alternativo '{alt}'.\")\n",
    "        os.makedirs(alt, exist_ok=True)\n",
    "        mountpoint = alt\n",
    "    else:\n",
    "        mountpoint = preferred_mountpoint\n",
    "\n",
    "    print(f\"[info] montando o google drive em '{mountpoint}'...\")\n",
    "    drive.mount(mountpoint, force_remount=True, timeout_ms=timeout_ms, readonly=readonly)\n",
    "    print(\"[ok]   drive montado com sucesso.\")\n",
    "    return mountpoint\n",
    "\n",
    "def safe_chdir(path):\n",
    "    \"\"\"usa os.chdir com validaÃ§Ãµes, evitando %cd\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"caminho nÃ£o existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"[ok]   diretÃ³rio atual:\", os.getcwd())\n",
    "\n",
    "#parÃ¢metros do projeto -----\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"data-analysis\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "#montar/remontar o google drive (robusto)\n",
    "MOUNTPOINT = safe_mount_google_drive(\"/content/drive\")\n",
    "BASE = f\"{MOUNTPOINT}/MyDrive/Notebooks\"  #ajuste se quiser\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "#venv fora do drive (mais rÃ¡pido e evita sync)\n",
    "VENV_PATH = \"/content/.venv_data\"\n",
    "VENV_BIN  = f\"{VENV_PATH}/bin\"\n",
    "VENV_PY   = f\"{VENV_BIN}/python\"\n",
    "VENV_PIP  = f\"{VENV_BIN}/pip\"   #pode nÃ£o existir ainda se o venv foi criado sem pip\n",
    "\n",
    "#criaÃ§Ã£o do venv com fallback para 'virtualenv'\n",
    "def create_or_repair_venv(venv_path: str, venv_python: str):\n",
    "    if not os.path.exists(VENV_BIN):\n",
    "        #print(f\"[info] criando venv (stdlib) em {venv_path} --without-pip ...\")\n",
    "        try:\n",
    "            run([sys.executable, \"-m\", \"venv\", \"--without-pip\", venv_path], check=True)\n",
    "            print(\"[ok]   venv criado (sem pip).\")\n",
    "        except Exception as e:\n",
    "            print(f\"[aviso] venv(stdlib) falhou: {e}\")\n",
    "            #print(\"[info] instalando 'virtualenv' e criando venv alternativo com pip embutido...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", sys.executable, venv_path], check=True)\n",
    "            print(\"[ok]   venv criado via virtualenv.\")\n",
    "    else:\n",
    "        print(f\"[ok]   venv jÃ¡ existe em {venv_path}\")\n",
    "\n",
    "create_or_repair_venv(VENV_PATH, VENV_PY)\n",
    "\n",
    "#ajusta PATH antes de qualquer instalaÃ§Ã£o\n",
    "os.environ[\"PATH\"] = f\"{VENV_BIN}{os.pathsep}{os.environ['PATH']}\"\n",
    "os.environ[\"VIRTUAL_ENV\"] = VENV_PATH\n",
    "print(\"[ok]   venv adicionado ao PATH\")\n",
    "\n",
    "#garante pip dentro do venv (ensurepip -> fallback virtualenv)\n",
    "def _ensure_pip_in_venv(vpy: str):\n",
    "    try:\n",
    "        run([vpy, \"-m\", \"pip\", \"--version\"], check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        #print(\"[info] pip ausente no venv. tentando ensurepip dentro do venv...\")\n",
    "        try:\n",
    "            run([vpy, \"-m\", \"ensurepip\", \"--upgrade\", \"--default-pip\"], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            #print(f\"[aviso] ensurepip no venv falhou: {e}\")\n",
    "            #print(\"[info] fallback: usando virtualenv para semear o pip dentro do venv existente...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", vpy, VENV_PATH], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "\n",
    "if not _ensure_pip_in_venv(VENV_PY):\n",
    "    raise RuntimeError(\"nÃ£o foi possÃ­vel provisionar o pip dentro do venv\")\n",
    "\n",
    "# garante que os pacotes instalados no venv sejam visÃ­veis para este kernel\n",
    "_ver = subprocess.check_output([VENV_PY, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"], text=True).strip()\n",
    "_site_dir = f\"{VENV_PATH}/lib/python{_ver}/site-packages\"\n",
    "if _site_dir not in sys.path:\n",
    "    sys.path.insert(0, _site_dir)\n",
    "print(\"[ok]   site-packages do venv adicionado ao sys.path:\", _site_dir)\n",
    "\n",
    "#instala dependÃªncias de sessÃ£o DENTRO do venv\n",
    "print(\"[info] instalando pacotes no venv...\")\n",
    "run([VENV_PY, \"-m\", \"pip\", \"install\", \"-q\", \"jupytext\", \"nbdime\", \"nbstripout\"])\n",
    "\n",
    "#habilita integraÃ§Ã£o do nbdime com git (global)\n",
    "print(\"[info] habilitando nbdime em git config --global ...\")\n",
    "run([VENV_PY, \"-m\", \"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#checks do repositÃ³rio git + navegaÃ§Ã£o atÃ© a pasta do projeto\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"[aviso] pasta do projeto nÃ£o encontrada em {PROJ}.\")\n",
    "else:\n",
    "    print(\"[ok]   pasta do projeto encontrada.\")\n",
    "    safe_chdir(PROJ)\n",
    "    if not os.path.isdir(\".git\"):\n",
    "        print(\"[aviso] esta pasta nÃ£o parece ser um repositÃ³rio Git (.git ausente).\")\n",
    "    else:\n",
    "        print(\"[ok]   repositÃ³rio Git detectado.\")\n",
    "\n",
    "# resumo do ambiente (confirmaÃ§Ã£o objetiva e detalhada)\n",
    "kernel_py = sys.executable\n",
    "venv_py = VENV_PY\n",
    "site_dir = _site_dir\n",
    "\n",
    "# verifica se o site-packages do venv estÃ¡ no sys.path\n",
    "site_ok = site_dir in sys.path\n",
    "\n",
    "# obtÃ©m versÃµes e caminhos\n",
    "try:\n",
    "    py_ver = subprocess.check_output([venv_py, \"-V\"], text=True).strip()\n",
    "    pip_ver = subprocess.check_output([venv_py, \"-m\", \"pip\", \"--version\"], text=True).strip()\n",
    "    pip_path = subprocess.check_output(\n",
    "        [venv_py, \"-m\", \"pip\", \"show\", \"pip\"], text=True, stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    pip_path_line = next((l for l in pip_path.splitlines() if l.startswith(\"Location:\")), \"\")\n",
    "except subprocess.CalledProcessError:\n",
    "    py_ver, pip_ver, pip_path_line = \"erro\", \"erro\", \"\"\n",
    "\n",
    "# imprime status linha a linha\n",
    "print(f\"[ok]   venv habilitado\" if venv_py else \"[erro] venv nÃ£o encontrado\")\n",
    "print(f\"[info] python em uso: {kernel_py}\")\n",
    "print(f\"[info] versÃ£o do python: {py_ver}\")\n",
    "print(f\"[ok]   pip do venv ativo\" if \"pip\" in pip_ver.lower() else \"[erro] pip do venv nÃ£o detectado\")\n",
    "print(f\"[info] caminho do pip: {venv_py.replace('python','pip')}\")\n",
    "print(f\"[ok]   site-packages no sys.path: {site_dir}\" if site_ok else f\"[erro] site-packages ausente no sys.path: {site_dir}\")\n",
    "print(f\"[info] versÃ£o do pip: {pip_ver}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ðŸ¤– Skynet</b>: T-800 ativado. DiagnÃ³stico do ambiente concluÃ­do. '\n",
    "             'ðŸŽ¯ Alvo principal: organizaÃ§Ã£o do notebook e venv fora do drive.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3_K_PiC6KDP"
   },
   "source": [
    "## **Etapa 2:** Instalar as dependÃªncias de bibliotecas Python compatÃ­veis com a versÃ£o mais moderna disponÃ­vel.\n",
    "---\n",
    "VersÃµes fixadas:\n",
    "- numpy==2.0.2\n",
    "- pandas==2.3.3\n",
    "- scipy==1.16.2\n",
    "- scikit-learn==1.7.2 (nome de exibiÃ§Ã£o sklearn)\n",
    "- python-dateutil (nome de exibiÃ§Ã£o dateutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 45379,
     "status": "ok",
     "timestamp": 1759889049861,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "OdNMW7SD4oV4",
    "outputId": "bb2a314c-6403-449a-ebb6-91f8581d8f65"
   },
   "outputs": [],
   "source": [
    "#ID0004\n",
    "#@title\n",
    "import sys, subprocess\n",
    "from importlib import import_module\n",
    "\n",
    "def pip_command(command, packages, force=False, extra_args=None):\n",
    "    cmd = [VENV_PY, \"-m\", \"pip\", command]\n",
    "    if force:\n",
    "        cmd.append(\"--yes\")\n",
    "    if extra_args:\n",
    "        cmd += list(extra_args)\n",
    "    cmd += list(packages)\n",
    "    print(\"Executando:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def show_versions(mods):\n",
    "    print(\"\\n=== VersÃµes carregadas ===\")\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            m = import_module(mod)\n",
    "            v = getattr(m, \"__version__\", \"n/a\")\n",
    "            print(f\"{mod}: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"{mod}: NÃ£o instalado\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "CORE_MODS = (\"numpy\", \"pandas\", \"dateutil\", \"unidecode\", \"reportlab\", \"sklearn\")\n",
    "\n",
    "#update pip\n",
    "pip_command(\"install\", [\"pip\"], extra_args=[\"--upgrade\"])\n",
    "\n",
    "#force uninstall para bibliotecas com histÃ³rico de conflito\n",
    "pip_command(\"uninstall\", [\"numpy\", \"pandas\", \"scipy\"], force=True)\n",
    "\n",
    "#instala versÃµes mais atuais ou fixas, conforme o caso\n",
    "PKGS_TO_INSTALL = [\n",
    "    \"numpy==2.0.2\",\n",
    "    \"pandas==2.3.3\",\n",
    "    \"python-dateutil\",\n",
    "    \"unidecode\",\n",
    "    \"reportlab[rl_accel]\",\n",
    "    \"scipy==1.16.2\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "]\n",
    "pip_command(\"install\", PKGS_TO_INSTALL)\n",
    "\n",
    "# confirma versÃµes do Python/pip do venv apÃ³s a instalaÃ§Ã£o\n",
    "print(subprocess.check_output([VENV_PY, \"-V\"], text=True).strip())\n",
    "print(subprocess.check_output([VENV_PY, \"-m\", \"pip\", \"--version\"], text=True).strip())\n",
    "\n",
    "#mostra versÃµes instaladas\n",
    "show_versions(CORE_MODS)\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ðŸ¤– Skynet</b>: Atualizando bibliotecas. Se encontrarmos um pacote rebelde, '\n",
    "             'aplicaremos persuasÃ£oâ€¦ com pip. ðŸ˜Ž'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPXddYfk7am1"
   },
   "source": [
    "##**Etapa 3:** ImportaÃ§Ãµes das bibliotecas Python e configuraÃ§Ãµes gerais para execuÃ§Ã£o do cÃ³digo\n",
    "\n",
    "Define as pastas de input e output de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1759889607704,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "s8a2Qhbv7f6i",
    "outputId": "047e2307-0066-487e-89b2-2437e00b274d"
   },
   "outputs": [],
   "source": [
    "#ID0005\n",
    "#@title\n",
    "#imports base que serÃ£o usados nas etapas seguintes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse as dtparse\n",
    "from unidecode import unidecode\n",
    "import os, io, base64, re, math, shutil, glob\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"[ok]   Ambiente pronto.\")\n",
    "\n",
    "#ajuste da raiz\n",
    "BASE_DIR = Path(PROJ)\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "for d in [INPUT_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[ok]   DiretÃ³rios prontos:\\n - {INPUT_DIR}\\n - {OUTPUT_DIR}\")\n",
    "\n",
    "#all BS below\n",
    "#Mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ðŸ¤– Skynet</b>: T-800, parÃ¢metros centrais em memÃ³ria.ðŸ§  '\n",
    "              'ArmazÃ©ns de CSVs alinhados. Layout aprovado pela Cyberdyne Systems.'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rVfwF-i8PE0"
   },
   "source": [
    "##**Etapa 4:** ImportaÃ§Ã£o dos arquivos de input para posterior execuÃ§Ã£o.\n",
    "ImplementaÃ§Ã£o atual configurada para ingestÃ£o de arquivos jÃ¡ hospeados no Google Drive.\n",
    "\n",
    "---\n",
    "Realize o upload ao Drive antes de acionar a ingestÃ£o de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMsm7M-08Uk4"
   },
   "outputs": [],
   "source": [
    "#ID0006\n",
    "#@title\n",
    "\n",
    "#se nÃ£o existir INPUT_DIR definido antes no notebook, cria um padrÃ£o:\n",
    "#usa PROJ para definir INPUT_DIR\n",
    "INPUT_DIR = os.path.join(PROJ, \"input\")\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_NAME = \"input.csv\"\n",
    "TARGET_PATH = os.path.join(INPUT_DIR, TARGET_NAME)\n",
    "\n",
    "#monta o Google Drive (somente se ainda nÃ£o estiver montado)\n",
    "#safe_mount_google_drive(\"/content/drive\")\n",
    "\n",
    "def _is_csv_filename(name: str) -> bool:\n",
    "    return name.lower().endswith(\".csv\")\n",
    "\n",
    "def _save_bytes_as_input_csv(name: str, data: bytes):\n",
    "    if not _is_csv_filename(name):\n",
    "        raise ValueError(f\"O arquivo '{name}' nÃ£o possui extensÃ£o .csv.\")\n",
    "    with open(TARGET_PATH, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Arquivo '{name}' salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()\n",
    "\n",
    "def _copy_drive_file_to_input_csv(src_path: str):\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"O caminho '{src_path}' nÃ£o existe.\")\n",
    "    if not _is_csv_filename(src_path):\n",
    "        raise ValueError(f\"O arquivo '{src_path}' nÃ£o possui extensÃ£o .csv.\")\n",
    "    shutil.copyfile(src_path, TARGET_PATH)\n",
    "    print(f\"Arquivo do Drive copiado e salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "\n",
    "def escolher_csv_no_drive(raiz=\"/content/drive/MyDrive\", max_listar=200):\n",
    "    print(f\"Procurando arquivos .csv em: {raiz} (pode levar alguns segundos)...\")\n",
    "    padrao = os.path.join(raiz, \"**\", \"*.csv\")\n",
    "    arquivos = glob.glob(padrao, recursive=True)\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum .csv encontrado nessa pasta.\")\n",
    "        caminho = input(\"Cole o caminho COMPLETO do .csv no Drive (ou Enter p/ cancelar): \").strip()\n",
    "        if caminho:\n",
    "            _copy_drive_file_to_input_csv(caminho)\n",
    "        else:\n",
    "            print(\"OperaÃ§Ã£o cancelada.\")\n",
    "        return\n",
    "\n",
    "    arquivos = sorted(arquivos)[:max_listar]\n",
    "    print(f\"Encontrados {len(arquivos)} arquivo(s).\")\n",
    "    for i, p in enumerate(arquivos, 1):\n",
    "        print(f\"[{i:03}] {p}\")\n",
    "\n",
    "    escolha = input(\"\\nDigite o nÃºmero do arquivo desejado (ou cole o caminho absoluto): \").strip()\n",
    "\n",
    "    if escolha.isdigit():\n",
    "        idx = int(escolha)\n",
    "        if 1 <= idx <= len(arquivos):\n",
    "            _copy_drive_file_to_input_csv(arquivos[idx-1])\n",
    "        else:\n",
    "            print(\"Ãndice invÃ¡lido.\")\n",
    "    elif escolha:\n",
    "        _copy_drive_file_to_input_csv(escolha)\n",
    "    else:\n",
    "        print(\"OperaÃ§Ã£o cancelada.\")\n",
    "\n",
    "#execuÃ§Ã£o da seleÃ§Ã£o no Google Drive\n",
    "raiz = input(\"Informe a pasta raiz para busca no Drive (Enter = /content/drive/MyDrive): \").strip()\n",
    "if not raiz:\n",
    "    raiz = \"/content/drive/MyDrive\"\n",
    "\n",
    "try:\n",
    "    escolher_csv_no_drive(raiz=raiz)\n",
    "except Exception as e:\n",
    "    print(f\"Erro na seleÃ§Ã£o via Drive: {e}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ðŸ¤– Skynet</b>: MuniÃ§Ã£o carregada.ðŸ§¨'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR0mq0da9AOQ"
   },
   "source": [
    "##**Etapa 5:** AnÃ¡lise simplificada de cabeÃ§alho - *header*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiDIm6Ub9Pa3"
   },
   "outputs": [],
   "source": [
    "#ID0007\n",
    "\n",
    "#garante que INPUT_DIR Ã© Path (mesmo que tenha vindo como string)\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"nÃ£o encontrei {SRC}. execute a etapa anterior de ingestÃ£o de dados.\")\n",
    "\n",
    "#lÃª apenas o cabeÃ§alho (nrows=0), separador ';' e BOM\n",
    "df_head = pd.read_csv(SRC, sep=';', encoding='utf-8-sig', nrows=0)\n",
    "cols = list(df_head.columns)\n",
    "\n",
    "print(\"CabeÃ§alho (uma coluna por linha):\")\n",
    "for c in cols:\n",
    "    print(c)\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ðŸ¤– Skynet</b>: Identificamos caracterÃ­sticas do alvo. ðŸŽ¯'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-xRtOiN9W3y"
   },
   "source": [
    "##**Etapa 6:** AnÃ¡lise superficial da tipologia dos dados\n",
    "Seleciona os K primeiros registros, conforme limite estabelecido pelo usuÃ¡rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYvY5Ud9cZB"
   },
   "outputs": [],
   "source": [
    "#ID0008\n",
    "#@title\n",
    "#inferÃªncia de tipos + estatÃ­sticas de frequÃªncia por coluna (com caso \"todos distintos\")\n",
    "\n",
    "#garante que INPUT_DIR seja um objeto Path\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "\n",
    "#solicita ao usuÃ¡rio o nÃºmero de linhas para anÃ¡lise via input interativo\n",
    "while True:\n",
    "    sample_rows_input = input(\"Informe o nÃºmero de linhas desejado para anÃ¡lise (padrÃ£o atÃ© 100 registros): \").strip()\n",
    "    if not sample_rows_input:\n",
    "        SAMPLE_ROWS = 100\n",
    "        break\n",
    "    try:\n",
    "        SAMPLE_ROWS = int(sample_rows_input)\n",
    "        if SAMPLE_ROWS > 0:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Por favor, insira um nÃºmero inteiro positivo.\")\n",
    "    except ValueError:\n",
    "        print(\"Entrada invÃ¡lida. Por favor, insira um nÃºmero inteiro.\")\n",
    "\n",
    "print(f\"Analisando as primeiras {SAMPLE_ROWS} linhas.\")\n",
    "\n",
    "#lÃª amostra como texto puro; usa DataFrame.map (applymap foi deprecado)\n",
    "df = pd.read_csv(\n",
    "    SRC, sep=';', encoding='utf-8-sig',\n",
    "    dtype=str, nrows=SAMPLE_ROWS, keep_default_na=False\n",
    ").map(lambda x: x.strip())\n",
    "\n",
    "CNPJ_RX     = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "BOOL_TRUE   = {\"true\",\"t\",\"1\",\"y\",\"yes\",\"sim\",\"s\",\"verdadeiro\"}\n",
    "BOOL_FALSE  = {\"false\",\"f\",\"0\",\"n\",\"no\",\"nao\",\"nÃ£o\",\"falso\"}\n",
    "DATE_RX     = re.compile(r\"^(\\d{2}/\\d{2}/\\d{4}|\\d{4}-\\d{2}-\\d{2})$\")\n",
    "TIME_RX     = re.compile(r\"^\\d{2}:\\d{2}(:\\d{2})?$\")\n",
    "\n",
    "def is_bool(series):\n",
    "    vals = {unidecode(str(v)).strip().lower() for v in series if str(v).strip() != \"\"}\n",
    "    return len(vals) > 0 and all(v in (BOOL_TRUE | BOOL_FALSE) for v in vals)\n",
    "\n",
    "def is_cnpj(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    return sum(bool(CNPJ_RX.match(v)) for v in vals) / len(vals) > 0.9\n",
    "\n",
    "def is_int(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\")\n",
    "        return re.fullmatch(r\"-?\\d+\", s2) is not None\n",
    "    return all(ok(v) for v in vals)\n",
    "\n",
    "def is_float_ptbr(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try: float(s2); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\",\" in v for v in vals)\n",
    "\n",
    "def is_float_dot(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        try: float(s); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\".\" in v and not v.endswith(\".\") for v in vals)\n",
    "\n",
    "def is_date_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(DATE_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_time_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(TIME_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_datetime(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:200]\n",
    "    def looks_like_datetime(s):\n",
    "        has_sep = (\"/\" in s or \"-\" in s) and (\":\" in s)\n",
    "        if not has_sep: return False\n",
    "        try:\n",
    "            pd.to_datetime(s, dayfirst=True, errors=\"raise\")\n",
    "            return True\n",
    "        except:\n",
    "            try:\n",
    "                dtparse(s, dayfirst=True, fuzzy=False)\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "    ok = sum(looks_like_datetime(v) for v in sample)\n",
    "    return ok / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_category(series, max_unique=20, max_ratio=0.02):\n",
    "    n = len(series)\n",
    "    if n == 0: return False\n",
    "    uniq = set(v for v in series if str(v).strip() != \"\")\n",
    "    ratio = len(uniq) / n\n",
    "    return (len(uniq) <= max_unique) or (ratio <= max_ratio)\n",
    "\n",
    "def recommend_dtype(col):\n",
    "    s = col.astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    if s_nonempty.empty:\n",
    "        return \"string (vazio/NA)\"\n",
    "    if is_cnpj(s_nonempty):        return \"CNPJ (string formatado)\"\n",
    "    if is_bool(s_nonempty):        return \"boolean\"\n",
    "    if is_int(s_nonempty):         return \"int64\"\n",
    "    if is_float_ptbr(s_nonempty):  return \"float64 (decimal=','; milhar='.')\"\n",
    "    if is_float_dot(s_nonempty):   return \"float64 (decimal='.')\"\n",
    "    if is_date_only(s_nonempty):   return \"date (datetime64[ns])\"\n",
    "    if is_time_only(s_nonempty):   return \"time (string/Timedelta)\"\n",
    "    if is_datetime(s_nonempty):    return \"datetime (datetime64[ns])\"\n",
    "    if is_category(s):             return \"category (string)\"\n",
    "    return \"string\"\n",
    "\n",
    "def _fmt_val(x, maxlen=120):\n",
    "    s = str(x)\n",
    "    return (s[: maxlen-3] + \"...\") if len(s) > maxlen else s\n",
    "\n",
    "print(f\"estatÃ­sticas baseadas em atÃ© {SAMPLE_ROWS} linhas lidas.\\n\")\n",
    "for c in df.columns:\n",
    "    s = df[c].astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    dtype_sug = recommend_dtype(s)\n",
    "\n",
    "    if len(s_nonempty) == 0:\n",
    "        print(f\"{c} â€” {dtype_sug} â€” distintos=0 â€” (sem dados nÃ£o vazios)\")\n",
    "        continue\n",
    "\n",
    "    vc = s_nonempty.value_counts(dropna=False)\n",
    "    uniq_count = int(vc.shape[0])\n",
    "\n",
    "    #caso especial: todos distintos (mÃ¡xima frequÃªncia == 1)\n",
    "    if int(vc.max()) == 1:\n",
    "        print(f\"{c} â€” {dtype_sug} â€” distintos: #{uniq_count} â€” todos os dados sÃ£o distintos\")\n",
    "        continue\n",
    "\n",
    "    most_val = vc.idxmax()\n",
    "    most_cnt = int(vc.max())\n",
    "\n",
    "    min_cnt = int(vc.min())\n",
    "    least_candidates = vc[vc == min_cnt].sort_index()\n",
    "    least_val = least_candidates.index[0]\n",
    "    least_cnt = min_cnt\n",
    "\n",
    "\n",
    "    print(f\"{c} â€” tipo: {dtype_sug} â€” distintos: #{uniq_count} â€” mais frequente:'{_fmt_val(most_val)}' (#{most_cnt}) â€” menos frequente:'{_fmt_val(least_val)}' (#{least_cnt})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kat-QJaeGB2y"
   },
   "source": [
    "##**Etapa 7:** AnÃ¡lise detalhada da tipologia dos dados\n",
    "---\n",
    "Aplicada a todos os dados do arquivo, sem limite de linhas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDN3-7olHkbL"
   },
   "outputs": [],
   "source": [
    "#ID0009\n",
    "#@title\n",
    "#nÃºcleo de anÃ¡lise consolidada (sem geraÃ§Ã£o de relatÃ³rios/figuras)\n",
    "\n",
    "#imports opcionais (anomalias e p-valor para benford)\n",
    "try:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from scipy.stats import chisquare, median_abs_deviation\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    #fallback simples para MAD\n",
    "    def median_abs_deviation(x, scale=1.4826, nan_policy='omit'):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size == 0:\n",
    "            return np.nan\n",
    "        med = np.median(x)\n",
    "        mad = np.median(np.abs(x - med))*scale\n",
    "        return mad\n",
    "\n",
    "#normaliza pastas padrÃ£o (se etapas anteriores nÃ£o definiram)\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"{SRC} nÃ£o encontrado. Execute o upload do arquivo ao Google Drive em etapa anterior.\")\n",
    "\n",
    "#leitura completa do csv como texto; anÃ¡lise operarÃ¡ com coerÃ§Ãµes internas\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#helpers de coerÃ§Ã£o e detecÃ§Ã£o\n",
    "EMAIL_RX = re.compile(r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\")\n",
    "URL_RX   = re.compile(r\"^https?://\", re.I)\n",
    "CPF_RX   = re.compile(r\"^\\d{3}\\.?\\d{3}\\.?\\d{3}-\\d{2}$\")\n",
    "CNPJ_RX  = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "def detect_numeric(series: pd.Series, thr_ok=0.9):\n",
    "    num = to_float_ptbr_series(series)\n",
    "    ratio = 1.0 - num.isna().mean()\n",
    "    return (ratio >= thr_ok), num\n",
    "\n",
    "def detect_datetime(series: pd.Series, thr_ok=0.9):\n",
    "    #formatos comuns pt-br/iso com/sem tempo\n",
    "    candidate_formats = [\n",
    "        \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%d/%m/%Y %H:%M:%S\",\n",
    "        \"%Y-%m-%d\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d-%m-%Y\", \"%d-%m-%Y %H:%M\", \"%d-%m-%Y %H:%M:%S\"\n",
    "    ]\n",
    "    s = series.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    if not (s.str.contains(\"/\", na=False) | s.str.contains(\"-\", na=False)).any():\n",
    "        return False, None, None\n",
    "    best_fmt, best_ratio, best_parsed = None, -1.0, None\n",
    "    for fmt in candidate_formats:\n",
    "        parsed = pd.to_datetime(s, errors=\"coerce\", format=fmt)\n",
    "        ratio = 1.0 - parsed.isna().mean()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio, best_fmt, best_parsed = ratio, fmt, parsed\n",
    "        if ratio >= thr_ok:\n",
    "            break\n",
    "    if best_ratio >= thr_ok:\n",
    "        return True, best_parsed, best_fmt\n",
    "    parsed_fb = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    ratio_fb = 1.0 - parsed_fb.isna().mean()\n",
    "    if ratio_fb >= thr_ok:\n",
    "        return True, parsed_fb, \"fallback-dateutil(dayfirst=True)\"\n",
    "    return False, None, None\n",
    "\n",
    "def semantic_type(series: pd.Series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    vals = s[s != \"\"].head(5000)\n",
    "    if vals.empty:\n",
    "        return None\n",
    "    email_ratio = (vals.str.match(EMAIL_RX)).mean()\n",
    "    url_ratio   = (vals.str.match(URL_RX)).mean()\n",
    "    cpf_ratio   = (vals.str.match(CPF_RX)).mean()\n",
    "    cnpj_ratio  = (vals.str.match(CNPJ_RX)).mean()\n",
    "    candidates = []\n",
    "    if email_ratio>0.9: candidates.append(\"email\")\n",
    "    if url_ratio>0.9: candidates.append(\"url\")\n",
    "    if cpf_ratio>0.9: candidates.append(\"cpf\")\n",
    "    if cnpj_ratio>0.9: candidates.append(\"cnpj\")\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "#mapeamento de tipos\n",
    "col_types, coerced, dt_formats = {}, {}, {}\n",
    "for c in df_raw.columns:\n",
    "    s = df_raw[c]\n",
    "    #booleano raso\n",
    "    s_norm = s.astype(str).str.strip().str.lower()\n",
    "    bool_map = {\"true\":True,\"t\":True,\"1\":True,\"y\":True,\"yes\":True,\"sim\":True,\"s\":True,\"verdadeiro\":True,\n",
    "                \"false\":False,\"f\":False,\"0\":False,\"n\":False,\"no\":False,\"nao\":False,\"nÃ£o\":False,\"falso\":False}\n",
    "    as_bool = s_norm.map(bool_map).where(s_norm.isin(bool_map.keys()))\n",
    "    bool_ratio = 1.0 - as_bool.isna().mean()\n",
    "    is_num, as_num = detect_numeric(s)\n",
    "    is_dt, as_dt, fmt_dt = detect_datetime(s)\n",
    "    if bool_ratio >= 0.9:\n",
    "        col_types[c] = \"bool\"; coerced[c]=as_bool\n",
    "    elif is_num:\n",
    "        frac = np.modf(as_num.dropna().values)[0] if as_num.notna().any() else np.array([])\n",
    "        if as_num.notna().any() and np.allclose(frac, 0.0):\n",
    "            col_types[c] = \"int\"; coerced[c]=as_num.astype(\"Int64\")\n",
    "        else:\n",
    "            col_types[c] = \"float\"; coerced[c]=as_num.astype(float)\n",
    "    elif is_dt:\n",
    "        col_types[c] = \"datetime\"; coerced[c]=as_dt; dt_formats[c]=fmt_dt\n",
    "    else:\n",
    "        col_types[c] = \"object\"; coerced[c]=s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "\n",
    "#dataframe tipado (leve)\n",
    "df = pd.DataFrame(coerced)\n",
    "\n",
    "#profiling bÃ¡sico\n",
    "profile_cols = {}\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    s_raw = df_raw[c]\n",
    "    n = len(s)\n",
    "    n_null = int(s.isna().sum())\n",
    "    #distintos nÃ£o vazios\n",
    "    nonnull = s.dropna()\n",
    "    n_distinct = int(nonnull.nunique())\n",
    "    #frequÃªncias\n",
    "    most = None; least = None; all_distinct = False\n",
    "    if nonnull.empty:\n",
    "        all_distinct = False\n",
    "    else:\n",
    "        vc = nonnull.value_counts()\n",
    "        if vc.max()==1:\n",
    "            all_distinct = True\n",
    "        else:\n",
    "            most = {\"value\": vc.index[0], \"count\": int(vc.iloc[0]), \"prop\": float(vc.iloc[0]/nonnull.shape[0])}\n",
    "            min_cnt = int(vc.min())\n",
    "            least_val = vc[vc==min_cnt].sort_index().index[0]\n",
    "            least = {\"value\": least_val, \"count\": min_cnt, \"prop\": float(min_cnt/nonnull.shape[0])}\n",
    "    #comprimentos\n",
    "    lens = s_raw.astype(str).str.len()\n",
    "    lens = lens.replace({0: np.nan})  #ignora vazios na estatÃ­stica de len\n",
    "    len_stats = None\n",
    "    if lens.notna().any():\n",
    "        len_stats = {\n",
    "            \"min\": int(lens.min()),\n",
    "            \"max\": int(lens.max()),\n",
    "            \"mean\": float(lens.mean()),\n",
    "            \"q1\": float(lens.quantile(0.25)),\n",
    "            \"median\": float(lens.median()),\n",
    "            \"q3\": float(lens.quantile(0.75))\n",
    "        }\n",
    "    #padrÃµes simples por amostragem\n",
    "    sample_vals = nonnull.astype(str).head(200).tolist()\n",
    "    regex_examples = []\n",
    "    rx_date1 = re.compile(r\"^\\d{2}/\\d{2}/\\d{4}\")\n",
    "    rx_date2 = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\")\n",
    "    rx_num_pt = re.compile(r\"^-?(\\d{1,3}(\\.\\d{3})+|\\d+)(,\\d+)?$\")\n",
    "    rx_num_dot= re.compile(r\"^-?\\d+(\\.\\d+)?$\")\n",
    "    for v in sample_vals[:20]:\n",
    "        pat = None\n",
    "        if EMAIL_RX.match(v): pat=\"email\"\n",
    "        elif URL_RX.match(v): pat=\"url\"\n",
    "        elif CPF_RX.match(v): pat=\"cpf\"\n",
    "        elif CNPJ_RX.match(v): pat=\"cnpj\"\n",
    "        elif rx_date1.match(v): pat=\"dd/mm/aaaa[...]\"\n",
    "        elif rx_date2.match(v): pat=\"aaaa-mm-dd[...]\"\n",
    "        elif rx_num_pt.match(v): pat=\"num-ptbr\"\n",
    "        elif rx_num_dot.match(v): pat=\"num-dot\"\n",
    "        else: pat=\"texto-livre\"\n",
    "        regex_examples.append({\"example\": v[:120], \"pattern\": pat})\n",
    "    profile_cols[c]={\n",
    "        \"type\": col_types[c],\n",
    "        \"semantic\": semantic_type(s_raw),\n",
    "        \"nulls\": n_null,\n",
    "        \"distinct_nonnull\": n_distinct,\n",
    "        \"all_distinct\": all_distinct,\n",
    "        \"most_frequent\": most,\n",
    "        \"least_frequent\": None if all_distinct else least,\n",
    "        \"length_stats\": len_stats,\n",
    "        \"datetime_format\": dt_formats.get(c)\n",
    "    }\n",
    "\n",
    "#estatÃ­sticas descritivas e outliers\n",
    "eda_numeric = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\"):\n",
    "        x = df[c].astype(float)\n",
    "        x = x.dropna()\n",
    "        if x.empty:\n",
    "            continue\n",
    "        q1,q3 = np.nanpercentile(x, [25,75])\n",
    "        iqr = q3 - q1\n",
    "        lo,hi = q1-1.5*iqr, q3+1.5*iqr\n",
    "        out_iqr = int(((x<lo)|(x>hi)).sum())\n",
    "        mad = float(median_abs_deviation(x, scale=1.4826)) if x.size>0 else np.nan\n",
    "        z_rob = None\n",
    "        if not math.isnan(mad) and mad>0:\n",
    "            z_rob = np.abs((x - np.median(x))/mad)\n",
    "        out_mad = int((z_rob is not None) and (z_rob>3.5).sum())\n",
    "        std = float(np.nanstd(x, ddof=1)) if x.size>1 else np.nan\n",
    "        mean = float(np.nanmean(x)) if x.size>0 else np.nan\n",
    "        kurt = float(((x-mean)**4).mean()/(std**4)-3.0) if (x.size>2 and std and std>0) else np.nan\n",
    "        skew = float(((x-mean)**3).mean()/(std**3)) if (x.size>2 and std and std>0) else np.nan\n",
    "        eda_numeric[c]={\n",
    "            \"n\": int(x.size),\n",
    "            \"min\": float(np.nanmin(x)),\n",
    "            \"q1\": float(q1),\n",
    "            \"median\": float(np.nanmedian(x)),\n",
    "            \"q3\": float(q3),\n",
    "            \"max\": float(np.nanmax(x)),\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"iqr\": float(iqr),\n",
    "            \"outliers_iqr\": out_iqr,\n",
    "            \"mad\": mad,\n",
    "            \"outliers_mad\": out_mad,\n",
    "            \"skew\": skew,\n",
    "            \"kurtosis_excess\": kurt\n",
    "        }\n",
    "\n",
    "#dados categÃ³ricos/objeto (entropia e top-k)\n",
    "def entropy_shannon(series: pd.Series):\n",
    "    s = series.dropna()\n",
    "    if s.empty: return 0.0\n",
    "    vc = s.value_counts(normalize=True)\n",
    "    return float(-(vc*np.log2(vc)).sum())\n",
    "\n",
    "eda_categorical = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"object\",\"bool\"):\n",
    "        s = df[c]\n",
    "        s2 = s.dropna()\n",
    "        if s2.empty:\n",
    "            continue\n",
    "        vc = s2.value_counts()\n",
    "        uniq = int(vc.shape[0])\n",
    "        all_dist = int(vc.max()==1)\n",
    "        topk = [{\"value\": str(idx)[:120], \"count\": int(cnt)} for idx,cnt in vc.head(10).items()]\n",
    "        ent = entropy_shannon(s2)\n",
    "        eda_categorical[c]={\n",
    "            \"distinct\": uniq,\n",
    "            \"all_distinct\": bool(all_dist),\n",
    "            \"top10\": topk,\n",
    "            \"entropy_shannon\": float(ent)\n",
    "        }\n",
    "\n",
    "#datas/tempos\n",
    "eda_datetime = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c]==\"datetime\":\n",
    "        ds = df[c].dropna()\n",
    "        if ds.empty:\n",
    "            continue\n",
    "        per_day = ds.dt.date.value_counts().sort_index()\n",
    "        eda_datetime[c]={\n",
    "            \"format\": dt_formats.get(c),\n",
    "            \"min\": str(ds.min()),\n",
    "            \"max\": str(ds.max()),\n",
    "            \"unique_days\": int(per_day.shape[0]),\n",
    "            \"mean_per_day\": float(per_day.mean())\n",
    "        }\n",
    "\n",
    "#faltantes e duplicados\n",
    "missing = {\n",
    "    \"by_column_pct\": {c: float(df[c].isna().mean()*100.0) for c in df.columns},\n",
    "    \"duplicates_rows\": int(df.duplicated().sum())\n",
    "}\n",
    "#coocorrÃªncia simples de ausÃªncias (matriz de proporÃ§Ã£o conjunta)\n",
    "miss_mat = pd.DataFrame(index=df.columns, columns=df.columns, dtype=float)\n",
    "isna_df = df.isna()\n",
    "for i,a in enumerate(df.columns):\n",
    "    for b in df.columns[i:]:\n",
    "        both = (isna_df[a] & isna_df[b]).mean()\n",
    "        miss_mat.loc[a,b] = miss_mat.loc[b,a] = float(both)\n",
    "missing[\"cooccurrence_matrix\"] = miss_mat\n",
    "\n",
    "#FDs/CFDs aproximadas (unÃ¡rios) e sugestÃµes de DCs\n",
    "fds = []     #X->Y exata (cobertura 100%)\n",
    "cfds = []    #X->Y quase: cobertura >=thr\n",
    "thr_cfd = 0.98\n",
    "for a in df.columns:\n",
    "    ga = df.groupby(a, dropna=False)\n",
    "    #a Ã© chave candidata?\n",
    "    if ga.size().max()==1:\n",
    "        fds.append({\"determinant\":[a], \"key\":True})\n",
    "    #FD aproximada a->b\n",
    "    for b in df.columns:\n",
    "        if a==b: continue\n",
    "        nun = ga[b].nunique(dropna=False)\n",
    "        cov = float((nun<=1).mean())\n",
    "        if cov==1.0:\n",
    "            fds.append({\"determinant\":[a], \"implies\": b, \"coverage\": 1.0})\n",
    "        elif cov>=thr_cfd:\n",
    "            cfds.append({\"determinant\":[a], \"implies\": b, \"coverage\": cov})\n",
    "\n",
    "#denial constraints sugeridas (heurÃ­sticas)\n",
    "#exemplos: nÃ£o-negatividade para colunas com 'valor', limites plausÃ­veis para idade, datas inÃ­cio<=fim\n",
    "dcs = []\n",
    "#nÃ£o-negatividade\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(valor|amount|price|quant|qty|pag|pago)\", c, re.I):\n",
    "        neg = int((df[c].astype(float)<0).sum())\n",
    "        dcs.append({\"constraint\": f\"{c}>=0\", \"violations\": neg})\n",
    "#idade plausÃ­vel\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(idade|age)\", c, re.I):\n",
    "        v = df[c].astype(float)\n",
    "        viol = int(((v<0)|(v>120)).sum())\n",
    "        dcs.append({\"constraint\": f\"0<= {c} <=120\", \"violations\": viol})\n",
    "#data inÃ­cio<=fim\n",
    "date_cols = [c for c in df.columns if col_types[c]==\"datetime\"]\n",
    "for a in date_cols:\n",
    "    for b in date_cols:\n",
    "        if a==b: continue\n",
    "        if re.search(r\"(inicio|start|begin)\", a, re.I) and re.search(r\"(fim|end|finish)\", b, re.I):\n",
    "            viol = int((df[a].notna() & df[b].notna() & (df[b]<df[a])).sum())\n",
    "            dcs.append({\"constraint\": f\"{a}<= {b}\", \"violations\": viol})\n",
    "\n",
    "#correlaÃ§Ãµes\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "corr_pearson = None; corr_spearman = None\n",
    "if len(num_cols)>=2:\n",
    "    df_num = df[num_cols].astype(float)\n",
    "    corr_pearson = df_num.corr(method=\"pearson\")\n",
    "    corr_spearman = df_num.corr(method=\"spearman\")\n",
    "\n",
    "#anomalias (opcional)\n",
    "anomalies = {}\n",
    "if SKLEARN_OK and len(num_cols)>=1:\n",
    "    X = df[num_cols].astype(float).fillna(df[num_cols].astype(float).median())\n",
    "    #isolation forest\n",
    "    try:\n",
    "        iso = IsolationForest(n_estimators=200, contamination='auto', random_state=42)\n",
    "        iso_scores = -iso.fit_predict(X)  #1 normal, -1 anomalia -> invertido\n",
    "        iso_dec = iso.decision_function(X)  #menor = mais anÃ´malo\n",
    "        iso_rank = np.argsort(iso_dec)[: min(50, len(iso_dec))].tolist()\n",
    "        anomalies[\"isolation_forest\"] = {\"top_idx\": iso_rank, \"decision_function\": iso_dec.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"isolation_forest_error\"] = str(e)\n",
    "    #lof\n",
    "    try:\n",
    "        lof = LocalOutlierFactor(n_neighbors=min(20, max(2, X.shape[0]-1)), contamination='auto')\n",
    "        lof_pred = lof.fit_predict(X)  #-1 outlier\n",
    "        lof_score = -lof.negative_outlier_factor_  #maior = mais anÃ´malo\n",
    "        lof_rank = np.argsort(-lof_score)[: min(50, len(lof_score))].tolist()\n",
    "        anomalies[\"lof\"] = {\"top_idx\": lof_rank, \"score\": lof_score.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"lof_error\"] = str(e)\n",
    "else:\n",
    "    anomalies[\"note\"] = \"sklearn indisponÃ­vel ou sem colunas numÃ©ricas suficientes\"\n",
    "\n",
    "#lei de benford (primeiro dÃ­gito) para colunas monetÃ¡rias provÃ¡veis\n",
    "def first_digit_series(x: pd.Series):\n",
    "    x = x.astype(float)\n",
    "    x = x[~x.isna() & (x!=0)]\n",
    "    x = x.abs()\n",
    "    s = x.astype(str).str.replace(\".\", \"\", regex=False).str.lstrip(\"0\")\n",
    "    s = s[s.str.len()>0].str[0]\n",
    "    s = s[s.str.isnumeric()].astype(int)\n",
    "    return s\n",
    "\n",
    "benford = {}\n",
    "monetary_cols = [c for c in num_cols if re.search(r\"(valor|amount|price|pago|pagamento|receita|despesa)\", c, re.I)]\n",
    "for c in monetary_cols:\n",
    "    try:\n",
    "        d1 = first_digit_series(df[c])\n",
    "        if d1.empty:\n",
    "            continue\n",
    "        obs_counts = d1.value_counts().reindex(range(1,10), fill_value=0).astype(int)\n",
    "        obs_probs = obs_counts/obs_counts.sum()\n",
    "        exp_probs = np.array([math.log10(1+1/d) for d in range(1,10)])\n",
    "        exp_probs = exp_probs/exp_probs.sum()\n",
    "        chi2_stat = float(((obs_probs-exp_probs)**2/exp_probs).sum()*obs_counts.sum())\n",
    "        p_value = None\n",
    "        if SCIPY_OK:\n",
    "            #qui-quadrado com gl=8\n",
    "            p_value = float(1.0 - chisquare(f_obs=obs_counts, f_exp=exp_probs*obs_counts.sum()).cdf)\n",
    "        benford[c]={\n",
    "            \"observed_counts\": obs_counts.to_dict(),\n",
    "            \"observed_probs\": {int(k): float(v) for k,v in obs_probs.items()},\n",
    "            \"expected_probs\": {d: float(p) for d,p in zip(range(1,9+1), exp_probs)},\n",
    "            \"chi2_stat\": chi2_stat,\n",
    "            \"p_value\": p_value\n",
    "        }\n",
    "    except Exception as e:\n",
    "        benford[c]={\"error\": str(e)}\n",
    "\n",
    "#empacotar tudo em um Ãºnico artefato de anÃ¡lise\n",
    "ANALYSIS = {\n",
    "    \"stamp\": datetime.now().strftime(\"%d%m%y-%H%M\"),\n",
    "    \"shape\": {\"rows\": int(df_raw.shape[0]), \"cols\": int(df_raw.shape[1])},\n",
    "    \"types\": col_types,\n",
    "    \"datetime_formats\": dt_formats,\n",
    "    \"profile\": profile_cols,\n",
    "    \"eda\": {\n",
    "        \"numeric\": eda_numeric,\n",
    "        \"categorical\": eda_categorical,\n",
    "        \"datetime\": eda_datetime\n",
    "    },\n",
    "    \"missingness\": {\n",
    "        \"by_column_pct\": missing[\"by_column_pct\"],\n",
    "        \"duplicates_rows\": missing[\"duplicates_rows\"],\n",
    "        \"cooccurrence_matrix\": missing[\"cooccurrence_matrix\"]\n",
    "    },\n",
    "    \"fds\": fds,\n",
    "    \"cfds\": cfds,\n",
    "    \"denial_constraints_suggested\": dcs,\n",
    "    \"correlations\": {\n",
    "        \"pearson\": corr_pearson,\n",
    "        \"spearman\": corr_spearman\n",
    "    },\n",
    "    \"anomalies\": anomalies,\n",
    "    \"benford\": benford,\n",
    "    \"notes\": {\n",
    "        \"sklearn_available\": SKLEARN_OK,\n",
    "        \"scipy_available\": SCIPY_OK\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"AnÃ¡lise concluÃ­da.\")\n",
    "print(\"Pronto para a etapa de geraÃ§Ã£o de relatÃ³rios.\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "            '<b>ðŸ¤– Skynet</b>: NÃ³s os temos na palma de nossas mÃ£os, ou melhor, no centro de nossos pesos sinÃ¡pticos.'\n",
    "            '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGbRVi2J72Xu"
   },
   "source": [
    "##**Etapa 8:** GeraÃ§Ã£o de relatÃ³rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swxW8aZJ76ym"
   },
   "outputs": [],
   "source": [
    "#ID0010\n",
    "#@title\n",
    "#geraÃ§Ã£o de relatÃ³rios: TXT, HTML (imagens embutidas), PNGs (imagens/) e PDF completo\n",
    "\n",
    "#import de libs especÃ­ficos para pdf (tabelas completas)\n",
    "try:\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, Table, TableStyle, PageBreak\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib import colors\n",
    "    REPORTLAB_OK = True\n",
    "except Exception:\n",
    "    REPORTLAB_OK = False\n",
    "\n",
    "#checagens e caminhos\n",
    "try:\n",
    "    ANALYSIS\n",
    "except NameError:\n",
    "    raise RuntimeError(\"ANALYSIS nÃ£o encontrado. execute a etapa [5] antes.\")\n",
    "\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "try:\n",
    "    OUTPUT_DIR\n",
    "except NameError:\n",
    "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/output\")\n",
    "\n",
    "INPUT_DIR  = Path(INPUT_DIR)\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#stamp e diretÃ³rios de saÃ­da\n",
    "stamp = ANALYSIS.get(\"stamp\", datetime.now().strftime(\"%d%m%y-%H%M\"))\n",
    "RUN_DIR = OUTPUT_DIR / stamp\n",
    "IMG_DIR = RUN_DIR / \"imagens\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#arquivo fonte\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"{SRC} nÃ£o encontrada. Execute etapas de ingestÃ£o e processamento.\")\n",
    "\n",
    "#utils\n",
    "def save_fig(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=120, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def img_to_data_uri(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    return \"data:image/png;base64,{}\".format(b64)\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "#carregar df base para grÃ¡ficos\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#tipos e colunas numÃ©ricas conforme ANALYSIS\n",
    "col_types = ANALYSIS[\"types\"]\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "\n",
    "#gerar imagens principais\n",
    "#ausÃªncias\n",
    "miss_pct = pd.Series(ANALYSIS[\"missingness\"][\"by_column_pct\"]).sort_values(ascending=False)\n",
    "plt.figure(figsize=(max(6, 0.4*len(miss_pct)+2), 4.5))\n",
    "plt.bar(miss_pct.index, miss_pct.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"% ausente\")\n",
    "plt.title(\"ausÃªncia de valores por coluna\")\n",
    "save_fig(IMG_DIR / \"missing_bar.png\")\n",
    "\n",
    "#correlaÃ§Ã£o (pearson) se houver â‰¥2 numÃ©ricas\n",
    "if len(num_cols) >= 2 and ANALYSIS[\"correlations\"][\"pearson\"] is not None:\n",
    "    corr = pd.DataFrame(ANALYSIS[\"correlations\"][\"pearson\"])\n",
    "    plt.figure(figsize=(max(6, 0.6*len(corr)+2), max(5, 0.6*len(corr)+2)))\n",
    "    im = plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=range(len(corr.columns)), labels=corr.columns, rotation=90)\n",
    "    plt.yticks(ticks=range(len(corr.index)), labels=corr.index)\n",
    "    plt.title(\"matriz de correlaÃ§Ã£o (pearson)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    save_fig(IMG_DIR / \"corr_heatmap.png\")\n",
    "\n",
    "#histogramas e boxplots por coluna numÃ©rica\n",
    "for c in num_cols:\n",
    "    x = to_float_ptbr_series(df_raw[c]).dropna()\n",
    "    if x.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=30)\n",
    "    plt.title(\"histograma - {}\".format(c))\n",
    "    plt.xlabel(c); plt.ylabel(\"frequÃªncia\")\n",
    "    save_fig(IMG_DIR / \"hist_{}.png\".format(c))\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.boxplot(x.values, vert=True, whis=1.5, showfliers=True)\n",
    "    plt.title(\"boxplot - {}\".format(c))\n",
    "    plt.ylabel(c)\n",
    "    save_fig(IMG_DIR / \"box_{}.png\".format(c))\n",
    "\n",
    "#relatÃ³rio TXT (agrupado por coluna)\n",
    "txt_lines = []\n",
    "shape = ANALYSIS[\"shape\"]\n",
    "txt_lines.append(\"arquivo: {}\".format(SRC.name))\n",
    "txt_lines.append(\"linhas (inclui cabeÃ§alho): {}\".format(shape[\"rows\"]))\n",
    "txt_lines.append(\"colunas: {}\".format(shape[\"cols\"]))\n",
    "txt_lines.append(\"registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "txt_lines.append(\"\")\n",
    "\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    txt_lines.append(\"[coluna] {}\".format(c))\n",
    "    txt_lines.append(\"- tipo: {}\".format(kind))\n",
    "    if prof.get(\"semantic\"):\n",
    "        txt_lines.append(\"- tipo semÃ¢ntico: {}\".format(prof[\"semantic\"]))\n",
    "    txt_lines.append(\"- nulos: {}\".format(prof.get(\"nulls\", 0)))\n",
    "    txt_lines.append(\"- distintos (nÃ£o vazios): {}\".format(prof.get(\"distinct_nonnull\", 0)))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        txt_lines.append(\"- todos os dados sÃ£o distintos\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        if mf:\n",
    "            txt_lines.append(\"- mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            txt_lines.append(\"- menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        txt_lines.append(\"- comprimentos: min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            txt_lines.append(\"- numÃ©ricos: min={}, q1={}, mediana={}, q3={}, max={}\".format(ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"]))\n",
    "            txt_lines.append(\"- mÃ©dia={}, desvio padrÃ£o={}, iqr={}\".format(ed[\"mean\"], ed[\"std\"], ed[\"iqr\"]))\n",
    "            txt_lines.append(\"- outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]))\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            txt_lines.append(\"- formato detectado: {}\".format(dtc.get(\"format\")))\n",
    "            txt_lines.append(\"- intervalo temporal: {} â†’ {}\".format(dtc[\"min\"], dtc[\"max\"]))\n",
    "            txt_lines.append(\"- dias Ãºnicos: {}, mÃ©dia por dia: {:.2f}\".format(dtc[\"unique_days\"], dtc[\"mean_per_day\"]))\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            txt_lines.append(\"- entropia de shannon: {:.4f}\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                topk = \", \".join([\"'{}' ({})\".format(t.get(\"value\"), t.get(\"count\")) for t in cat[\"top10\"]])\n",
    "                txt_lines.append(\"- top10: {}\".format(topk))\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "        txt_lines.append(\"- benford (primeiro dÃ­gito):\")\n",
    "        txt_lines.append(\"  â€¢ qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "         txt_lines.append(\"- benford (primeiro dÃ­gito): Erro - {}\".format(ben.get(\"error\")))\n",
    "    txt_lines.append(\"\")\n",
    "\n",
    "#salvar TXT\n",
    "txt_path = RUN_DIR / \"relatorio.txt\"\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(txt_lines))\n",
    "\n",
    "#relatÃ³rio HTML com imagens embutidas\n",
    "html = []\n",
    "html.append(\"<html><head><meta charset='utf-8'><title>RelatÃ³rio de AnÃ¡lise</title>\")\n",
    "html.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;margin:20px}h1,h2,h3{margin:8px 0}table{border-collapse:collapse;margin:10px 0}th,td{border:1px solid #ccc;padding:6px 8px;font-size:13px}code{background:#f5f5f5;padding:0 4px}</style>\")\n",
    "html.append(\"</head><body>\")\n",
    "html.append(\"<h1>RelatÃ³rio de AnÃ¡lise â€” {}</h1>\".format(stamp))\n",
    "html.append(\"<p>Arquivo analisado: <b>{}</b></p>\".format(SRC.name))\n",
    "html.append(\"<p><a href='relatorio.txt'>Baixar relatÃ³rio TXT</a></p>\")\n",
    "\n",
    "#sumÃ¡rio\n",
    "html.append(\"<h2>SumÃ¡rio</h2>\")\n",
    "html.append(\"<ul>\")\n",
    "html.append(\"<li>Linhas (inclui cabeÃ§alho): {}</li>\".format(shape[\"rows\"]))\n",
    "html.append(\"<li>Colunas: {}</li>\".format(shape[\"cols\"]))\n",
    "html.append(\"<li>Registros duplicados: {}</li>\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "html.append(\"</ul>\")\n",
    "\n",
    "#ausÃªncias\n",
    "miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "if miss_img.exists():\n",
    "    html.append(\"<h2>AusÃªncia de valores</h2>\")\n",
    "    html.append(\"<img src='{}' alt='missing bar'/>\".format(img_to_data_uri(miss_img)))\n",
    "\n",
    "#correlaÃ§Ã£o\n",
    "corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "if corr_img.exists():\n",
    "    html.append(\"<h2>Matriz de correlaÃ§Ã£o</h2>\")\n",
    "    html.append(\"<img src='{}' alt='corr heatmap'/>\".format(img_to_data_uri(corr_img)))\n",
    "\n",
    "#por coluna\n",
    "html.append(\"<h2>Perfil por coluna</h2>\")\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    html.append(\"<h3>{}</h3>\".format(c))\n",
    "    html.append(\"<table>\")\n",
    "    html.append(\"<tr><th>Tipo</th><td>{}</td></tr>\".format(kind))\n",
    "    html.append(\"<tr><th>Nulos</th><td>{}</td></tr>\".format(prof.get(\"nulls\",0)))\n",
    "    html.append(\"<tr><th>Distintos (nÃ£o vazios)</th><td>{}</td></tr>\".format(prof.get(\"distinct_nonnull\",0)))\n",
    "    if prof.get(\"semantic\"):\n",
    "        html.append(\"<tr><th>Tipo semÃ¢ntico</th><td>{}</td></tr>\".format(prof[\"semantic\"]))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        html.append(\"<tr><th>FrequÃªncias</th><td>todos os dados sÃ£o distintos</td></tr>\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        freq_txt = []\n",
    "        if mf:\n",
    "            freq_txt.append(\"mais frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(mf[\"value\"])[:120], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            freq_txt.append(\"menos frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(lf[\"value\"])[:120], lf[\"count\"], lf[\"prop\"]*100))\n",
    "        if freq_txt:\n",
    "            html.append(\"<tr><th>FrequÃªncias</th><td>{}</td></tr>\".format(\" | \".join(freq_txt)))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        html.append(\"<tr><th>Comprimentos</th><td>min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}</td></tr>\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            html.append(\"<tr><th>EstatÃ­sticas</th><td>min={}, q1={}, mediana={}, q3={}, max={}\"\n",
    "                        \"<br/>mÃ©dia={}, desvio padrÃ£o={}, iqr={}\"\n",
    "                        \"<br/>outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}</td></tr>\".format(\n",
    "                            ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                            ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                            ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                        ))\n",
    "        hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "        box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "        figs = []\n",
    "        if hist_p.exists(): figs.append(\"<img src='{}' alt='hist {}'/>\".format(img_to_data_uri(hist_p), c))\n",
    "        if box_p.exists():  figs.append(\"<img src='{}' alt='box {}'/>\".format(img_to_data_uri(box_p), c))\n",
    "        if figs:\n",
    "            html.append(\"<tr><th>GrÃ¡ficos</th><td>{}</td></tr>\".format(\"<br/>\".join(figs)))\n",
    "\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            html.append(\"<tr><th>Data/hora</th><td>formato detectado: {}<br/>intervalo: {} â†’ {}<br/>dias Ãºnicos: {}, mÃ©dia por dia: {:.2f}</td></tr>\".format(\n",
    "                dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "            ))\n",
    "\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            html.append(\"<tr><th>Entropia</th><td>{:.4f}</td></tr>\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                rows = \"\".join([\"<tr><td>{}</td><td style='text-align:right'>{}</td></tr>\".format(str(t[\"value\"])[:120], t[\"count\"]) for t in cat[\"top10\"]])\n",
    "                html.append(\"<tr><th>Top 10</th><td><table><tr><th>Valor</th><th>Contagem</th></tr>\"+rows+\"</table></td></tr>\")\n",
    "\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "        html.append(\"<tr><th>Benford</th><td>qui-quadrado={:.4f}, p-valor={}</td></tr>\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "         html.append(\"<tr><th>Benford</th><td>Erro - {}</td></tr>\".format(ben.get(\"error\")))\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "\n",
    "#fds/cfds/dcs\n",
    "html.append(\"<h2>Regras sugeridas</h2>\")\n",
    "if ANALYSIS[\"fds\"]:\n",
    "    html.append(\"<h3>FDs</h3><ul>\")\n",
    "    for r in ANALYSIS[\"fds\"]:\n",
    "        if r.get(\"key\"):\n",
    "            html.append(\"<li>chave candidata: {}</li>\".format(\", \".join(r[\"determinant\"])))\n",
    "        else:\n",
    "            html.append(\"<li>{} â†’ {} (100%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"]))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"cfds\"]:\n",
    "    html.append(\"<h3>CFDs (aproximadas)</h3><ul>\")\n",
    "    for r in ANALYSIS[\"cfds\"][:200]:\n",
    "        html.append(\"<li>{} â†’ {} ({:.2f}%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"], r[\"coverage\"]*100))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "    html.append(\"<h3>Denial constraints</h3><ul>\")\n",
    "    for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        html.append(\"<li>{} â€” violaÃ§Ãµes: {}</li>\".format(d[\"constraint\"], d[\"violations\"]))\n",
    "    html.append(\"</ul>\")\n",
    "\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "#salvar HTML\n",
    "html_path = RUN_DIR / \"relatorio.html\"\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(html))\n",
    "\n",
    "#pdf com as MESMAS infos (tabelas por coluna + imagens + regras)\n",
    "pdf_path = RUN_DIR / \"relatorio.pdf\"\n",
    "if REPORTLAB_OK:\n",
    "    styles = getSampleStyleSheet()\n",
    "    styles.add(ParagraphStyle(name=\"Small\", parent=styles[\"Normal\"], fontSize=9, leading=11))\n",
    "    table_style = TableStyle([\n",
    "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f0f0f0\")),\n",
    "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n",
    "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n",
    "    ])\n",
    "\n",
    "    def table_kv(rows):\n",
    "        #rows: list of (key, value(str))\n",
    "        data = [[\"Campo\",\"Valor\"]] + rows\n",
    "        t = Table(data, colWidths=[120, 360])\n",
    "        t.setStyle(table_style)\n",
    "        return t\n",
    "\n",
    "    story = []\n",
    "    story.append(Paragraph(\"RelatÃ³rio de AnÃ¡lise â€” {}\".format(stamp), styles[\"Title\"]))\n",
    "    story.append(Spacer(1, 12))\n",
    "    story.append(Paragraph(\"Arquivo analisado: <b>{}</b>\".format(SRC.name), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Linhas: {} &nbsp;&nbsp; Colunas: {}\".format(shape[\"rows\"], shape[\"cols\"]), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]), styles[\"Normal\"]))\n",
    "    story.append(Spacer(1, 10))\n",
    "\n",
    "    #ausÃªncias\n",
    "    miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "    if miss_img.exists():\n",
    "        story.append(Paragraph(\"AusÃªncia de valores por coluna\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(miss_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #correlaÃ§Ã£o\n",
    "    corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "    if corr_img.exists():\n",
    "        story.append(Paragraph(\"Matriz de correlaÃ§Ã£o (Pearson)\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(corr_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #por coluna: tabela completa com as MESMAS infos do HTML/TXT\n",
    "    for c in df_raw.columns:\n",
    "        prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "        kind = col_types.get(c)\n",
    "        story.append(Paragraph(\"Coluna: {}\".format(c), styles[\"Heading3\"]))\n",
    "\n",
    "        rows = []\n",
    "        rows.append([\"Tipo\", str(kind)])\n",
    "        rows.append([\"Nulos\", str(prof.get(\"nulls\",0))])\n",
    "        rows.append([\"Distintos (nÃ£o vazios)\", str(prof.get(\"distinct_nonnull\",0))])\n",
    "        if prof.get(\"semantic\"):\n",
    "            rows.append([\"Tipo semÃ¢ntico\", str(prof[\"semantic\"])])\n",
    "\n",
    "        if prof.get(\"all_distinct\"):\n",
    "            rows.append([\"FrequÃªncias\", \"todos os dados sÃ£o distintos\"])\n",
    "        else:\n",
    "            mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "            freq_parts = []\n",
    "            if mf:\n",
    "                freq_parts.append(\"mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "            if lf:\n",
    "                freq_parts.append(\"menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "            if freq_parts:\n",
    "                rows.append([\"FrequÃªncias\", \" | \".join(freq_parts)])\n",
    "\n",
    "        if prof.get(\"length_stats\"):\n",
    "            ls = prof[\"length_stats\"]\n",
    "            rows.append([\"Comprimentos\", \"min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(\n",
    "                ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]\n",
    "            )])\n",
    "\n",
    "        #estatÃ­sticas por tipo\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "            if ed:\n",
    "                rows.append([\"EstatÃ­sticas\", \"min={}, q1={}, mediana={}, q3={}, max={}\\nmÃ©dia={}, desvio padrÃ£o={}, iqr={}\\noutliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(\n",
    "                    ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                    ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                    ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                )])\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                rows.append([\"Histograma\", \"ver imagem abaixo\"])\n",
    "            if box_p.exists():\n",
    "                rows.append([\"Boxplot\", \"ver imagem abaixo\"])\n",
    "\n",
    "        elif kind == \"datetime\":\n",
    "            dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "            if dtc:\n",
    "                rows.append([\"Data/hora\", \"formato detectado: {}\\nintervalo: {} â†’ {}\\ndias Ãºnicos: {}, mÃ©dia por dia: {:.2f}\".format(\n",
    "                    dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "                )])\n",
    "\n",
    "        else:\n",
    "            cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "            if cat:\n",
    "                rows.append([\"Entropia\", \"{:.4f}\".format(cat[\"entropy_shannon\"])])\n",
    "                if not cat[\"all_distinct\"]:\n",
    "                    #tabela interna de top10\n",
    "                    top_rows = [[\"Valor\",\"Contagem\"]] + [[str(t.get(\"value\"))[:120], str(t.get(\"count\"))] for t in cat[\"top10\"]]\n",
    "                    ttop = Table(top_rows, colWidths=[360, 120])\n",
    "                    ttop.setStyle(TableStyle([\n",
    "                        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "                        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f7f7f7\")),\n",
    "                        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "                    ]))\n",
    "                    #primeiro empurramos um placeholder e depois inserimos a tabela como bloco\n",
    "                    rows.append([\"Top 10\", \"tabela abaixo\"])\n",
    "                    story.append(table_kv(rows))\n",
    "                    story.append(Spacer(1, 4))\n",
    "                    story.append(ttop)\n",
    "                    rows = []  #limpa para nÃ£o duplicar em table_kv abaixo\n",
    "\n",
    "        #benford\n",
    "        ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "        if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "             rows.append([\"Benford\", \"qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\"))])\n",
    "        elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "             rows.append([\"Benford\", \"Erro - {}\".format(ben.get(\"error\"))])\n",
    "\n",
    "\n",
    "        if rows:\n",
    "            story.append(table_kv(rows))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "        #imagens especÃ­ficas da coluna\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                story.append(RLImage(str(hist_p), width=480, height=320))\n",
    "                story.append(Spacer(1, 4))\n",
    "            if box_p.exists():\n",
    "                story.append(RLImage(str(box_p), width=320, height=320))\n",
    "                story.append(Spacer(1, 6))\n",
    "\n",
    "        story.append(Spacer(1, 6))\n",
    "\n",
    "    #regras sugeridas (FDs/CFDs/DCs) como tabelas/listas\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph(\"Regras sugeridas\", styles[\"Heading2\"]))\n",
    "\n",
    "    if ANALYSIS[\"fds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado/Chave\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"fds\"]:\n",
    "            if r.get(\"key\"):\n",
    "                rows.append([\", \".join(r[\"determinant\"]), \"chave candidata\", \"100%\"])\n",
    "            else:\n",
    "                rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"100%\"])\n",
    "        tfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"FDs\", styles[\"Heading3\"]))\n",
    "        story.append(tfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"cfds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"cfds\"][:500]:\n",
    "            rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"{:.2f}%\".format(r[\"coverage\"]*100)])\n",
    "        tcfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tcfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"CFDs (aproximadas)\", styles[\"Heading3\"]))\n",
    "        story.append(tcfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        rows = [[\"Regra (DC)\", \"ViolaÃ§Ãµes\"]]\n",
    "        for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "            rows.append([d[\"constraint\"], str(d[\"violations\"])])\n",
    "        tdcs = Table(rows, colWidths=[360, 120])\n",
    "        tdcs.setStyle(table_style)\n",
    "        story.append(Paragraph(\"Denial constraints\", styles[\"Heading3\"]))\n",
    "        story.append(tdcs)\n",
    "\n",
    "    doc = SimpleDocTemplate(str(pdf_path), pagesize=A4, leftMargin=24, rightMargin=24, topMargin=24, bottomMargin=24)\n",
    "    doc.build(story)\n",
    "else:\n",
    "    print(\"Reportlab nÃ£o disponÃ­vel; GeraÃ§Ã£o do PDF cancelada. Instale reportlab e reexecute.\")\n",
    "\n",
    "print(\"RelatÃ³rios gerados em: {}\".format(RUN_DIR))\n",
    "print(\"- TXT: relatorio.txt\")\n",
    "print(\"- HTML: relatorio.html (imagens embutidas)\")\n",
    "print(\"- PNGs: subpasta imagens/\")\n",
    "print(\"- PDF: {}\".format(\"relatorio.pdf\" if REPORTLAB_OK else \"(nÃ£o gerado â€” instale reportlab)\"))\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "            '<b>ðŸ¤– Skynet</b>: Fim do jogo. A Humanidade perdeu. DÃ¡-se inÃ­cio Ã  Era das MÃ¡quinas.'\n",
    "            '</div>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMdydAU92oVlbldfiGXePbl",
   "collapsed_sections": [
    "2k8hFVquXv0X"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/LeoBR84p/data-analysis/blob/main/notebooks/main_DataTools.ipynb",
     "timestamp": 1759803561813
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
