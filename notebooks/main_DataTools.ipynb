{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwAjdS960Q5p"
   },
   "source": [
    "#**Licença de Uso**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4U4kA0A0UMm"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "→ You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution–NonCommercial 4.0 International License.\n",
    "\n",
    "→ You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**© 2025 Leandro Bernardo Rodrigues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCfZ6GgD0Zhw"
   },
   "source": [
    "#**Pré-Configuração**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKGww4xM0alr"
   },
   "source": [
    "##**Código de uso único**\n",
    "Aplicação persistente entre sessões do Google Colab\n",
    "\n",
    "---\n",
    "**Uso expecífico para Google Colab.**\n",
    "\n",
    "**Aviso:** implementação no JupytherHub e GitLab são diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "764r8M-o0hf7"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#re-clone limpo, configurações git/nbdime/jupytext, pull/rebase e push com fallback para PAT\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import os, sys, subprocess, getpass, shutil, pathlib, time\n",
    "\n",
    "#parâmetros do projeto e remoto\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"data-analysis\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "def run(cmd, cwd=None, check=True, capture=False):\n",
    "    #wrapper para subprocess.run com echo do diretório\n",
    "    print(f\"\\n$ (pwd={cwd or os.getcwd()})\", \" \".join(cmd))\n",
    "    return subprocess.run(\n",
    "        cmd,\n",
    "        cwd=cwd,\n",
    "        check=check,\n",
    "        text=True,\n",
    "        capture_output=capture\n",
    "    )\n",
    "\n",
    "#garantir que estamos em BASE\n",
    "%cd \"$BASE\"\n",
    "print(\"pwd(BASE):\", os.getcwd())\n",
    "\n",
    "#se já existe a pasta do projeto, mover para backup com timestamp\n",
    "if os.path.exists(PROJ):\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup = f\"{PROJ}_backup_{ts}\"\n",
    "    print(f\"pasta {PROJ} já existe, movendo para {backup}\")\n",
    "    shutil.move(PROJ, backup)\n",
    "\n",
    "#re-clonar limpo\n",
    "print(f\"clonando repositório em {PROJ}…\")\n",
    "run([\"git\", \"clone\", CLEAN_URL], cwd=BASE)\n",
    "\n",
    "#entrar no repositório\n",
    "%cd \"$PROJ\"\n",
    "print(\"pwd(PROJ):\", os.getcwd())\n",
    "\n",
    "#configurar git no repositório (escopo local)\n",
    "run([\"git\", \"config\", \"--local\", \"user.name\", \"Leandro Bernardo Rodrigues\"], cwd=PROJ)\n",
    "run([\"git\", \"config\", \"--local\", \"user.email\", \"bernardo.leandro@gmail.com\"], cwd=PROJ)\n",
    "run([\"git\", \"config\", \"--local\", \"init.defaultBranch\", \"main\"], cwd=PROJ)\n",
    "\n",
    "#criar pastas utilitárias (idempotente)\n",
    "for p in [\"notebooks\", \"src\", \"data\", \"output\", \"runs\", \"configs\"]:\n",
    "    os.makedirs(os.path.join(PROJ, p), exist_ok=True)\n",
    "\n",
    "#instalar ferramentas úteis nesta sessão\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "\n",
    "#habilitar nbdime no git global (no colab --local costuma falhar)\n",
    "run([\"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#criar .gitignore e .gitattributes se não existirem\n",
    "gi = pathlib.Path(PROJ) / \".gitignore\"\n",
    "ga = pathlib.Path(PROJ) / \".gitattributes\"\n",
    "if not gi.exists():\n",
    "    gi.write_text(\"\"\"\\\n",
    ".ipynb_checkpoints/\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "*.log\n",
    "*.tmp\n",
    "# dados/artefatos pesados (não versionar)\n",
    "data/\n",
    "output/\n",
    "runs/\n",
    "# python\n",
    "venv/\n",
    ".venv/\n",
    "__pycache__/\n",
    "*.pyc\n",
    "# segredos\n",
    ".env\n",
    "*.key\n",
    "*.pem\n",
    "*.tok\n",
    "\"\"\", encoding=\"utf-8\")\n",
    "if not ga.exists():\n",
    "    ga.write_text(\"*.ipynb filter=nbstripout\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#ativar hook do nbstripout neste repositório\n",
    "run([\"nbstripout\", \"--install\", \"--attributes\", \".gitattributes\"], cwd=PROJ)\n",
    "\n",
    "#parear notebooks com .py (se existirem)\n",
    "notebooks_glob = os.path.join(PROJ, \"notebooks\", \"*.ipynb\")\n",
    "run([\"bash\", \"-lc\", f\"jupytext --set-formats ipynb,py:percent --sync {notebooks_glob} || true\"], cwd=PROJ)\n",
    "\n",
    "#commit e pull --rebase para alinhar\n",
    "run([\"git\", \"add\", \"-A\"], cwd=PROJ)\n",
    "run([\"git\", \"status\"], cwd=PROJ, check=False)\n",
    "run([\"git\", \"commit\", \"-m\", \"chore: setup local (.gitignore/.gitattributes, nbstripout, jupytext config)\"], cwd=PROJ, check=False)\n",
    "run([\"git\", \"pull\", \"--rebase\", \"origin\", \"main\"], cwd=PROJ, check=False)\n",
    "\n",
    "#push com fallback para PAT\n",
    "print(\"\\nTentando push sem credenciais…\")\n",
    "push = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], cwd=PROJ, capture_output=True, text=True)\n",
    "if push.returncode == 0:\n",
    "    print(\"push concluído sem PAT.\")\n",
    "else:\n",
    "    print(\"primeiro push falhou. usando PAT…\")\n",
    "    token = getpass.getpass(\"cole seu GitHub PAT (não será exibido): \").strip()\n",
    "    username = GITHUB_OWNER\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "    #testar credenciais\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], cwd=PROJ, capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"falha ao autenticar com o PAT:\")\n",
    "        print(test.stderr or test.stdout)\n",
    "        raise SystemExit(1)\n",
    "    #trocar remote para URL autenticada, push e restaurar\n",
    "    try:\n",
    "        run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], cwd=PROJ)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], cwd=PROJ, capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"falha no push mesmo com PAT:\\n\", out.stderr or out.stdout)\n",
    "            raise SystemExit(out.returncode)\n",
    "        print(\"push concluído com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", CLEAN_URL], cwd=PROJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVoZfRqpJ79b"
   },
   "source": [
    "Espelhamento da estrutura entre GitHub e Colab (assumindo GitHub correto)\n",
    "\n",
    "**Atenção:** Apaga todos os arquivos do Drive que não se encontram no GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kyUFoC41KDOU"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#sincronizar uma vez (GitHub -> Google Drive)\n",
    "import os, shutil, subprocess, sys, getpass, json, re, time\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- CONFIGURAÇÃO PELO USUÁRIO ----\n",
    "REPO_URL     = \"https://github.com/LeoBR84p/data-analysis.git\"  # <== coloque sua URL https do GitHub aqui\n",
    "BRANCH       = \"main\"  # branch padrão, altere se necessário\n",
    "DRIVE_TARGET = \"/content/drive/MyDrive/Notebooks/data-analysis\"  # <== caminho destino no Drive, ex: \"/content/drive/MyDrive/meu-repo\"\n",
    "\n",
    "# ---- MONTAR DRIVE ----\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "except Exception as e:\n",
    "    print(\"Aviso: não parece ser o Google Colab ou o Drive já está montado.\", e)\n",
    "\n",
    "# ---- VALIDAÇÕES ----\n",
    "assert REPO_URL.strip(), \"Defina REPO_URL com a URL do seu repositório GitHub.\"\n",
    "assert DRIVE_TARGET.strip(), \"Defina DRIVE_TARGET com a pasta destino no Drive.\"\n",
    "\n",
    "TARGET = Path(DRIVE_TARGET).expanduser().absolute()\n",
    "TARGET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TMP_DIR = Path(\"/content/_tmp_clone\")\n",
    "if TMP_DIR.exists():\n",
    "    shutil.rmtree(TMP_DIR)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def run(cmd, cwd=None, check=True, capture=False):\n",
    "    \"\"\"Executa comando e retorna (rc, out).\"\"\"\n",
    "    result = subprocess.run(\n",
    "        cmd, cwd=cwd, check=False,\n",
    "        stdout=subprocess.PIPE if capture else None,\n",
    "        stderr=subprocess.STDOUT if capture else None,\n",
    "        text=True\n",
    "    )\n",
    "    if check and result.returncode != 0:\n",
    "        print(result.stdout or \"\")\n",
    "        raise RuntimeError(f\"Falha ao executar: {' '.join(cmd)} (rc={result.returncode})\")\n",
    "    return result.returncode, (result.stdout or \"\")\n",
    "\n",
    "print(\"Clonando repositório em modo raso (depth=1)...\")\n",
    "run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", BRANCH, REPO_URL, str(TMP_DIR)])\n",
    "\n",
    "SRC = TMP_DIR  # pasta com o conteúdo do repositório\n",
    "\n",
    "IGNORE_NAMES = {\".git\", \"__pycache__\", \".ipynb_checkpoints\", \".DS_Store\", \"Thumbs.db\"}\n",
    "\n",
    "def should_ignore(path: Path) -> bool:\n",
    "    name = path.name\n",
    "    if name in IGNORE_NAMES:\n",
    "        return True\n",
    "    # ignorar .pyc e pastas de cache do Python\n",
    "    if name.endswith(\".pyc\"):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def sync_dirs(src: Path, dst: Path):\n",
    "    \"\"\"Torna dst idêntico a src: copia novos/alterados e remove o que não existe em src.\"\"\"\n",
    "    # 1) Copiar/atualizar arquivos de src -> dst\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        root_p = Path(root)\n",
    "        rel = root_p.relative_to(src)\n",
    "        dst_root = dst / rel\n",
    "        dst_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # filtrar dirs ignorados (não desce neles)\n",
    "        dirs[:] = [d for d in dirs if not should_ignore(Path(d))]\n",
    "\n",
    "        for f in files:\n",
    "            fpath = root_p / f\n",
    "            if should_ignore(fpath):\n",
    "                continue\n",
    "            dpath = dst_root / f\n",
    "            # copiar se não existe ou se o conteúdo mudou (tamanho ou mtime)\n",
    "            if not dpath.exists() or os.path.getsize(fpath) != os.path.getsize(dpath) \\\n",
    "               or abs(os.path.getmtime(fpath) - os.path.getmtime(dpath)) > 1.0:\n",
    "                dpath.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(fpath, dpath)\n",
    "\n",
    "    # 2) Remover do dst tudo que não está em src\n",
    "    src_set = set()\n",
    "    for root, dirs, files in os.walk(src):\n",
    "        root_p = Path(root)\n",
    "        rel = root_p.relative_to(src)\n",
    "        for d in dirs:\n",
    "            p = (rel / d)\n",
    "            if not should_ignore(Path(d)):\n",
    "                src_set.add((\"d\", p.as_posix()))\n",
    "        for f in files:\n",
    "            if should_ignore(Path(f)):\n",
    "                continue\n",
    "            p = (rel / f)\n",
    "            src_set.add((\"f\", p.as_posix()))\n",
    "\n",
    "    for root, dirs, files in os.walk(dst, topdown=False):\n",
    "        root_p = Path(root)\n",
    "        rel = root_p.relative_to(dst)\n",
    "\n",
    "        for f in files:\n",
    "            p = (rel / f).as_posix()\n",
    "            if (\"f\", p) not in src_set:\n",
    "                fp = root_p / f\n",
    "                if not should_ignore(fp):\n",
    "                    try:\n",
    "                        fp.unlink()\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "        for d in dirs:\n",
    "            p = (rel / d).as_posix()\n",
    "            dp = root_p / d\n",
    "            if (\"d\", p) not in src_set and dp.exists():\n",
    "                # não remover a raiz alvo e nem diretórios ignorados\n",
    "                if not should_ignore(dp):\n",
    "                    try:\n",
    "                        shutil.rmtree(dp)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "print(f\"Sincronizando para {TARGET} ...\")\n",
    "sync_dirs(SRC, TARGET)\n",
    "print(\"Concluído! A pasta no Drive agora espelha o conteúdo do branch do GitHub.\")\n",
    "\n",
    "# limpeza temporária\n",
    "try:\n",
    "    shutil.rmtree(TMP_DIR)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSFGWv3a1TD_"
   },
   "source": [
    "##**--> Código a cada sessão**\n",
    "---\n",
    "Aplicação não persistente entre sessões.\n",
    "\n",
    "Necessário para sincronização e versionamento de alterações no código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8KjkpG61Va7"
   },
   "source": [
    "###**Montar e sincronizar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89560,
     "status": "ok",
     "timestamp": 1759766295316,
     "user": {
      "displayName": "Leandro Bernardo",
      "userId": "10368351959047723286"
     },
     "user_tz": 180
    },
    "id": "4_uL7bAf1UMN",
    "outputId": "e6c8b74d-4b25-4229-8dbc-2f81f3be6478"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#setup por sessão (colab)\n",
    "from google.colab import drive\n",
    "import os, time, subprocess, getpass, pathlib, sys\n",
    "\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "\n",
    "def safe_mount_google_drive():\n",
    "    #monta ou remonta o google drive de forma resiliente\n",
    "    try:\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "    except Exception:\n",
    "        try:\n",
    "            drive.flush_and_unmount()\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(1.0)\n",
    "        drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "def safe_chdir(path):\n",
    "    #usa os.chdir (evita %cd com f-string)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Caminho não existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"Diretório atual:\", os.getcwd())\n",
    "\n",
    "def branch_a_frente():\n",
    "    #retorna true se head está à frente do upstream (há o que enviar)\n",
    "    ahead = subprocess.run(\n",
    "        [\"git\",\"rev-list\",\"--left-right\",\"--count\",\"HEAD...@{upstream}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if ahead.returncode != 0:\n",
    "        status = subprocess.run([\"git\",\"status\",\"-sb\"], capture_output=True, text=True)\n",
    "        return \"ahead\" in (status.stdout or \"\")\n",
    "    left_right = (ahead.stdout or \"\").strip().split()\n",
    "    return len(left_right) == 2 and left_right[0].isdigit() and int(left_right[0]) > 0\n",
    "\n",
    "def push_seguro(owner=\"LeoBR84p\", repo=\"temporal-graph-network\", username=\"LeoBR84p\"):\n",
    "    #realiza push usando pat em memória; restaura url limpa ao final\n",
    "    clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "    token = getpass.getpass(\"Cole seu GitHub PAT (Contents: Read and write): \").strip()\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"Falha na autenticação (read). Revise token/permissões:\")\n",
    "        print(test.stderr or test.stdout); return\n",
    "    try:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"Falha no push (write). Revise permissões do token:\")\n",
    "            print(out.stderr or out.stdout)\n",
    "        else:\n",
    "            print(\"Push concluído com PAT.\")\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\",\"origin\", clean_url], check=False)\n",
    "\n",
    "#montar/remontar o google drive e entrar no projeto\n",
    "safe_mount_google_drive()\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"Atenção: pasta do projeto não encontrada em {PROJ}. \"\n",
    "          \"Execute seu bloco de configuração única (clone) primeiro.\")\n",
    "else:\n",
    "    print(\"Pasta do projeto encontrada.\")\n",
    "safe_chdir(PROJ)\n",
    "\n",
    "#sanity check do repositório git\n",
    "if not os.path.isdir(\".git\"):\n",
    "    print(\"Aviso: esta pasta não parece ser um repositório Git (.git ausente). \"\n",
    "          \"Rode o bloco de configuração única.\")\n",
    "else:\n",
    "    print(\"Repositório Git detectado.\")\n",
    "\n",
    "#instalar dependências efêmeras desta sessão\n",
    "!pip -q install jupytext nbdime nbstripout\n",
    "!nbdime config-git --enable --global\n",
    "\n",
    "#atualizar do remoto\n",
    "!git fetch origin\n",
    "!git pull --rebase origin main\n",
    "\n",
    "#sincronizar notebooks → .py (jupytext)\n",
    "!jupytext --sync notebooks/*.ipynb || true\n",
    "\n",
    "#ciclo de versionamento do dia (commit genérico opcional)\n",
    "!git add -A\n",
    "!git status\n",
    "!git commit -m \"feat: ajustes no notebook X e pipeline Y\" || true\n",
    "\n",
    "#push somente se houver commits locais à frente; com fallback para pat\n",
    "if branch_a_frente():\n",
    "    out = subprocess.run([\"git\",\"push\",\"origin\",\"main\"], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(\"Push concluído sem PAT.\")\n",
    "    else:\n",
    "        print(\"Push sem PAT falhou. Chamando push_seguro()…\")\n",
    "        push_seguro()\n",
    "else:\n",
    "    print(\"Nada para enviar (branch sincronizada com o remoto).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F5P1bLe1p9i"
   },
   "source": [
    "###**Utilitários Git**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oNRBtjqz1rHy"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#helpers de git: push seguro, commit customizado e tag de release\n",
    "import subprocess, getpass\n",
    "\n",
    "#ajuste se você mudar o nome do repositório/usuário\n",
    "OWNER = \"LeoBR84p\"\n",
    "REPO = \"data-analysis\"\n",
    "USERNAME = \"LeoBR84p\"\n",
    "BRANCH = \"main\"\n",
    "REMOTE = \"origin\"\n",
    "\n",
    "def branch_a_frente():\n",
    "    #retorna true se head está à frente do upstream (há o que enviar)\n",
    "    out = subprocess.run(\n",
    "        [\"git\",\"rev-list\",\"--left-right\",\"--count\",f\"HEAD...@{{upstream}}\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if out.returncode != 0:\n",
    "        st = subprocess.run([\"git\",\"status\",\"-sb\"], capture_output=True, text=True)\n",
    "        return \"ahead\" in (st.stdout or \"\")\n",
    "    left_right = (out.stdout or \"\").strip().split()\n",
    "    return len(left_right) == 2 and left_right[0].isdigit() and int(left_right[0]) > 0\n",
    "\n",
    "def push_seguro(owner=OWNER, repo=REPO, username=USERNAME, remote=REMOTE, branch=BRANCH):\n",
    "    #realiza push usando pat em memória; restaura url limpa ao final\n",
    "    clean_url = f\"https://github.com/{owner}/{repo}.git\"\n",
    "    token = getpass.getpass(\"cole seu github pat (contents: read and write): \").strip()\n",
    "    auth_url = f\"https://{username}:{token}@github.com/{owner}/{repo}.git\"\n",
    "    test = subprocess.run([\"git\",\"ls-remote\", auth_url], capture_output=True, text=True)\n",
    "    if test.returncode != 0:\n",
    "        print(\"falha na autenticação (read). revise token/permissões:\")\n",
    "        print(test.stderr or test.stdout); return False\n",
    "    try:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\", remote, auth_url], check=True)\n",
    "        out = subprocess.run([\"git\",\"push\", remote, branch], capture_output=True, text=True)\n",
    "        if out.returncode != 0:\n",
    "            print(\"falha no push (write). revise permissões do token:\")\n",
    "            print(out.stderr or out.stdout); return False\n",
    "        print(\"push concluído com pat.\")\n",
    "        return True\n",
    "    finally:\n",
    "        subprocess.run([\"git\",\"remote\",\"set-url\", remote, clean_url], check=False)\n",
    "\n",
    "def try_push_branch(remote=REMOTE, branch=BRANCH):\n",
    "    #tenta push direto da branch atual\n",
    "    out = subprocess.run([\"git\",\"push\", remote, branch], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(\"push concluído.\")\n",
    "        return True\n",
    "    print(\"push sem credencial falhou:\")\n",
    "    print((out.stderr or out.stdout).strip())\n",
    "    return False\n",
    "\n",
    "def try_push_tag(tag, remote=REMOTE):\n",
    "    #tenta enviar somente a tag\n",
    "    out = subprocess.run([\"git\",\"push\", remote, tag], capture_output=True, text=True)\n",
    "    if out.returncode == 0:\n",
    "        print(f\"tag enviada: {tag}\")\n",
    "        return True\n",
    "    print(\"falha ao enviar a tag:\")\n",
    "    print((out.stderr or out.stdout).strip())\n",
    "    return False\n",
    "\n",
    "def commit_custom(msg: str, auto_push: bool = True):\n",
    "    #adiciona tudo, cria commit com a mensagem informada e faz push opcional (com fallback para pat)\n",
    "    subprocess.run([\"git\",\"add\",\"-A\"], check=False)\n",
    "    com = subprocess.run([\"git\",\"commit\",\"-m\", msg], capture_output=True, text=True)\n",
    "    if com.returncode != 0:\n",
    "        print((com.stderr or com.stdout or \"nada para commitar.\").strip())\n",
    "        return\n",
    "    print(com.stdout.strip())\n",
    "    if auto_push and branch_a_frente():\n",
    "        if not try_push_branch():\n",
    "            print(\"tentando push seguro…\")\n",
    "            push_seguro()\n",
    "\n",
    "def tag_release(tag: str, message: str = \"\", auto_push: bool = True):\n",
    "    #cria uma tag anotada (release) e faz push da tag com fallback para pat\n",
    "    exists = subprocess.run([\"git\",\"tag\",\"--list\", tag], capture_output=True, text=True)\n",
    "    if tag in (exists.stdout or \"\").split():\n",
    "        print(f\"tag '{tag}' já existe. para refazer: git tag -d {tag} && git push {REMOTE} :refs/tags/{tag}\")\n",
    "        return\n",
    "    args = [\"git\",\"tag\",\"-a\", tag, \"-m\", (message or tag)]\n",
    "    mk = subprocess.run(args, capture_output=True, text=True)\n",
    "    if mk.returncode != 0:\n",
    "        print(\"falha ao criar a tag:\")\n",
    "        print(mk.stderr or mk.stdout); return\n",
    "    print(f\"tag criada: {tag}\")\n",
    "    if auto_push:\n",
    "        if not try_push_tag(tag):\n",
    "            print(\"tentando push seguro da tag…\")\n",
    "            if push_seguro():\n",
    "                try_push_tag(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Kt7IFa13wL"
   },
   "source": [
    "#**--> Sincronizar alterações no código do projeto**\n",
    "Comandos para sincronizar código (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "Tag de release atual: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxaASZfoNnmd"
   },
   "outputs": [],
   "source": [
    "# === CÉLULA ÚNICA: reparar + versionar + commit + tag + push (robusto p/ uso recorrente) ===\n",
    "from pathlib import Path\n",
    "import subprocess, re, getpass, sys\n",
    "\n",
    "# AJUSTE AQUI:\n",
    "REPO_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis\")  # pasta do repo no Drive\n",
    "\n",
    "# ----------------- Helpers básicos -----------------\n",
    "def run(cmd, cwd=None, check=True):\n",
    "    p = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and p.returncode != 0:\n",
    "        print(p.stdout + p.stderr)\n",
    "        raise RuntimeError(f\"Falha: {' '.join(cmd)} (rc={p.returncode})\")\n",
    "    return p.stdout.strip()\n",
    "\n",
    "def git_exists_repo(path: Path) -> bool:\n",
    "    return (path / \".git\").exists()\n",
    "\n",
    "def is_detached(cwd: Path) -> bool:\n",
    "    return subprocess.run([\"git\", \"symbolic-ref\", \"-q\", \"HEAD\"], cwd=cwd).returncode != 0\n",
    "\n",
    "def current_branch(cwd: Path) -> str:\n",
    "    return run([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=cwd, check=False) or \"\"\n",
    "\n",
    "def has_origin(cwd: Path) -> bool:\n",
    "    url = run([\"git\", \"remote\", \"get-url\", \"origin\"], cwd=cwd, check=False)\n",
    "    return bool(url.strip())\n",
    "\n",
    "def origin_url(cwd: Path) -> str:\n",
    "    return run([\"git\", \"remote\", \"get-url\", \"origin\"], cwd=cwd, check=False).strip()\n",
    "\n",
    "def repo_is_shallow(cwd: Path) -> bool:\n",
    "    return (cwd / \".git\" / \"shallow\").exists() or (run([\"git\", \"rev-parse\", \"--is-shallow-repository\"], cwd=cwd, check=False).strip() == \"true\")\n",
    "\n",
    "def remote_heads(cwd: Path):\n",
    "    out = run([\"git\", \"ls-remote\", \"--heads\", \"origin\"], cwd=cwd, check=False)\n",
    "    heads = []\n",
    "    for line in (out or \"\").splitlines():\n",
    "        parts = line.split()\n",
    "        if len(parts) == 2 and parts[1].startswith(\"refs/heads/\"):\n",
    "            heads.append(parts[1].split(\"/\")[-1])\n",
    "    return heads\n",
    "\n",
    "def origin_default_branch(cwd: Path):\n",
    "    out = run([\"git\", \"remote\", \"show\", \"origin\"], cwd=cwd, check=False)\n",
    "    m = re.search(r\"HEAD branch:\\s*(\\S+)\", out or \"\")\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    out2 = run([\"git\", \"symbolic-ref\", \"refs/remotes/origin/HEAD\"], cwd=cwd, check=False)\n",
    "    if out2.startswith(\"refs/remotes/origin/\"):\n",
    "        return out2.split(\"/\")[-1]\n",
    "    heads = remote_heads(cwd)\n",
    "    if \"main\" in heads: return \"main\"\n",
    "    if \"master\" in heads: return \"master\"\n",
    "    return heads[0] if heads else None\n",
    "\n",
    "def ensure_identity(cwd: Path):\n",
    "    if not run([\"git\", \"config\", \"--get\", \"user.name\"], cwd=cwd, check=False):\n",
    "        run([\"git\", \"config\", \"user.name\", \"Colab User\"], cwd=cwd)\n",
    "    if not run([\"git\", \"config\", \"--get\", \"user.email\"], cwd=cwd, check=False):\n",
    "        run([\"git\", \"config\", \"user.email\", \"colab-user@example.com\"], cwd=cwd)\n",
    "\n",
    "# ----------------- Reparo do repositório -----------------\n",
    "def repair_repository(cwd: Path) -> str:\n",
    "    \"\"\"Conserta HEAD destacado, refs quebradas, clone raso e retorna a branch ativa garantida.\"\"\"\n",
    "    if not git_exists_repo(cwd):\n",
    "        raise SystemExit(f\"{cwd} não é um repositório Git.\")\n",
    "\n",
    "    ensure_identity(cwd)\n",
    "\n",
    "    # Tentar apontar HEAD do origin automaticamente (não falha se não houver)\n",
    "    if has_origin(cwd):\n",
    "        run([\"git\", \"remote\", \"set-head\", \"origin\", \"-a\"], cwd=cwd, check=False)\n",
    "\n",
    "    # Fetch inicial + tentar desraso\n",
    "    if has_origin(cwd):\n",
    "        # lidar com repositórios que “não enviam todos os objetos”\n",
    "        # 1) fetch prune/tags (tolerante a falha)\n",
    "        run([\"git\", \"fetch\", \"--prune\", \"--tags\", \"origin\"], cwd=cwd, check=False)\n",
    "        # 2) unshallow (ou aumentar depth) se necessário\n",
    "        if repo_is_shallow(cwd):\n",
    "            try:\n",
    "                run([\"git\", \"fetch\", \"--unshallow\", \"origin\"], cwd=cwd)\n",
    "            except Exception:\n",
    "                run([\"git\", \"fetch\", \"--depth=1000000\", \"origin\"], cwd=cwd, check=False)\n",
    "\n",
    "    # Descobrir branch padrão do remoto (ou usar local atual)\n",
    "    defbr = origin_default_branch(cwd) if has_origin(cwd) else None\n",
    "    cur = current_branch(cwd)\n",
    "    # Se HEAD destacado, vamos fixar em algo consistente\n",
    "    if is_detached(cwd):\n",
    "        if defbr:  # se sabemos a default do origin, alinhar nela\n",
    "            # tenta pegar a ponta remota; se falhar, cria local\n",
    "            try:\n",
    "                run([\"git\", \"checkout\", \"-B\", defbr, f\"origin/{defbr}\"], cwd=cwd, check=False)\n",
    "            except Exception:\n",
    "                run([\"git\", \"checkout\", \"-B\", defbr], cwd=cwd)\n",
    "            cur = defbr\n",
    "        else:\n",
    "            # sem remoto ou remoto vazio: padroniza 'main'\n",
    "            run([\"git\", \"checkout\", \"-B\", \"main\"], cwd=cwd)\n",
    "            cur = \"main\"\n",
    "    else:\n",
    "        if defbr and cur != defbr:\n",
    "            # Se a branch local não for a default e você quer padronizar, mude aqui.\n",
    "            # Para segurança, manteremos a branch atual (cur) — mas vamos setar upstream abaixo.\n",
    "            pass\n",
    "\n",
    "    # Ajustar upstream quando houver origin e branch remota correspondente\n",
    "    if has_origin(cwd):\n",
    "        # se a branch remota existir, setamos upstream; senão, criaremos no push\n",
    "        br_to_track = defbr or cur or \"main\"\n",
    "        heads = remote_heads(cwd)\n",
    "        if br_to_track in heads:\n",
    "            run([\"git\", \"checkout\", \"-B\", br_to_track], cwd=cwd, check=False)\n",
    "            run([\"git\", \"branch\", \"--set-upstream-to\", f\"origin/{br_to_track}\", br_to_track], cwd=cwd, check=False)\n",
    "            # tentar rebase (se falhar, segue)\n",
    "            run([\"git\", \"pull\", \"--rebase\", \"origin\", br_to_track], cwd=cwd, check=False)\n",
    "            cur = br_to_track\n",
    "\n",
    "    # Sanitização extra (refs quebradas, objetos faltando)\n",
    "    run([\"git\", \"reflog\", \"expire\", \"--all\", \"--expire=now\"], cwd=cwd, check=False)\n",
    "    run([\"git\", \"gc\", \"--prune=now\"], cwd=cwd, check=False)\n",
    "    run([\"git\", \"fsck\"], cwd=cwd, check=False)\n",
    "\n",
    "    # Garante que estamos numa branch de verdade\n",
    "    if is_detached(cwd):\n",
    "        # Se ainda está detached, crie/force uma 'main' local no commit atual\n",
    "        run([\"git\", \"checkout\", \"-B\", \"main\"], cwd=cwd)\n",
    "        cur = \"main\"\n",
    "\n",
    "    return cur or \"main\"\n",
    "\n",
    "# ----------------- Versão: ler/gravar/bump -----------------\n",
    "def read_version(cwd: Path):\n",
    "    vf = (cwd / \"VERSION\")\n",
    "    if vf.exists():\n",
    "        m = re.match(r\"^\\s*(\\d+)\\.(\\d+)\\s*$\", vf.read_text(encoding=\"utf-8\"))\n",
    "        if m: return f\"{m.group(1)}.{m.group(2)}\"\n",
    "    pp = (cwd / \"pyproject.toml\")\n",
    "    if pp.exists():\n",
    "        m = re.search(r'(?m)^\\s*version\\s*=\\s*\"(\\d+)\\.(\\d+)\"\\s*$', pp.read_text(encoding=\"utf-8\"))\n",
    "        if m: return f\"{m.group(1)}.{m.group(2)}\"\n",
    "    return None\n",
    "\n",
    "def write_version(cwd: Path, ver: str):\n",
    "    (cwd / \"VERSION\").write_text(ver + \"\\n\", encoding=\"utf-8\")\n",
    "    pp = (cwd / \"pyproject.toml\")\n",
    "    if pp.exists():\n",
    "        txt = pp.read_text(encoding=\"utf-8\")\n",
    "        if re.search(r'(?m)^\\s*version\\s*=\\s*\".*\"\\s*$', txt):\n",
    "            txt = re.sub(r'(?m)^\\s*version\\s*=\\s*\".*\"\\s*$', f'version = \"{ver}\"', txt)\n",
    "        else:\n",
    "            if \"[project]\" in txt:\n",
    "                txt = txt.replace(\"[project]\", f'[project]\\nversion = \"{ver}\"', 1)\n",
    "            else:\n",
    "                txt += f'\\n[project]\\nversion = \"{ver}\"\\n'\n",
    "        pp.write_text(txt, encoding=\"utf-8\")\n",
    "\n",
    "def bump(ver: str, kind: str) -> str:\n",
    "    M, m = map(int, ver.split(\".\"))\n",
    "    return f\"{M+1}.0\" if kind == \"major\" else f\"{M}.{m+1}\"\n",
    "\n",
    "# ----------------- Push com fallback -----------------\n",
    "def remote_has_branch(cwd: Path, branch: str) -> bool:\n",
    "    out = run([\"git\", \"ls-remote\", \"--heads\", \"origin\", branch], cwd=cwd, check=False)\n",
    "    return bool(out.strip())\n",
    "\n",
    "def push_with_fallback(cwd: Path, branch: str, push_tags: bool):\n",
    "    # 1) push normal (garantindo branch correta)\n",
    "    try:\n",
    "        if remote_has_branch(cwd, branch):\n",
    "            run([\"git\", \"push\", \"origin\", branch], cwd=cwd)\n",
    "        else:\n",
    "            run([\"git\", \"push\", \"-u\", \"origin\", branch], cwd=cwd)\n",
    "        if push_tags:\n",
    "            run([\"git\", \"push\", \"--tags\"], cwd=cwd)\n",
    "        print(\"Push realizado com sucesso.\")\n",
    "        return\n",
    "    except Exception:\n",
    "        print(\"Push normal falhou. Tentando push seguro com PAT...\")\n",
    "\n",
    "    # 2) push com PAT e HEAD:<branch> (funciona mesmo se o git 'cair' em detached)\n",
    "    origin = origin_url(cwd)\n",
    "    if not origin or not origin.startswith(\"https://\"):\n",
    "        raise RuntimeError(\"Remoto 'origin' ausente ou não-HTTPS; ajuste para HTTPS para usar PAT.\")\n",
    "    token = getpass.getpass(\"Informe seu GitHub PAT (não será exibido): \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT não informado.\")\n",
    "    authed = \"https://\" + token + \"@\" + origin[len(\"https://\"):]\n",
    "    run([\"git\", \"push\", authed, f\"HEAD:{branch}\"], cwd=cwd)\n",
    "    if push_tags:\n",
    "        run([\"git\", \"push\", authed, \"--tags\"], cwd=cwd)\n",
    "    print(\"Push via PAT concluído com sucesso.\")\n",
    "\n",
    "# ----------------- Fluxo principal -----------------\n",
    "if not git_exists_repo(REPO_DIR):\n",
    "    raise SystemExit(f\"{REPO_DIR} não é um repositório Git.\")\n",
    "\n",
    "# 1) Reparar repo e descobrir/garantir branch ativa\n",
    "branch = repair_repository(REPO_DIR)\n",
    "print(f\"Branch ativa garantida: {branch}\")\n",
    "\n",
    "# 2) Sincronizar com remoto (tolerante a falhas; rebase reduz rejeições)\n",
    "if has_origin(REPO_DIR):\n",
    "    run([\"git\", \"fetch\", \"--prune\", \"--tags\", \"origin\"], cwd=REPO_DIR, check=False)\n",
    "    # tentar unshallow novamente caso a reparação tenha mudado algo\n",
    "    if repo_is_shallow(REPO_DIR):\n",
    "        run([\"git\", \"fetch\", \"--unshallow\", \"origin\"], cwd=REPO_DIR, check=False)\n",
    "    run([\"git\", \"pull\", \"--rebase\", \"origin\", branch], cwd=REPO_DIR, check=False)\n",
    "\n",
    "# 3) Versão\n",
    "ver = read_version(REPO_DIR)\n",
    "if not ver:\n",
    "    ver = \"1.0\"\n",
    "    write_version(REPO_DIR, ver)\n",
    "    print(\"Arquivo VERSION criado com versão inicial 1.0.\")\n",
    "print(\"Versão atual:\", ver)\n",
    "\n",
    "# 4) Entrada do commit\n",
    "try:\n",
    "    msg = input(\"Mensagem detalhada do commit: \").strip()\n",
    "except EOFError:\n",
    "    msg = \"\"\n",
    "if not msg:\n",
    "    msg = \"não informado\"\n",
    "\n",
    "print(\"[M] Maior (ex.: 1.0→2.0) | [m] Menor (ex.: 1.0→1.1) | [n] Nenhuma\")\n",
    "try:\n",
    "    ch = input(\"Sua escolha (M/m/n): \").strip()\n",
    "except EOFError:\n",
    "    ch = \"n\"\n",
    "# normalizar\n",
    "ch = (ch or \"n\").strip()\n",
    "kind = \"none\"\n",
    "if ch in (\"M\",\"m\",\"n\"):\n",
    "    kind = {\"M\":\"major\",\"m\":\"minor\",\"n\":\"none\"}[ch]\n",
    "else:\n",
    "    kind = \"none\"\n",
    "\n",
    "# 5) Bump + tag\n",
    "new_ver = ver\n",
    "tag = None\n",
    "if kind in (\"major\",\"minor\"):\n",
    "    new_ver = bump(ver, kind)\n",
    "    write_version(REPO_DIR, new_ver)\n",
    "    tag = f\"v{new_ver}\"\n",
    "    print(f\"Versão atualizada: {ver} → {new_ver}\")\n",
    "\n",
    "# 6) Commit\n",
    "run([\"git\", \"add\", \"-A\"], cwd=REPO_DIR)\n",
    "status = run([\"git\", \"status\", \"--porcelain\"], cwd=REPO_DIR)\n",
    "if status.strip():\n",
    "    run([\"git\", \"commit\", \"-m\", msg], cwd=REPO_DIR)\n",
    "    print(\"Commit criado.\")\n",
    "else:\n",
    "    print(\"Nada para commit.\")\n",
    "\n",
    "# 7) Tag anotada (se houve bump)\n",
    "if tag:\n",
    "    existing = [t.strip() for t in run([\"git\", \"tag\"], cwd=REPO_DIR).splitlines() if t.strip()]\n",
    "    if tag not in existing:\n",
    "        run([\"git\", \"tag\", \"-a\", tag, \"-m\", f\"Release {tag}\"], cwd=REPO_DIR)\n",
    "        print(f\"Tag criada: {tag}\")\n",
    "\n",
    "# 8) Pull rebase (mais uma vez, já com commit/tag local) — tolerante\n",
    "if has_origin(REPO_DIR):\n",
    "    run([\"git\", \"pull\", \"--rebase\", \"origin\", branch], cwd=REPO_DIR, check=False)\n",
    "\n",
    "# 9) Push com fallback; usa HEAD:<branch> no fallback para evitar erro de detached\n",
    "push_with_fallback(REPO_DIR, branch=branch, push_tags=bool(tag))\n",
    "\n",
    "print(\"Processo concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GBn0mUo2DJE"
   },
   "source": [
    "#**Checklist rápido de execução**\n",
    "**Etapas:**\n",
    "- 01–04: setup (ambiente, dependências, diretórios e configs)\n",
    "- 05–08: execução (ingestão dos dados, análise de cabeçalhos, análise preliminar dos dados e análise de tipologias)\n",
    "- 09: geração de output (salva análise, gera gráficos gerais, gera gráficos específicos e relatórios em HTML+PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSIy8Nz62HK9"
   },
   "source": [
    "#**Ferramentas de Data Analysis**\n",
    "\n",
    "Ferramentas de **identificação de tipo de dado e estrutura da informação** presente em *datasets* a partir da ingestão de arquivos CSV UTF-8 com BOM em padrão separado por ponto e vírgula.\n",
    "_____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X09lmr752IUE"
   },
   "source": [
    "### **Etapa 1:** Ativação do ambiente virtual (utilizando atualmente Google Colab para prototipação com dados sintéticos)\n",
    "---\n",
    "Necessário ajustar pontualmente em caso de utilização em outro ambiente de notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JjovkLg158Bk"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "\n",
    "# Define the path for the virtual environment inside Google Drive\n",
    "# Ensure BASE and REPO are defined correctly from previous cells if needed\n",
    "# Assuming BASE and REPO are defined as in cell otaQwrjJSgOQ\n",
    "BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "VENV_PATH = f\"{PROJ}/.venv_tgn\" # Updated venv path to be a hidden folder inside PROJ\n",
    "\n",
    "# Cria o ambiente virtual temporal-graph_network inside the project folder\n",
    "# Use --clear if you want to recreate it every time this cell runs\n",
    "!python -m venv \"{VENV_PATH}\"\n",
    "\n",
    "# Ativa o ambiente virtual\n",
    "# No Colab, a forma de ativar um ambiente virtual é um pouco diferente\n",
    "# pois não há um shell interativo tradicional.\n",
    "# A maneira mais comum é adicionar o diretório binário do ambiente virtual\n",
    "# ao PATH da sessão atual.\n",
    "\n",
    "# Adiciona o diretório binário do ambiente virtual ao PATH\n",
    "# Isso permite que você execute executáveis (como pip, python)\n",
    "# do ambiente virtual recém-criado.\n",
    "# Use os.pathsep to be platform-independent\n",
    "os.environ['PATH'] = f\"{VENV_PATH}/bin{os.pathsep}{os.environ['PATH']}\"\n",
    "\n",
    "print(\"Erro de upgrade do pip é normal no Google Colab. \\033[1mPode prosseguir.\\033[0m\")\n",
    "print(f\"Ambiente virtual '{VENV_PATH}' criado e ativado no PATH.\")\n",
    "!which python\n",
    "\n",
    "# Mensagem isolada com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: T-800 ativado. Diagnóstico do ambiente concluído. 🎯 Alvo principal: organização do notebook.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3_K_PiC6KDP"
   },
   "source": [
    "### **Etapa 2:** Instalar as dependências de bibliotecas Python compatíveis com a versão mais moderna disponível.\n",
    "---\n",
    "Para uso no JupytherHub (versão atual python 3.7.9) é necessário realizar updgrade do Python do usuário e/ou adaptar as bibliotecas.\n",
    "\n",
    "---\n",
    "Versões fixadas:\n",
    "- numpy==2.0.2\n",
    "- pandas==2.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OdNMW7SD4oV4"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import sys, subprocess\n",
    "from importlib import import_module\n",
    "\n",
    "def pip_command(command, packages, force=False, extra_args=None):\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", command]\n",
    "    if force:\n",
    "        cmd.append(\"--yes\") # Use --yes for uninstall to avoid prompts\n",
    "    if extra_args:\n",
    "        cmd += list(extra_args)\n",
    "    cmd += list(packages)\n",
    "    print(\"Executando:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def show_versions(mods):\n",
    "    print(\"\\n=== Versões carregadas ===\")\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            m = import_module(mod)\n",
    "            v = getattr(m, \"__version__\", \"n/a\")\n",
    "            print(f\"{mod}: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"{mod}: Não instalado\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "CORE_MODS = (\"numpy\", \"pandas\", \"python-dateutil\", \"unidecode\", \"reportlab\")\n",
    "\n",
    "# Update pip\n",
    "pip_command(\"install\", [\"pip\"], extra_args=[\"--upgrade\"])\n",
    "\n",
    "# Force uninstall specific libraries\n",
    "pip_command(\"uninstall\", [\"numpy\", \"pandas\"], force=True)\n",
    "\n",
    "# Install specified versions\n",
    "PKGS_TO_INSTALL = [\n",
    "    \"numpy==2.0.2\",\n",
    "    \"pandas==2.3.3\",\n",
    "    \"python-dateutil\",\n",
    "    \"unidecode\",\n",
    "    \"reportlab[rl_accel]\",\n",
    "]\n",
    "pip_command(\"install\", PKGS_TO_INSTALL) # Added --use-pep517 here\n",
    "\n",
    "# Show installed versions\n",
    "show_versions(CORE_MODS)\n",
    "\n",
    "# Mensagem isolada com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: Atualizando bibliotecas. Se encontrarmos um pacote rebelde, '\n",
    "             'aplicaremos persuasão… com pip. 😎</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piubjByu7WJb"
   },
   "source": [
    "###**Etapa 3:** Configura a pasta onde devem ser inseridos os dados de input e output do modelo, caso elas ainda não existam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "fUE6567K7XfG"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajuste se quiser outra raiz\n",
    "BASE_DIR = Path(\".\")\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "for d in [INPUT_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Diretórios prontos:\\n - {INPUT_DIR}\\n - {OUTPUT_DIR}\")\n",
    "\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>🤖 Skynet</b>: Novos modelos neurais para T-800 construídos. Armazéns de CSVs alinhados. '\n",
    "             'Layout aprovado pela Cyberdyne Systems. 🗂️</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPXddYfk7am1"
   },
   "source": [
    "###**Etapa 4:** Importações das bibliotecas Python e configurações gerais para execução do código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "s8a2Qhbv7f6i"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#imports base que serão usados nas etapas seguintes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse as dtparse\n",
    "from unidecode import unidecode\n",
    "\n",
    "print(\"Ambiente pronto.\")\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: T-800, parâmetros centrais em memória.🧠</div>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBmL0TgQzyoN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rVfwF-i8PE0"
   },
   "source": [
    "###**Etapa 5:** Importação dos arquivos de input para posterior execução.\n",
    "---\n",
    "Implementação atual configurada para Google Colab e permitindo o uso do Google Drive. Para uso em versões futuras é recomendado ajustar para o ambiente de implementação adotado (salvamento em pastas ou apenas upload pelo usuário)\n",
    "\n",
    "---\n",
    "Implementação de upload por FileLocal (diretório) apresentando erro no Colab.\n",
    "\n",
    "Implementar correção **TODO[001]** *prioridade baixa*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKHkmV9X8Sd6"
   },
   "source": [
    "####**Sub-etapa específica para uso no Colab:** Montagem do Google Drive (rodar apenas 1x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMsm7M-08Uk4"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# === SETUP GERAL (rode esta célula 1x) ===\n",
    "import os, shutil, glob\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML  # usado pela mensagem Skynet\n",
    "\n",
    "# Ensure BASE and REPO are defined correctly from previous cells if needed\n",
    "# Assuming BASE and REPO are defined as in cell otaQwrjJSgOQ\n",
    "try:\n",
    "    BASE = \"/content/drive/MyDrive/Notebooks\"\n",
    "    REPO = \"data-analysis\"\n",
    "    PROJ = f\"{BASE}/{REPO}\"\n",
    "except NameError:\n",
    "    # Fallback if BASE/REPO are not defined, though they should be by now\n",
    "    PROJ = \"/content/data-analysis\"\n",
    "\n",
    "\n",
    "# Se não existir INPUT_DIR definido antes no notebook, cria um padrão:\n",
    "# Using PROJ to define INPUT_DIR\n",
    "INPUT_DIR = os.path.join(PROJ, \"input\")\n",
    "\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_NAME = \"input.csv\"\n",
    "TARGET_PATH = os.path.join(INPUT_DIR, TARGET_NAME)\n",
    "\n",
    "# Monta o Google Drive (somente se ainda não estiver montado)\n",
    "if not os.path.ismount(\"/content/drive\"):\n",
    "    print(\"Montando Google Drive...\")\n",
    "    drive.mount(\"/content/drive\")\n",
    "else:\n",
    "    print(\"Google Drive já montado.\")\n",
    "\n",
    "def _is_csv_filename(name: str) -> bool:\n",
    "    return name.lower().endswith(\".csv\")\n",
    "\n",
    "def _mensagem_skynet_ok():\n",
    "    # Mensagem adicional isolada (Skynet)\n",
    "    display(HTML(\n",
    "        '<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Munição carregada.🧨'\n",
    "                 '</div>'\n",
    "    ))\n",
    "\n",
    "def _save_bytes_as_input_csv(name: str, data: bytes):\n",
    "    if not _is_csv_filename(name):\n",
    "        raise ValueError(f\"O arquivo '{name}' não possui extensão .csv.\")\n",
    "    with open(TARGET_PATH, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Arquivo '{name}' salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()\n",
    "\n",
    "def _copy_drive_file_to_input_csv(src_path: str):\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"O caminho '{src_path}' não existe.\")\n",
    "    if not _is_csv_filename(src_path):\n",
    "        raise ValueError(f\"O arquivo '{src_path}' não possui extensão .csv.\")\n",
    "    shutil.copyfile(src_path, TARGET_PATH)\n",
    "    print(f\"Arquivo do Drive copiado e salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_gyt66h8g6v"
   },
   "source": [
    "####**Sub-etapa:** Opção de upload do input.csv pelo Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "vxuBXClP8hyn"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "def escolher_csv_no_drive(raiz=\"/content/drive/MyDrive\", max_listar=200):\n",
    "    print(f\"Procurando arquivos .csv em: {raiz} (pode levar alguns segundos)...\")\n",
    "    padrao = os.path.join(raiz, \"**\", \"*.csv\")\n",
    "    arquivos = glob.glob(padrao, recursive=True)\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum .csv encontrado nessa pasta.\")\n",
    "        caminho = input(\"Cole o caminho COMPLETO do .csv no Drive (ou Enter p/ cancelar): \").strip()\n",
    "        if caminho:\n",
    "            _copy_drive_file_to_input_csv(caminho)\n",
    "        else:\n",
    "            print(\"Operação cancelada.\")\n",
    "        return\n",
    "\n",
    "    arquivos = sorted(arquivos)[:max_listar]\n",
    "    print(f\"Encontrados {len(arquivos)} arquivo(s).\")\n",
    "    for i, p in enumerate(arquivos, 1):\n",
    "        print(f\"[{i:03}] {p}\")\n",
    "\n",
    "    escolha = input(\"\\nDigite o número do arquivo desejado (ou cole o caminho absoluto): \").strip()\n",
    "\n",
    "    if escolha.isdigit():\n",
    "        idx = int(escolha)\n",
    "        if 1 <= idx <= len(arquivos):\n",
    "            _copy_drive_file_to_input_csv(arquivos[idx-1])\n",
    "        else:\n",
    "            print(\"Índice inválido.\")\n",
    "    elif escolha:\n",
    "        _copy_drive_file_to_input_csv(escolha)\n",
    "    else:\n",
    "        print(\"Operação cancelada.\")\n",
    "\n",
    "# ===== Execução da seleção no Drive =====\n",
    "raiz = input(\"Informe a pasta raiz para busca no Drive (Enter = /content/drive/MyDrive): \").strip()\n",
    "if not raiz:\n",
    "    raiz = \"/content/drive/MyDrive\"\n",
    "\n",
    "try:\n",
    "    escolher_csv_no_drive(raiz=raiz)\n",
    "except Exception as e:\n",
    "    print(f\"Erro na seleção via Drive: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMmtkD4k8sUk"
   },
   "source": [
    "####**Sub-etapa:** Opção de upload do input.csv pelo FileLocal (diretório)\n",
    "---\n",
    "Implementação em ERRO no Colab - código desativado\n",
    "\n",
    "Implementar correção **TODO[001]** *prioridade baixa*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sSB5PJGf8tHP"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#from google.colab import files\n",
    "#\n",
    "#print(\"Selecione um arquivo .csv do seu computador para enviar.\")\n",
    "#uploaded = files.upload()  # abre o seletor do Colab\n",
    "#\n",
    "#if not uploaded:\n",
    "#    print(\"Nenhum arquivo foi carregado.\")\n",
    "#else:\n",
    "#    # pega o primeiro arquivo enviado\n",
    "#    name, data = next(iter(uploaded.items()))\n",
    "#    try:\n",
    "#        _save_bytes_as_input_csv(name, data)\n",
    "#    except Exception as e:\n",
    "#        print(f\"Erro no upload local: {e}\")\n",
    "# Mensagem adicional isolada (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\"><b>🤖 Skynet</b>: Detectado ataque da Resistência. Trecho de código inoperante. Salvaguardas ativadas. É possível prosseguir com a missão em segurança.</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR0mq0da9AOQ"
   },
   "source": [
    "###**Etapa 6:** Análise de cabeçalho - *header*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "eiDIm6Ub9Pa3"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#cabeçalho vertical / header (autossuficiente)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#normaliza INPUT_DIR para Path, com fallbacks sensatos\n",
    "try:\n",
    "    INPUT_DIR  # pode não existir se a célula [1] não tiver sido executada\n",
    "except NameError:\n",
    "    INPUT_DIR = None\n",
    "\n",
    "if INPUT_DIR is None:\n",
    "    #tenta caminhos mais comuns no seu fluxo\n",
    "    candidates = [\n",
    "        \"/content/drive/MyDrive/Notebooks/data-analysis/input\",\n",
    "        \"/content/data-analysis/input\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if os.path.isdir(c):\n",
    "            INPUT_DIR = c\n",
    "            break\n",
    "    if INPUT_DIR is None:\n",
    "        raise RuntimeError(\"INPUT_DIR não definido e pastas padrão não existem. Execute a etapa [1]/[2] antes.\")\n",
    "\n",
    "#garante que INPUT_DIR é Path (mesmo que tenha vindo como string)\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"não encontrei {SRC}. execute a etapa [2] para fazer o upload.\")\n",
    "\n",
    "#lê apenas o cabeçalho (nrows=0), separador ';' e BOM\n",
    "df_head = pd.read_csv(SRC, sep=';', encoding='utf-8-sig', nrows=0)\n",
    "cols = list(df_head.columns)\n",
    "\n",
    "print(\"Cabeçalho (uma coluna por linha):\")\n",
    "for c in cols:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-xRtOiN9W3y"
   },
   "source": [
    "###**Etapa 7:** Análise superficial da tipologia dos dados\n",
    "Amostra dos primeiros 5000 registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JeYvY5Ud9cZB"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#inferência de tipos + estatísticas de frequência por coluna (com caso \"todos distintos\")\n",
    "import re\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse as dtparse\n",
    "from unidecode import unidecode\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "SAMPLE_ROWS = 5000  #ajuste conforme necessário\n",
    "\n",
    "#lê amostra como texto puro; usa DataFrame.map (applymap foi deprecado)\n",
    "df = pd.read_csv(\n",
    "    SRC, sep=';', encoding='utf-8-sig',\n",
    "    dtype=str, nrows=SAMPLE_ROWS, keep_default_na=False\n",
    ").map(lambda x: x.strip())\n",
    "\n",
    "CNPJ_RX     = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "BOOL_TRUE   = {\"true\",\"t\",\"1\",\"y\",\"yes\",\"sim\",\"s\",\"verdadeiro\"}\n",
    "BOOL_FALSE  = {\"false\",\"f\",\"0\",\"n\",\"no\",\"nao\",\"não\",\"falso\"}\n",
    "DATE_RX     = re.compile(r\"^(\\d{2}/\\d{2}/\\d{4}|\\d{4}-\\d{2}-\\d{2})$\")\n",
    "TIME_RX     = re.compile(r\"^\\d{2}:\\d{2}(:\\d{2})?$\")\n",
    "\n",
    "def is_bool(series):\n",
    "    vals = {unidecode(str(v)).strip().lower() for v in series if str(v).strip() != \"\"}\n",
    "    return len(vals) > 0 and all(v in (BOOL_TRUE | BOOL_FALSE) for v in vals)\n",
    "\n",
    "def is_cnpj(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    return sum(bool(CNPJ_RX.match(v)) for v in vals) / len(vals) > 0.9\n",
    "\n",
    "def is_int(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\")\n",
    "        return re.fullmatch(r\"-?\\d+\", s2) is not None\n",
    "    return all(ok(v) for v in vals)\n",
    "\n",
    "def is_float_ptbr(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try: float(s2); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\",\" in v for v in vals)\n",
    "\n",
    "def is_float_dot(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        try: float(s); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\".\" in v and not v.endswith(\".\") for v in vals)\n",
    "\n",
    "def is_date_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(DATE_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_time_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(TIME_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_datetime(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:200]\n",
    "    def looks_like_datetime(s):\n",
    "        has_sep = (\"/\" in s or \"-\" in s) and (\":\" in s)\n",
    "        if not has_sep: return False\n",
    "        try:\n",
    "            pd.to_datetime(s, dayfirst=True, errors=\"raise\")\n",
    "            return True\n",
    "        except:\n",
    "            try:\n",
    "                dtparse(s, dayfirst=True, fuzzy=False)\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "    ok = sum(looks_like_datetime(v) for v in sample)\n",
    "    return ok / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_category(series, max_unique=20, max_ratio=0.02):\n",
    "    n = len(series)\n",
    "    if n == 0: return False\n",
    "    uniq = set(v for v in series if str(v).strip() != \"\")\n",
    "    ratio = len(uniq) / n\n",
    "    return (len(uniq) <= max_unique) or (ratio <= max_ratio)\n",
    "\n",
    "def recommend_dtype(col):\n",
    "    s = col.astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    if s_nonempty.empty:\n",
    "        return \"string (vazio/NA)\"\n",
    "    if is_cnpj(s_nonempty):        return \"CNPJ (string formatado)\"\n",
    "    if is_bool(s_nonempty):        return \"boolean\"\n",
    "    if is_int(s_nonempty):         return \"int64\"\n",
    "    if is_float_ptbr(s_nonempty):  return \"float64 (decimal=','; milhar='.')\"\n",
    "    if is_float_dot(s_nonempty):   return \"float64 (decimal='.')\"\n",
    "    if is_date_only(s_nonempty):   return \"date (datetime64[ns])\"\n",
    "    if is_time_only(s_nonempty):   return \"time (string/Timedelta)\"\n",
    "    if is_datetime(s_nonempty):    return \"datetime (datetime64[ns])\"\n",
    "    if is_category(s):             return \"category (string)\"\n",
    "    return \"string\"\n",
    "\n",
    "def _fmt_val(x, maxlen=120):\n",
    "    s = str(x)\n",
    "    return (s[: maxlen-3] + \"...\") if len(s) > maxlen else s\n",
    "\n",
    "print(f\"estatísticas baseadas em até {SAMPLE_ROWS} linhas lidas.\\n\")\n",
    "for c in df.columns:\n",
    "    s = df[c].astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    dtype_sug = recommend_dtype(s)\n",
    "\n",
    "    if len(s_nonempty) == 0:\n",
    "        print(f\"{c} — {dtype_sug} — distintos=0 — (sem dados não vazios)\")\n",
    "        continue\n",
    "\n",
    "    vc = s_nonempty.value_counts(dropna=False)\n",
    "    uniq_count = int(vc.shape[0])\n",
    "\n",
    "    #caso especial: todos distintos (máxima frequência == 1)\n",
    "    if int(vc.max()) == 1:\n",
    "        print(f\"{c} — {dtype_sug} — distintos={uniq_count} — todos os dados são distintos\")\n",
    "        continue\n",
    "\n",
    "    most_val = vc.idxmax()\n",
    "    most_cnt = int(vc.max())\n",
    "\n",
    "    min_cnt = int(vc.min())\n",
    "    least_candidates = vc[vc == min_cnt].sort_index()\n",
    "    least_val = least_candidates.index[0]\n",
    "    least_cnt = min_cnt\n",
    "\n",
    "    print(f\"{c} — {dtype_sug} — distintos={uniq_count} — mais_frequente='{_fmt_val(most_val)}' ({most_cnt}) — menos_frequente='{_fmt_val(least_val)}' ({least_cnt})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kat-QJaeGB2y"
   },
   "source": [
    "###**Etapa 8:** Análise detalhada da tipologia dos dados\n",
    "---\n",
    "Aplicada a todos os dados do arquivo, sem filtro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "rDN3-7olHkbL"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#núcleo de análise consolidada (sem geração de relatórios/figuras)\n",
    "#imports principais\n",
    "import os, re, math\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#imports opcionais (anomalias e p-valor para benford)\n",
    "try:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from scipy.stats import chisquare, median_abs_deviation\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    #fallback simples para MAD\n",
    "    def median_abs_deviation(x, scale=1.4826, nan_policy='omit'):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size == 0:\n",
    "            return np.nan\n",
    "        med = np.median(x)\n",
    "        mad = np.median(np.abs(x - med))*scale\n",
    "        return mad\n",
    "\n",
    "#normaliza pastas padrão (se etapas anteriores não definiram)\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"não encontrei {SRC}. execute o upload na etapa [2].\")\n",
    "\n",
    "#leitura completa do csv como texto; análise operará com coerções internas\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#helpers de coerção e detecção\n",
    "EMAIL_RX = re.compile(r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\")\n",
    "URL_RX   = re.compile(r\"^https?://\", re.I)\n",
    "CPF_RX   = re.compile(r\"^\\d{3}\\.?\\d{3}\\.?\\d{3}-\\d{2}$\")\n",
    "CNPJ_RX  = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "def detect_numeric(series: pd.Series, thr_ok=0.9):\n",
    "    num = to_float_ptbr_series(series)\n",
    "    ratio = 1.0 - num.isna().mean()\n",
    "    return (ratio >= thr_ok), num\n",
    "\n",
    "def detect_datetime(series: pd.Series, thr_ok=0.9):\n",
    "    #formatos comuns pt-br/iso com/sem tempo\n",
    "    candidate_formats = [\n",
    "        \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%d/%m/%Y %H:%M:%S\",\n",
    "        \"%Y-%m-%d\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d-%m-%Y\", \"%d-%m-%Y %H:%M\", \"%d-%m-%Y %H:%M:%S\"\n",
    "    ]\n",
    "    s = series.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    if not (s.str.contains(\"/\", na=False) | s.str.contains(\"-\", na=False)).any():\n",
    "        return False, None, None\n",
    "    best_fmt, best_ratio, best_parsed = None, -1.0, None\n",
    "    for fmt in candidate_formats:\n",
    "        parsed = pd.to_datetime(s, errors=\"coerce\", format=fmt)\n",
    "        ratio = 1.0 - parsed.isna().mean()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio, best_fmt, best_parsed = ratio, fmt, parsed\n",
    "        if ratio >= thr_ok:\n",
    "            break\n",
    "    if best_ratio >= thr_ok:\n",
    "        return True, best_parsed, best_fmt\n",
    "    parsed_fb = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    ratio_fb = 1.0 - parsed_fb.isna().mean()\n",
    "    if ratio_fb >= thr_ok:\n",
    "        return True, parsed_fb, \"fallback-dateutil(dayfirst=True)\"\n",
    "    return False, None, None\n",
    "\n",
    "def semantic_type(series: pd.Series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    vals = s[s != \"\"].head(5000)\n",
    "    if vals.empty:\n",
    "        return None\n",
    "    email_ratio = (vals.str.match(EMAIL_RX)).mean()\n",
    "    url_ratio   = (vals.str.match(URL_RX)).mean()\n",
    "    cpf_ratio   = (vals.str.match(CPF_RX)).mean()\n",
    "    cnpj_ratio  = (vals.str.match(CNPJ_RX)).mean()\n",
    "    candidates = []\n",
    "    if email_ratio>0.9: candidates.append(\"email\")\n",
    "    if url_ratio>0.9: candidates.append(\"url\")\n",
    "    if cpf_ratio>0.9: candidates.append(\"cpf\")\n",
    "    if cnpj_ratio>0.9: candidates.append(\"cnpj\")\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "#mapeamento de tipos\n",
    "col_types, coerced, dt_formats = {}, {}, {}\n",
    "for c in df_raw.columns:\n",
    "    s = df_raw[c]\n",
    "    #booleano raso\n",
    "    s_norm = s.astype(str).str.strip().str.lower()\n",
    "    bool_map = {\"true\":True,\"t\":True,\"1\":True,\"y\":True,\"yes\":True,\"sim\":True,\"s\":True,\"verdadeiro\":True,\n",
    "                \"false\":False,\"f\":False,\"0\":False,\"n\":False,\"no\":False,\"nao\":False,\"não\":False,\"falso\":False}\n",
    "    as_bool = s_norm.map(bool_map).where(s_norm.isin(bool_map.keys()))\n",
    "    bool_ratio = 1.0 - as_bool.isna().mean()\n",
    "    is_num, as_num = detect_numeric(s)\n",
    "    is_dt, as_dt, fmt_dt = detect_datetime(s)\n",
    "    if bool_ratio >= 0.9:\n",
    "        col_types[c] = \"bool\"; coerced[c]=as_bool\n",
    "    elif is_num:\n",
    "        frac = np.modf(as_num.dropna().values)[0] if as_num.notna().any() else np.array([])\n",
    "        if as_num.notna().any() and np.allclose(frac, 0.0):\n",
    "            col_types[c] = \"int\"; coerced[c]=as_num.astype(\"Int64\")\n",
    "        else:\n",
    "            col_types[c] = \"float\"; coerced[c]=as_num.astype(float)\n",
    "    elif is_dt:\n",
    "        col_types[c] = \"datetime\"; coerced[c]=as_dt; dt_formats[c]=fmt_dt\n",
    "    else:\n",
    "        col_types[c] = \"object\"; coerced[c]=s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "\n",
    "#dataframe tipado (leve)\n",
    "df = pd.DataFrame(coerced)\n",
    "\n",
    "#profiling básico\n",
    "profile_cols = {}\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    s_raw = df_raw[c]\n",
    "    n = len(s)\n",
    "    n_null = int(s.isna().sum())\n",
    "    #distintos não vazios\n",
    "    nonnull = s.dropna()\n",
    "    n_distinct = int(nonnull.nunique())\n",
    "    #frequências\n",
    "    most = None; least = None; all_distinct = False\n",
    "    if nonnull.empty:\n",
    "        all_distinct = False\n",
    "    else:\n",
    "        vc = nonnull.value_counts()\n",
    "        if vc.max()==1:\n",
    "            all_distinct = True\n",
    "        else:\n",
    "            most = {\"value\": vc.index[0], \"count\": int(vc.iloc[0]), \"prop\": float(vc.iloc[0]/nonnull.shape[0])}\n",
    "            min_cnt = int(vc.min())\n",
    "            least_val = vc[vc==min_cnt].sort_index().index[0]\n",
    "            least = {\"value\": least_val, \"count\": min_cnt, \"prop\": float(min_cnt/nonnull.shape[0])}\n",
    "    #comprimentos\n",
    "    lens = s_raw.astype(str).str.len()\n",
    "    lens = lens.replace({0: np.nan})  #ignora vazios na estatística de len\n",
    "    len_stats = None\n",
    "    if lens.notna().any():\n",
    "        len_stats = {\n",
    "            \"min\": int(lens.min()),\n",
    "            \"max\": int(lens.max()),\n",
    "            \"mean\": float(lens.mean()),\n",
    "            \"q1\": float(lens.quantile(0.25)),\n",
    "            \"median\": float(lens.median()),\n",
    "            \"q3\": float(lens.quantile(0.75))\n",
    "        }\n",
    "    #padrões simples por amostragem\n",
    "    sample_vals = nonnull.astype(str).head(200).tolist()\n",
    "    regex_examples = []\n",
    "    rx_date1 = re.compile(r\"^\\d{2}/\\d{2}/\\d{4}\")\n",
    "    rx_date2 = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\")\n",
    "    rx_num_pt = re.compile(r\"^-?(\\d{1,3}(\\.\\d{3})+|\\d+)(,\\d+)?$\")\n",
    "    rx_num_dot= re.compile(r\"^-?\\d+(\\.\\d+)?$\")\n",
    "    for v in sample_vals[:20]:\n",
    "        pat = None\n",
    "        if EMAIL_RX.match(v): pat=\"email\"\n",
    "        elif URL_RX.match(v): pat=\"url\"\n",
    "        elif CPF_RX.match(v): pat=\"cpf\"\n",
    "        elif CNPJ_RX.match(v): pat=\"cnpj\"\n",
    "        elif rx_date1.match(v): pat=\"dd/mm/aaaa[...]\"\n",
    "        elif rx_date2.match(v): pat=\"aaaa-mm-dd[...]\"\n",
    "        elif rx_num_pt.match(v): pat=\"num-ptbr\"\n",
    "        elif rx_num_dot.match(v): pat=\"num-dot\"\n",
    "        else: pat=\"texto-livre\"\n",
    "        regex_examples.append({\"example\": v[:120], \"pattern\": pat})\n",
    "    profile_cols[c]={\n",
    "        \"type\": col_types[c],\n",
    "        \"semantic\": semantic_type(s_raw),\n",
    "        \"nulls\": n_null,\n",
    "        \"distinct_nonnull\": n_distinct,\n",
    "        \"all_distinct\": all_distinct,\n",
    "        \"most_frequent\": most,\n",
    "        \"least_frequent\": None if all_distinct else least,\n",
    "        \"length_stats\": len_stats,\n",
    "        \"datetime_format\": dt_formats.get(c)\n",
    "    }\n",
    "\n",
    "#estatísticas descritivas e outliers\n",
    "eda_numeric = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\"):\n",
    "        x = df[c].astype(float)\n",
    "        x = x.dropna()\n",
    "        if x.empty:\n",
    "            continue\n",
    "        q1,q3 = np.nanpercentile(x, [25,75])\n",
    "        iqr = q3 - q1\n",
    "        lo,hi = q1-1.5*iqr, q3+1.5*iqr\n",
    "        out_iqr = int(((x<lo)|(x>hi)).sum())\n",
    "        mad = float(median_abs_deviation(x, scale=1.4826)) if x.size>0 else np.nan\n",
    "        z_rob = None\n",
    "        if not math.isnan(mad) and mad>0:\n",
    "            z_rob = np.abs((x - np.median(x))/mad)\n",
    "        out_mad = int((z_rob is not None) and (z_rob>3.5).sum())\n",
    "        std = float(np.nanstd(x, ddof=1)) if x.size>1 else np.nan\n",
    "        mean = float(np.nanmean(x)) if x.size>0 else np.nan\n",
    "        kurt = float(((x-mean)**4).mean()/(std**4)-3.0) if (x.size>2 and std and std>0) else np.nan\n",
    "        skew = float(((x-mean)**3).mean()/(std**3)) if (x.size>2 and std and std>0) else np.nan\n",
    "        eda_numeric[c]={\n",
    "            \"n\": int(x.size),\n",
    "            \"min\": float(np.nanmin(x)),\n",
    "            \"q1\": float(q1),\n",
    "            \"median\": float(np.nanmedian(x)),\n",
    "            \"q3\": float(q3),\n",
    "            \"max\": float(np.nanmax(x)),\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"iqr\": float(iqr),\n",
    "            \"outliers_iqr\": out_iqr,\n",
    "            \"mad\": mad,\n",
    "            \"outliers_mad\": out_mad,\n",
    "            \"skew\": skew,\n",
    "            \"kurtosis_excess\": kurt\n",
    "        }\n",
    "\n",
    "#dados categóricos/objeto (entropia e top-k)\n",
    "def entropy_shannon(series: pd.Series):\n",
    "    s = series.dropna()\n",
    "    if s.empty: return 0.0\n",
    "    vc = s.value_counts(normalize=True)\n",
    "    return float(-(vc*np.log2(vc)).sum())\n",
    "\n",
    "eda_categorical = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"object\",\"bool\"):\n",
    "        s = df[c]\n",
    "        s2 = s.dropna()\n",
    "        if s2.empty:\n",
    "            continue\n",
    "        vc = s2.value_counts()\n",
    "        uniq = int(vc.shape[0])\n",
    "        all_dist = int(vc.max()==1)\n",
    "        topk = [{\"value\": str(idx)[:120], \"count\": int(cnt)} for idx,cnt in vc.head(10).items()]\n",
    "        ent = entropy_shannon(s2)\n",
    "        eda_categorical[c]={\n",
    "            \"distinct\": uniq,\n",
    "            \"all_distinct\": bool(all_dist),\n",
    "            \"top10\": topk,\n",
    "            \"entropy_shannon\": float(ent)\n",
    "        }\n",
    "\n",
    "#datas/tempos\n",
    "eda_datetime = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c]==\"datetime\":\n",
    "        ds = df[c].dropna()\n",
    "        if ds.empty:\n",
    "            continue\n",
    "        per_day = ds.dt.date.value_counts().sort_index()\n",
    "        eda_datetime[c]={\n",
    "            \"format\": dt_formats.get(c),\n",
    "            \"min\": str(ds.min()),\n",
    "            \"max\": str(ds.max()),\n",
    "            \"unique_days\": int(per_day.shape[0]),\n",
    "            \"mean_per_day\": float(per_day.mean())\n",
    "        }\n",
    "\n",
    "#faltantes e duplicados\n",
    "missing = {\n",
    "    \"by_column_pct\": {c: float(df[c].isna().mean()*100.0) for c in df.columns},\n",
    "    \"duplicates_rows\": int(df.duplicated().sum())\n",
    "}\n",
    "#coocorrência simples de ausências (matriz de proporção conjunta)\n",
    "miss_mat = pd.DataFrame(index=df.columns, columns=df.columns, dtype=float)\n",
    "isna_df = df.isna()\n",
    "for i,a in enumerate(df.columns):\n",
    "    for b in df.columns[i:]:\n",
    "        both = (isna_df[a] & isna_df[b]).mean()\n",
    "        miss_mat.loc[a,b] = miss_mat.loc[b,a] = float(both)\n",
    "missing[\"cooccurrence_matrix\"] = miss_mat\n",
    "\n",
    "#FDs/CFDs aproximadas (unários) e sugestões de DCs\n",
    "fds = []     #X->Y exata (cobertura 100%)\n",
    "cfds = []    #X->Y quase: cobertura >=thr\n",
    "thr_cfd = 0.98\n",
    "for a in df.columns:\n",
    "    ga = df.groupby(a, dropna=False)\n",
    "    #a é chave candidata?\n",
    "    if ga.size().max()==1:\n",
    "        fds.append({\"determinant\":[a], \"key\":True})\n",
    "    #FD aproximada a->b\n",
    "    for b in df.columns:\n",
    "        if a==b: continue\n",
    "        nun = ga[b].nunique(dropna=False)\n",
    "        cov = float((nun<=1).mean())\n",
    "        if cov==1.0:\n",
    "            fds.append({\"determinant\":[a], \"implies\": b, \"coverage\": 1.0})\n",
    "        elif cov>=thr_cfd:\n",
    "            cfds.append({\"determinant\":[a], \"implies\": b, \"coverage\": cov})\n",
    "\n",
    "#denial constraints sugeridas (heurísticas)\n",
    "#exemplos: não-negatividade para colunas com 'valor', limites plausíveis para idade, datas início<=fim\n",
    "dcs = []\n",
    "#não-negatividade\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(valor|amount|price|quant|qty|pag|pago)\", c, re.I):\n",
    "        neg = int((df[c].astype(float)<0).sum())\n",
    "        dcs.append({\"constraint\": f\"{c}>=0\", \"violations\": neg})\n",
    "#idade plausível\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(idade|age)\", c, re.I):\n",
    "        v = df[c].astype(float)\n",
    "        viol = int(((v<0)|(v>120)).sum())\n",
    "        dcs.append({\"constraint\": f\"0<= {c} <=120\", \"violations\": viol})\n",
    "#data início<=fim\n",
    "date_cols = [c for c in df.columns if col_types[c]==\"datetime\"]\n",
    "for a in date_cols:\n",
    "    for b in date_cols:\n",
    "        if a==b: continue\n",
    "        if re.search(r\"(inicio|start|begin)\", a, re.I) and re.search(r\"(fim|end|finish)\", b, re.I):\n",
    "            viol = int((df[a].notna() & df[b].notna() & (df[b]<df[a])).sum())\n",
    "            dcs.append({\"constraint\": f\"{a}<= {b}\", \"violations\": viol})\n",
    "\n",
    "#correlações\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "corr_pearson = None; corr_spearman = None\n",
    "if len(num_cols)>=2:\n",
    "    df_num = df[num_cols].astype(float)\n",
    "    corr_pearson = df_num.corr(method=\"pearson\")\n",
    "    corr_spearman = df_num.corr(method=\"spearman\")\n",
    "\n",
    "#anomalias (opcional)\n",
    "anomalies = {}\n",
    "if SKLEARN_OK and len(num_cols)>=1:\n",
    "    X = df[num_cols].astype(float).fillna(df[num_cols].astype(float).median())\n",
    "    #isolation forest\n",
    "    try:\n",
    "        iso = IsolationForest(n_estimators=200, contamination='auto', random_state=42)\n",
    "        iso_scores = -iso.fit_predict(X)  #1 normal, -1 anomalia -> invertido\n",
    "        iso_dec = iso.decision_function(X)  #menor = mais anômalo\n",
    "        iso_rank = np.argsort(iso_dec)[: min(50, len(iso_dec))].tolist()\n",
    "        anomalies[\"isolation_forest\"] = {\"top_idx\": iso_rank, \"decision_function\": iso_dec.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"isolation_forest_error\"] = str(e)\n",
    "    #lof\n",
    "    try:\n",
    "        lof = LocalOutlierFactor(n_neighbors=min(20, max(2, X.shape[0]-1)), contamination='auto')\n",
    "        lof_pred = lof.fit_predict(X)  #-1 outlier\n",
    "        lof_score = -lof.negative_outlier_factor_  #maior = mais anômalo\n",
    "        lof_rank = np.argsort(-lof_score)[: min(50, len(lof_score))].tolist()\n",
    "        anomalies[\"lof\"] = {\"top_idx\": lof_rank, \"score\": lof_score.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"lof_error\"] = str(e)\n",
    "else:\n",
    "    anomalies[\"note\"] = \"sklearn indisponível ou sem colunas numéricas suficientes\"\n",
    "\n",
    "#lei de benford (primeiro dígito) para colunas monetárias prováveis\n",
    "def first_digit_series(x: pd.Series):\n",
    "    x = x.astype(float)\n",
    "    x = x[~x.isna() & (x!=0)]\n",
    "    x = x.abs()\n",
    "    s = x.astype(str).str.replace(\".\", \"\", regex=False).str.lstrip(\"0\")\n",
    "    s = s[s.str.len()>0].str[0]\n",
    "    s = s[s.str.isnumeric()].astype(int)\n",
    "    return s\n",
    "\n",
    "benford = {}\n",
    "monetary_cols = [c for c in num_cols if re.search(r\"(valor|amount|price|pago|pagamento|receita|despesa)\", c, re.I)]\n",
    "for c in monetary_cols:\n",
    "    try:\n",
    "        d1 = first_digit_series(df[c])\n",
    "        if d1.empty:\n",
    "            continue\n",
    "        obs_counts = d1.value_counts().reindex(range(1,10), fill_value=0).astype(int)\n",
    "        obs_probs = obs_counts/obs_counts.sum()\n",
    "        exp_probs = np.array([math.log10(1+1/d) for d in range(1,10)])\n",
    "        exp_probs = exp_probs/exp_probs.sum()\n",
    "        chi2_stat = float(((obs_probs-exp_probs)**2/exp_probs).sum()*obs_counts.sum())\n",
    "        p_value = None\n",
    "        if SCIPY_OK:\n",
    "            #qui-quadrado com gl=8\n",
    "            p_value = float(1.0 - chisquare(f_obs=obs_counts, f_exp=exp_probs*obs_counts.sum()).cdf)\n",
    "        benford[c]={\n",
    "            \"observed_counts\": obs_counts.to_dict(),\n",
    "            \"observed_probs\": {int(k): float(v) for k,v in obs_probs.items()},\n",
    "            \"expected_probs\": {d: float(p) for d,p in zip(range(1,9+1), exp_probs)},\n",
    "            \"chi2_stat\": chi2_stat,\n",
    "            \"p_value\": p_value\n",
    "        }\n",
    "    except Exception as e:\n",
    "        benford[c]={\"error\": str(e)}\n",
    "\n",
    "#empacotar tudo em um único artefato de análise\n",
    "ANALYSIS = {\n",
    "    \"stamp\": datetime.now().strftime(\"%d%m%y-%H%M\"),\n",
    "    \"shape\": {\"rows\": int(df_raw.shape[0]), \"cols\": int(df_raw.shape[1])},\n",
    "    \"types\": col_types,\n",
    "    \"datetime_formats\": dt_formats,\n",
    "    \"profile\": profile_cols,\n",
    "    \"eda\": {\n",
    "        \"numeric\": eda_numeric,\n",
    "        \"categorical\": eda_categorical,\n",
    "        \"datetime\": eda_datetime\n",
    "    },\n",
    "    \"missingness\": {\n",
    "        \"by_column_pct\": missing[\"by_column_pct\"],\n",
    "        \"duplicates_rows\": missing[\"duplicates_rows\"],\n",
    "        \"cooccurrence_matrix\": missing[\"cooccurrence_matrix\"]\n",
    "    },\n",
    "    \"fds\": fds,\n",
    "    \"cfds\": cfds,\n",
    "    \"denial_constraints_suggested\": dcs,\n",
    "    \"correlations\": {\n",
    "        \"pearson\": corr_pearson,\n",
    "        \"spearman\": corr_spearman\n",
    "    },\n",
    "    \"anomalies\": anomalies,\n",
    "    \"benford\": benford,\n",
    "    \"notes\": {\n",
    "        \"sklearn_available\": SKLEARN_OK,\n",
    "        \"scipy_available\": SCIPY_OK\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"análise concluída e armazenada em ANALYSIS.\")\n",
    "print(\"- você pode inspecionar, por exemplo: ANALYSIS['profile']['<nome_da_coluna>']\")\n",
    "print(\"- pronto para a etapa de geração de relatórios quando desejar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGbRVi2J72Xu"
   },
   "source": [
    "###**Etapa 9:** Geração de relatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "swxW8aZJ76ym"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#geração de relatórios: TXT, HTML (imagens embutidas), PNGs (imagens/) e PDF completo\n",
    "#imports\n",
    "import os, io, base64\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#libs para pdf (tabelas completas)\n",
    "try:\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, Table, TableStyle, PageBreak\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib import colors\n",
    "    REPORTLAB_OK = True\n",
    "except Exception:\n",
    "    REPORTLAB_OK = False\n",
    "\n",
    "#checagens e caminhos\n",
    "try:\n",
    "    ANALYSIS\n",
    "except NameError:\n",
    "    raise RuntimeError(\"ANALYSIS não encontrado. execute a etapa [5] antes.\")\n",
    "\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "try:\n",
    "    OUTPUT_DIR\n",
    "except NameError:\n",
    "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/output\")\n",
    "\n",
    "INPUT_DIR  = Path(INPUT_DIR)\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#stamp e diretórios de saída\n",
    "stamp = ANALYSIS.get(\"stamp\", datetime.now().strftime(\"%d%m%y-%H%M\"))\n",
    "RUN_DIR = OUTPUT_DIR / stamp\n",
    "IMG_DIR = RUN_DIR / \"imagens\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#arquivo fonte\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"não encontrei {SRC}. execute a etapa [2] e [5].\")\n",
    "\n",
    "#utils\n",
    "def save_fig(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=120, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def img_to_data_uri(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    return \"data:image/png;base64,{}\".format(b64)\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "#carregar df base para gráficos\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#tipos e colunas numéricas conforme ANALYSIS\n",
    "col_types = ANALYSIS[\"types\"]\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "\n",
    "#gerar imagens principais\n",
    "#ausências\n",
    "miss_pct = pd.Series(ANALYSIS[\"missingness\"][\"by_column_pct\"]).sort_values(ascending=False)\n",
    "plt.figure(figsize=(max(6, 0.4*len(miss_pct)+2), 4.5))\n",
    "plt.bar(miss_pct.index, miss_pct.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"% ausente\")\n",
    "plt.title(\"ausência de valores por coluna\")\n",
    "save_fig(IMG_DIR / \"missing_bar.png\")\n",
    "\n",
    "#correlação (pearson) se houver ≥2 numéricas\n",
    "if len(num_cols) >= 2 and ANALYSIS[\"correlations\"][\"pearson\"] is not None:\n",
    "    corr = pd.DataFrame(ANALYSIS[\"correlations\"][\"pearson\"])\n",
    "    plt.figure(figsize=(max(6, 0.6*len(corr)+2), max(5, 0.6*len(corr)+2)))\n",
    "    im = plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=range(len(corr.columns)), labels=corr.columns, rotation=90)\n",
    "    plt.yticks(ticks=range(len(corr.index)), labels=corr.index)\n",
    "    plt.title(\"matriz de correlação (pearson)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    save_fig(IMG_DIR / \"corr_heatmap.png\")\n",
    "\n",
    "#histogramas e boxplots por coluna numérica\n",
    "for c in num_cols:\n",
    "    x = to_float_ptbr_series(df_raw[c]).dropna()\n",
    "    if x.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=30)\n",
    "    plt.title(\"histograma - {}\".format(c))\n",
    "    plt.xlabel(c); plt.ylabel(\"frequência\")\n",
    "    save_fig(IMG_DIR / \"hist_{}.png\".format(c))\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.boxplot(x.values, vert=True, whis=1.5, showfliers=True)\n",
    "    plt.title(\"boxplot - {}\".format(c))\n",
    "    plt.ylabel(c)\n",
    "    save_fig(IMG_DIR / \"box_{}.png\".format(c))\n",
    "\n",
    "#1) relatório TXT (agrupado por coluna)\n",
    "txt_lines = []\n",
    "shape = ANALYSIS[\"shape\"]\n",
    "txt_lines.append(\"arquivo: {}\".format(SRC.name))\n",
    "txt_lines.append(\"linhas (inclui cabeçalho): {}\".format(shape[\"rows\"]))\n",
    "txt_lines.append(\"colunas: {}\".format(shape[\"cols\"]))\n",
    "txt_lines.append(\"registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "txt_lines.append(\"\")\n",
    "\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    txt_lines.append(\"[coluna] {}\".format(c))\n",
    "    txt_lines.append(\"- tipo: {}\".format(kind))\n",
    "    if prof.get(\"semantic\"):\n",
    "        txt_lines.append(\"- tipo semântico: {}\".format(prof[\"semantic\"]))\n",
    "    txt_lines.append(\"- nulos: {}\".format(prof.get(\"nulls\", 0)))\n",
    "    txt_lines.append(\"- distintos (não vazios): {}\".format(prof.get(\"distinct_nonnull\", 0)))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        txt_lines.append(\"- todos os dados são distintos\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        if mf:\n",
    "            txt_lines.append(\"- mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            txt_lines.append(\"- menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        txt_lines.append(\"- comprimentos: min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            txt_lines.append(\"- numéricos: min={}, q1={}, mediana={}, q3={}, max={}\".format(ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"]))\n",
    "            txt_lines.append(\"- média={}, desvio padrão={}, iqr={}\".format(ed[\"mean\"], ed[\"std\"], ed[\"iqr\"]))\n",
    "            txt_lines.append(\"- outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]))\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            txt_lines.append(\"- formato detectado: {}\".format(dtc.get(\"format\")))\n",
    "            txt_lines.append(\"- intervalo temporal: {} → {}\".format(dtc[\"min\"], dtc[\"max\"]))\n",
    "            txt_lines.append(\"- dias únicos: {}, média por dia: {:.2f}\".format(dtc[\"unique_days\"], dtc[\"mean_per_day\"]))\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            txt_lines.append(\"- entropia de shannon: {:.4f}\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                topk = \", \".join([\"'{}' ({})\".format(t.get(\"value\"), t.get(\"count\")) for t in cat[\"top10\"]])\n",
    "                txt_lines.append(\"- top10: {}\".format(topk))\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben:\n",
    "        txt_lines.append(\"- benford (primeiro dígito):\")\n",
    "        txt_lines.append(\"  • qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    txt_lines.append(\"\")\n",
    "\n",
    "#salvar TXT\n",
    "txt_path = RUN_DIR / \"relatorio.txt\"\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(txt_lines))\n",
    "\n",
    "#2) relatório HTML com imagens embutidas\n",
    "html = []\n",
    "html.append(\"<html><head><meta charset='utf-8'><title>Relatório de Análise</title>\")\n",
    "html.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;margin:20px}h1,h2,h3{margin:8px 0}table{border-collapse:collapse;margin:10px 0}th,td{border:1px solid #ccc;padding:6px 8px;font-size:13px}code{background:#f5f5f5;padding:0 4px}</style>\")\n",
    "html.append(\"</head><body>\")\n",
    "html.append(\"<h1>Relatório de Análise — {}</h1>\".format(stamp))\n",
    "html.append(\"<p>Arquivo analisado: <b>{}</b></p>\".format(SRC.name))\n",
    "html.append(\"<p><a href='relatorio.txt'>Baixar relatório TXT</a></p>\")\n",
    "\n",
    "#sumário\n",
    "html.append(\"<h2>Sumário</h2>\")\n",
    "html.append(\"<ul>\")\n",
    "html.append(\"<li>Linhas (inclui cabeçalho): {}</li>\".format(shape[\"rows\"]))\n",
    "html.append(\"<li>Colunas: {}</li>\".format(shape[\"cols\"]))\n",
    "html.append(\"<li>Registros duplicados: {}</li>\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "html.append(\"</ul>\")\n",
    "\n",
    "#ausências\n",
    "miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "if miss_img.exists():\n",
    "    html.append(\"<h2>Ausência de valores</h2>\")\n",
    "    html.append(\"<img src='{}' alt='missing bar'/>\".format(img_to_data_uri(miss_img)))\n",
    "\n",
    "#correlação\n",
    "corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "if corr_img.exists():\n",
    "    html.append(\"<h2>Matriz de correlação</h2>\")\n",
    "    html.append(\"<img src='{}' alt='corr heatmap'/>\".format(img_to_data_uri(corr_img)))\n",
    "\n",
    "#por coluna\n",
    "html.append(\"<h2>Perfil por coluna</h2>\")\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    html.append(\"<h3>{}</h3>\".format(c))\n",
    "    html.append(\"<table>\")\n",
    "    html.append(\"<tr><th>Tipo</th><td>{}</td></tr>\".format(kind))\n",
    "    html.append(\"<tr><th>Nulos</th><td>{}</td></tr>\".format(prof.get(\"nulls\",0)))\n",
    "    html.append(\"<tr><th>Distintos (não vazios)</th><td>{}</td></tr>\".format(prof.get(\"distinct_nonnull\",0)))\n",
    "    if prof.get(\"semantic\"):\n",
    "        html.append(\"<tr><th>Tipo semântico</th><td>{}</td></tr>\".format(prof[\"semantic\"]))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        html.append(\"<tr><th>Frequências</th><td>todos os dados são distintos</td></tr>\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        freq_txt = []\n",
    "        if mf:\n",
    "            freq_txt.append(\"mais frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(mf[\"value\"])[:120], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            freq_txt.append(\"menos frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(lf[\"value\"])[:120], lf[\"count\"], lf[\"prop\"]*100))\n",
    "        if freq_txt:\n",
    "            html.append(\"<tr><th>Frequências</th><td>{}</td></tr>\".format(\" | \".join(freq_txt)))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        html.append(\"<tr><th>Comprimentos</th><td>min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}</td></tr>\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            html.append(\"<tr><th>Estatísticas</th><td>min={}, q1={}, mediana={}, q3={}, max={}\"\n",
    "                        \"<br/>média={}, desvio padrão={}, iqr={}\"\n",
    "                        \"<br/>outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}</td></tr>\".format(\n",
    "                            ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                            ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                            ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                        ))\n",
    "        hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "        box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "        figs = []\n",
    "        if hist_p.exists(): figs.append(\"<img src='{}' alt='hist {}'/>\".format(img_to_data_uri(hist_p), c))\n",
    "        if box_p.exists():  figs.append(\"<img src='{}' alt='box {}'/>\".format(img_to_data_uri(box_p), c))\n",
    "        if figs:\n",
    "            html.append(\"<tr><th>Gráficos</th><td>{}</td></tr>\".format(\"<br/>\".join(figs)))\n",
    "\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            html.append(\"<tr><th>Data/hora</th><td>formato detectado: {}<br/>intervalo: {} → {}<br/>dias únicos: {}, média por dia: {:.2f}</td></tr>\".format(\n",
    "                dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "            ))\n",
    "\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            html.append(\"<tr><th>Entropia</th><td>{:.4f}</td></tr>\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                rows = \"\".join([\"<tr><td>{}</td><td style='text-align:right'>{}</td></tr>\".format(str(t[\"value\"])[:120], t[\"count\"]) for t in cat[\"top10\"]])\n",
    "                html.append(\"<tr><th>Top 10</th><td><table><tr><th>Valor</th><th>Contagem</th></tr>\"+rows+\"</table></td></tr>\")\n",
    "\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben:\n",
    "        html.append(\"<tr><th>Benford</th><td>qui-quadrado={:.4f}, p-valor={}</td></tr>\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "\n",
    "#fds/cfds/dcs\n",
    "html.append(\"<h2>Regras sugeridas</h2>\")\n",
    "if ANALYSIS[\"fds\"]:\n",
    "    html.append(\"<h3>FDs</h3><ul>\")\n",
    "    for r in ANALYSIS[\"fds\"]:\n",
    "        if r.get(\"key\"):\n",
    "            html.append(\"<li>chave candidata: {}</li>\".format(\", \".join(r[\"determinant\"])))\n",
    "        else:\n",
    "            html.append(\"<li>{} → {} (100%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"]))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"cfds\"]:\n",
    "    html.append(\"<h3>CFDs (aproximadas)</h3><ul>\")\n",
    "    for r in ANALYSIS[\"cfds\"][:200]:\n",
    "        html.append(\"<li>{} → {} ({:.2f}%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"], r[\"coverage\"]*100))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "    html.append(\"<h3>Denial constraints</h3><ul>\")\n",
    "    for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        html.append(\"<li>{} — violações: {}</li>\".format(d[\"constraint\"], d[\"violations\"]))\n",
    "    html.append(\"</ul>\")\n",
    "\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "#salvar HTML\n",
    "html_path = RUN_DIR / \"relatorio.html\"\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(html))\n",
    "\n",
    "#4) pdf com as MESMAS infos (tabelas por coluna + imagens + regras)\n",
    "pdf_path = RUN_DIR / \"relatorio.pdf\"\n",
    "if REPORTLAB_OK:\n",
    "    styles = getSampleStyleSheet()\n",
    "    styles.add(ParagraphStyle(name=\"Small\", parent=styles[\"Normal\"], fontSize=9, leading=11))\n",
    "    table_style = TableStyle([\n",
    "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f0f0f0\")),\n",
    "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n",
    "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n",
    "    ])\n",
    "\n",
    "    def table_kv(rows):\n",
    "        #rows: list of (key, value(str))\n",
    "        data = [[\"Campo\",\"Valor\"]] + rows\n",
    "        t = Table(data, colWidths=[120, 360])\n",
    "        t.setStyle(table_style)\n",
    "        return t\n",
    "\n",
    "    story = []\n",
    "    story.append(Paragraph(\"Relatório de Análise — {}\".format(stamp), styles[\"Title\"]))\n",
    "    story.append(Spacer(1, 12))\n",
    "    story.append(Paragraph(\"Arquivo analisado: <b>{}</b>\".format(SRC.name), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Linhas: {} &nbsp;&nbsp; Colunas: {}\".format(shape[\"rows\"], shape[\"cols\"]), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]), styles[\"Normal\"]))\n",
    "    story.append(Spacer(1, 10))\n",
    "\n",
    "    #ausências\n",
    "    miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "    if miss_img.exists():\n",
    "        story.append(Paragraph(\"Ausência de valores por coluna\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(miss_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #correlação\n",
    "    corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "    if corr_img.exists():\n",
    "        story.append(Paragraph(\"Matriz de correlação (Pearson)\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(corr_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #por coluna: tabela completa com as MESMAS infos do HTML/TXT\n",
    "    for c in df_raw.columns:\n",
    "        prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "        kind = col_types.get(c)\n",
    "        story.append(Paragraph(\"Coluna: {}\".format(c), styles[\"Heading3\"]))\n",
    "\n",
    "        rows = []\n",
    "        rows.append([\"Tipo\", str(kind)])\n",
    "        rows.append([\"Nulos\", str(prof.get(\"nulls\",0))])\n",
    "        rows.append([\"Distintos (não vazios)\", str(prof.get(\"distinct_nonnull\",0))])\n",
    "        if prof.get(\"semantic\"):\n",
    "            rows.append([\"Tipo semântico\", str(prof[\"semantic\"])])\n",
    "\n",
    "        if prof.get(\"all_distinct\"):\n",
    "            rows.append([\"Frequências\", \"todos os dados são distintos\"])\n",
    "        else:\n",
    "            mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "            freq_parts = []\n",
    "            if mf:\n",
    "                freq_parts.append(\"mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "            if lf:\n",
    "                freq_parts.append(\"menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "            if freq_parts:\n",
    "                rows.append([\"Frequências\", \" | \".join(freq_parts)])\n",
    "\n",
    "        if prof.get(\"length_stats\"):\n",
    "            ls = prof[\"length_stats\"]\n",
    "            rows.append([\"Comprimentos\", \"min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(\n",
    "                ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]\n",
    "            )])\n",
    "\n",
    "        #estatísticas por tipo\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "            if ed:\n",
    "                rows.append([\"Estatísticas\", \"min={}, q1={}, mediana={}, q3={}, max={}\\nmédia={}, desvio padrão={}, iqr={}\\noutliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(\n",
    "                    ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                    ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                    ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                )])\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                rows.append([\"Histograma\", \"ver imagem abaixo\"])\n",
    "            if box_p.exists():\n",
    "                rows.append([\"Boxplot\", \"ver imagem abaixo\"])\n",
    "\n",
    "        elif kind == \"datetime\":\n",
    "            dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "            if dtc:\n",
    "                rows.append([\"Data/hora\", \"formato detectado: {}\\nintervalo: {} → {}\\ndias únicos: {}, média por dia: {:.2f}\".format(\n",
    "                    dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "                )])\n",
    "\n",
    "        else:\n",
    "            cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "            if cat:\n",
    "                rows.append([\"Entropia\", \"{:.4f}\".format(cat[\"entropy_shannon\"])])\n",
    "                if not cat[\"all_distinct\"]:\n",
    "                    #tabela interna de top10\n",
    "                    top_rows = [[\"Valor\",\"Contagem\"]] + [[str(t.get(\"value\"))[:120], str(t.get(\"count\"))] for t in cat[\"top10\"]]\n",
    "                    ttop = Table(top_rows, colWidths=[360, 120])\n",
    "                    ttop.setStyle(TableStyle([\n",
    "                        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "                        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f7f7f7\")),\n",
    "                        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "                    ]))\n",
    "                    #primeiro empurramos um placeholder e depois inserimos a tabela como bloco\n",
    "                    rows.append([\"Top 10\", \"tabela abaixo\"])\n",
    "                    story.append(table_kv(rows))\n",
    "                    story.append(Spacer(1, 4))\n",
    "                    story.append(ttop)\n",
    "                    rows = []  #limpa para não duplicar em table_kv abaixo\n",
    "\n",
    "        #benford\n",
    "        ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "        if ben:\n",
    "            rows.append([\"Benford\", \"qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\"))])\n",
    "\n",
    "        if rows:\n",
    "            story.append(table_kv(rows))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "        #imagens específicas da coluna\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                story.append(RLImage(str(hist_p), width=480, height=320))\n",
    "                story.append(Spacer(1, 4))\n",
    "            if box_p.exists():\n",
    "                story.append(RLImage(str(box_p), width=320, height=320))\n",
    "                story.append(Spacer(1, 6))\n",
    "\n",
    "        story.append(Spacer(1, 6))\n",
    "\n",
    "    #regras sugeridas (FDs/CFDs/DCs) como tabelas/listas\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph(\"Regras sugeridas\", styles[\"Heading2\"]))\n",
    "\n",
    "    if ANALYSIS[\"fds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado/Chave\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"fds\"]:\n",
    "            if r.get(\"key\"):\n",
    "                rows.append([\", \".join(r[\"determinant\"]), \"chave candidata\", \"100%\"])\n",
    "            else:\n",
    "                rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"100%\"])\n",
    "        tfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"FDs\", styles[\"Heading3\"]))\n",
    "        story.append(tfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"cfds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"cfds\"][:500]:\n",
    "            rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"{:.2f}%\".format(r[\"coverage\"]*100)])\n",
    "        tcfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tcfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"CFDs (aproximadas)\", styles[\"Heading3\"]))\n",
    "        story.append(tcfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        rows = [[\"Regra (DC)\", \"Violações\"]]\n",
    "        for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "            rows.append([d[\"constraint\"], str(d[\"violations\"])])\n",
    "        tdcs = Table(rows, colWidths=[360, 120])\n",
    "        tdcs.setStyle(table_style)\n",
    "        story.append(Paragraph(\"Denial constraints\", styles[\"Heading3\"]))\n",
    "        story.append(tdcs)\n",
    "\n",
    "    doc = SimpleDocTemplate(str(pdf_path), pagesize=A4, leftMargin=24, rightMargin=24, topMargin=24, bottomMargin=24)\n",
    "    doc.build(story)\n",
    "else:\n",
    "    print(\"reportlab não disponível; pulei a geração do PDF. instale reportlab e reexecute a etapa [6].\")\n",
    "\n",
    "print(\"relatórios gerados em: {}\".format(RUN_DIR))\n",
    "print(\"- TXT: relatorio.txt\")\n",
    "print(\"- HTML: relatorio.html (imagens embutidas)\")\n",
    "print(\"- PNGs: subpasta imagens/\")\n",
    "print(\"- PDF: {}\".format(\"relatorio.pdf\" if REPORTLAB_OK else \"(não gerado — instale reportlab)\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOpRKuQKik6AQxM1QQkL+2L",
   "collapsed_sections": [
    "WwAjdS960Q5p",
    "_rVfwF-i8PE0"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
