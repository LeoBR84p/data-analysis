{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2vVr44uPpfD"
   },
   "source": [
    "---\n",
    "\n",
    "<a id=\"etapas-78\"></a>Etapas 7/8 ‚Äî Tipagem, detec√ß√£o e perfil\n",
    "\n",
    "1. Uso de retorno de fun√ß√£o inconsistente (detect_numeric)\n",
    "üîé Buscar por: as_num = detect_numeric(s)\n",
    "Linhas aprox.: 1‚Äì6 (da c√©lula de infer√™ncia antiga)\n",
    "Link direto: (colar link da c√©lula aqui)\n",
    "\n",
    "\n",
    "2. Linha truncada em semantic_type\n",
    "üîé Buscar por: url_ratio = (vals.str.match(URL_RX)).\n",
    "Linhas aprox.: 38‚Äì44\n",
    "Link direto: (colar link da c√©lula aqui)\n",
    "\n",
    "\n",
    "3. Fun√ß√µes redefinidas em c√©lulas diferentes (detect_datetime, semantic_type)\n",
    "üîé Buscar por: def detect_datetime(\n",
    "Linhas aprox. (vers√£o √≠ntegra): 19‚Äì33\n",
    "üîé Buscar por: def semantic_type(\n",
    "Linhas aprox. (variante): 12‚Äì20 (+ sequ√™ncia)\n",
    "Links diretos: (colar link da(s) c√©lula(s) aqui)\n",
    "\n",
    "\n",
    "4. Hotspots de mem√≥ria/performance\n",
    "üîé Buscar por: pd.read_csv( e dtype=str\n",
    "Linhas aprox. (leitura): 47‚Äì49\n",
    "üîé Buscar por: .value_counts()\n",
    "Linhas aprox. (profiling): 34‚Äì38\n",
    "Links diretos: (colar link da(s) c√©lula(s) aqui)\n",
    "\n",
    "\n",
    "5. Warning do to_datetime\n",
    "üîé Buscar por: SettingWithCopyWarning ou to_datetime\n",
    "Linhas aprox. (warning): 1‚Äì4\n",
    "Link direto: (colar link da c√©lula aqui)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"etapa-9\"></a>Etapa 9 ‚Äî Gera√ß√£o de relat√≥rios (TXT/HTML/PNGs/PDF)\n",
    "\n",
    "1. Blocos de gera√ß√£o de relat√≥rios duplicados\n",
    "üîé Buscar por: gera√ß√£o de relat√≥rios: TXT, HTML, PNGs e PDF\n",
    "Linhas aprox.: 2‚Äì7 e 41‚Äì50\n",
    "Links diretos: (colar links das c√©lulas aqui)\n",
    "\n",
    "\n",
    "2. Resumo final duplicado\n",
    "üîé Buscar por: Relat√≥rios gerados em\n",
    "Linhas aprox.: 22‚Äì26 e 3‚Äì7\n",
    "Links diretos: (colar links das c√©lulas aqui)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwAjdS960Q5p"
   },
   "source": [
    "#**Licen√ßa de Uso**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4U4kA0A0UMm"
   },
   "source": [
    "This repository uses a **dual-license model** to distinguish between source code and creative/documental content.\n",
    "\n",
    "**Code** (Python scripts, modules, utilities):\n",
    "Licensed under the MIT License.\n",
    "\n",
    "‚Üí You may freely use, modify, and redistribute the code, including for commercial purposes, provided that you preserve the copyright notice.\n",
    "\n",
    "**Content** (Jupyter notebooks, documentation, reports, datasets, and generated outputs):\n",
    "Licensed under the Creative Commons Attribution‚ÄìNonCommercial 4.0 International License.\n",
    "\n",
    "‚Üí You may share and adapt the content for non-commercial purposes, provided that proper credit is given to the original author.\n",
    "\n",
    "\n",
    "**¬© 2025 Leandro Bernardo Rodrigues**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9K7JEpmVLXlE"
   },
   "source": [
    "#**Utilit√°rio:** verifica√ß√£o da formata√ß√£o de c√≥digo\n",
    "\n",
    "Black [88] + Isort, desconsiderando c√©lulas m√°gicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "2QkokQSCT2IC"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0001\n",
    "#pr√©-visualizar/aplicar (pula magics) ‚Äî isort(profile=black)+black(88) { display-mode: \"form\" }\n",
    "import sys, subprocess, os, re, difflib, textwrap, time\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ===== CONFIG =====\n",
    "NOTEBOOK = \"/content/drive/MyDrive/Notebooks/data-analysis/notebooks/main_DataTools.ipynb\"  # <- ajuste\n",
    "LINE_LENGTH = 88\n",
    "# ==================\n",
    "\n",
    "# 1) Instalar libs no MESMO Python do kernel\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"black\", \"isort\", \"nbformat\"])\n",
    "\n",
    "import nbformat\n",
    "import black\n",
    "import isort\n",
    "\n",
    "BLACK_MODE = black.Mode(line_length=LINE_LENGTH)\n",
    "ISORT_CFG  = isort.Config(profile=\"black\", line_length=LINE_LENGTH)\n",
    "\n",
    "# 2) Regras para pular c√©lulas com magics/shell\n",
    "#   - linhas come√ßando com %, %%, !\n",
    "#   - chamadas a get_ipython(\n",
    "MAGIC_LINE = re.compile(r\"^\\s*(%{1,2}|!)\", re.M)\n",
    "GET_IPY    = re.compile(r\"get_ipython\\s*\\(\")\n",
    "\n",
    "def has_magics(code: str) -> bool:\n",
    "    return bool(MAGIC_LINE.search(code) or GET_IPY.search(code))\n",
    "\n",
    "def format_code(code: str) -> str:\n",
    "    # isort primeiro, depois black\n",
    "    sorted_code = isort.api.sort_code_string(code, config=ISORT_CFG)\n",
    "    return black.format_str(sorted_code, mode=BLACK_MODE)\n",
    "\n",
    "def summarize_diff(diff_lines: List[str]) -> Tuple[int, int]:\n",
    "    added = removed = 0\n",
    "    for ln in diff_lines:\n",
    "        # ignorar cabe√ßalhos do diff\n",
    "        if ln.startswith((\"---\", \"+++\", \"@@\")):\n",
    "            continue\n",
    "        if ln.startswith(\"+\"):\n",
    "            added += 1\n",
    "        elif ln.startswith(\"-\"):\n",
    "            removed += 1\n",
    "    return added, removed\n",
    "\n",
    "def header(title: str):\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(title)\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "if not os.path.exists(NOTEBOOK):\n",
    "    raise FileNotFoundError(f\"Notebook n√£o encontrado:\\n{NOTEBOOK}\")\n",
    "\n",
    "# 3) Leitura do .ipynb\n",
    "with open(NOTEBOOK, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "changed_cells = []  # (idx, added, removed, diff_text, preview_snippet, new_code)\n",
    "\n",
    "# 4) Pr√©-visualiza√ß√£o c√©lula a c√©lula\n",
    "header(\"Pr√©-visualiza√ß√£o (N√ÉO grava) ‚Äî somente c√©lulas com mudan√ßas\")\n",
    "for i, cell in enumerate(nb.cells):\n",
    "    if cell.get(\"cell_type\") != \"code\":\n",
    "        continue\n",
    "\n",
    "    original = cell.get(\"source\", \"\")\n",
    "    if not original.strip():\n",
    "        continue\n",
    "\n",
    "    # Pular c√©lulas com magics/shell\n",
    "    if has_magics(original):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        formatted = format_code(original)\n",
    "    except Exception as e:\n",
    "        print(f\"[Aviso] c√©lula {i}: erro no formatador ‚Äî pulando ({e})\")\n",
    "        continue\n",
    "\n",
    "    if original.strip() != formatted.strip():\n",
    "        # Gerar diff unificado leg√≠vel\n",
    "        diff = list(difflib.unified_diff(\n",
    "            original.splitlines(), formatted.splitlines(),\n",
    "            fromfile=f\"cell_{i}:before\", tofile=f\"cell_{i}:after\", lineterm=\"\"\n",
    "        ))\n",
    "        add, rem = summarize_diff(diff)\n",
    "        snippet = original.strip().splitlines()[0][:120] if original.strip().splitlines() else \"<c√©lula vazia>\"\n",
    "        changed_cells.append((i, add, rem, \"\\n\".join(diff), snippet, formatted))\n",
    "\n",
    "# 5) Exibi√ß√£o dos diffs por c√©lula (se houver)\n",
    "if not changed_cells:\n",
    "    print(\"‚úî Nada a alterar: todas as c√©lulas (n√£o m√°gicas) j√° est√£o conforme isort/black.\")\n",
    "else:\n",
    "    total_add = total_rem = 0\n",
    "    for (idx, add, rem, diff_text, snippet, _new) in changed_cells:\n",
    "        total_add += add\n",
    "        total_rem += rem\n",
    "        header(f\"Diff ‚Äî C√©lula #{idx}  (+{add}/-{rem})\")\n",
    "        print(f\"Primeira linha da c√©lula: {snippet!r}\\n\")\n",
    "        print(diff_text)\n",
    "\n",
    "    header(\"Resumo\")\n",
    "    print(f\"C√©lulas com mudan√ßas: {len(changed_cells)}\")\n",
    "    print(f\"Linhas adicionadas:   {total_add}\")\n",
    "    print(f\"Linhas removidas:     {total_rem}\")\n",
    "\n",
    "# 6) Perguntar se aplica\n",
    "if changed_cells:\n",
    "    print(\"\\nDigite 'p' para **Proceder** e gravar as mudan√ßas nessas c√©lulas, ou 'c' para **Cancelar**.\")\n",
    "    try:\n",
    "        choice = input(\"Proceder (p) / Cancelar (c): \").strip().lower()\n",
    "    except Exception:\n",
    "        choice = \"c\"\n",
    "\n",
    "    if choice == \"p\":\n",
    "        # Backup antes de escrever\n",
    "        backup = NOTEBOOK + \".bak\"\n",
    "        if not os.path.exists(backup):\n",
    "            with open(backup, \"w\", encoding=\"utf-8\") as bf:\n",
    "                nbformat.write(nb, bf)\n",
    "\n",
    "        # Aplicar somente nas c√©lulas com mudan√ßas\n",
    "        idx_to_new = {idx: new for (idx, _a, _r, _d, _s, new) in changed_cells}\n",
    "        for i, cell in enumerate(nb.cells):\n",
    "            if i in idx_to_new and cell.get(\"cell_type\") == \"code\":\n",
    "                cell[\"source\"] = idx_to_new[i]\n",
    "\n",
    "        # Escrever no .ipynb\n",
    "        with open(NOTEBOOK, \"w\", encoding=\"utf-8\") as f:\n",
    "            nbformat.write(nb, f)\n",
    "\n",
    "        # Sync delay (Drive)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "        header(\"Conclu√≠do\")\n",
    "        print(f\"‚úî Mudan√ßas aplicadas em {len(changed_cells)} c√©lula(s).\")\n",
    "        print(f\"Backup criado em: {backup}\")\n",
    "        print(\"Dica: recarregue o notebook no Colab para ver a formata√ß√£o atualizada.\")\n",
    "    else:\n",
    "        print(\"\\nOpera√ß√£o cancelada. Nada foi gravado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Kt7IFa13wL"
   },
   "source": [
    "#**Sincronizar altera√ß√µes no c√≥digo do projeto**\n",
    "Comandos para sincronizar c√≥digo (Google Drive, Git, GitHub) e realizar versionamento\n",
    "\n",
    "---\n",
    "Google Drive √© considerado o ponto de verdade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93LRJc0Wqk3X"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0002\n",
    "#push do Drive -> GitHub (Drive √© a fonte da verdade)\n",
    "#respeita .gitignore do Drive\n",
    "#sempre em 'main', sem pull, commit + push imediato\n",
    "#mensagem de commit padronizada com timestamp SP\n",
    "#bump de vers√£o (M/m/n) + tag anotada\n",
    "#force push (branch e tags), silencioso; s√≥ 1 print final\n",
    "#PAT lido de segredo do Colab: GITHUB_PAT_DA (fallback: env; √∫ltimo caso: prompt)\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess, os, re, shutil, sys, getpass\n",
    "from urllib.parse import quote as urlquote\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "#configura√ß√µes do projeto\n",
    "author_name    = \"Leandro Bernardo Rodrigues\"\n",
    "owner          = \"LeoBR84p\"         # dono do reposit√≥rio no GitHub\n",
    "repo_name      = \"data-analysis\"    # nome do reposit√≥rio\n",
    "default_branch = \"main\"\n",
    "repo_dir       = Path(\"/content/drive/MyDrive/Notebooks/data-analysis\")\n",
    "remote_base    = f\"https://github.com/{owner}/{repo_name}.git\"\n",
    "author_email   = f\"bernardo.leandro@gmail.com\"  # evita erro de identidade\n",
    "\n",
    "#nbstripout: \"install\" para limpar outputs; \"disable\" para versionar outputs\n",
    "nbstripout_mode = \"install\"\n",
    "import shutil\n",
    "exe = shutil.which(\"nbstripout\")\n",
    "git(\"config\", \"--local\", \"filter.nbstripout.clean\", exe if exe else \"nbstripout\", cwd=repo_dir)\n",
    "\n",
    "#utilit√°rios silenciosos\n",
    "def sh(cmd, cwd=None, check=True):\n",
    "    \"\"\"\n",
    "    Executa comando silencioso. Em erro, levanta RuntimeError com rc e UM rascunho de causa,\n",
    "    mascarando URLs com credenciais (ex.: https://***:***@github.com/...).\n",
    "    \"\"\"\n",
    "    safe_cmd = []\n",
    "    for x in cmd:\n",
    "        if isinstance(x, str) and \"github.com\" in x and \"@\" in x:\n",
    "            #mascara credenciais: https://user:token@ -> https://***:***@\n",
    "            x = re.sub(r\"https://[^:/]+:[^@]+@\", \"https://***:***@\", x)\n",
    "        safe_cmd.append(x)\n",
    "\n",
    "    r = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if check and r.returncode != 0:\n",
    "        #heur√≠stica curtinha p/ tornar rc=128 mais informativo sem vazar nada\n",
    "        stderr = (r.stderr or \"\").strip().lower()\n",
    "        if \"authentication failed\" in stderr or \"permission\" in stderr or \"not found\" in stderr:\n",
    "            hint = \"auth/permiss√µes/URL\"\n",
    "        elif \"not a git repository\" in stderr:\n",
    "            hint = \"repo local inv√°lido\"\n",
    "        else:\n",
    "            hint = \"git falhou\"\n",
    "        cmd_hint = \" \".join(safe_cmd[:3])\n",
    "        raise RuntimeError(f\"rc={r.returncode}; {hint}; cmd={cmd_hint}\")\n",
    "    return r.stdout\n",
    "\n",
    "def git(*args, cwd=None, check=True):\n",
    "    return sh([\"git\", *args], cwd=cwd, check=check)\n",
    "\n",
    "#ambiente: Colab + Drive\n",
    "def ensure_drive():\n",
    "    try:\n",
    "        from google.colab import drive  # type: ignore\n",
    "        base = Path(\"/content/drive/MyDrive\")\n",
    "        if not base.exists():\n",
    "            drive.mount(\"/content/drive\")\n",
    "        if not base.exists():\n",
    "            raise RuntimeError(\"Google Drive n√£o montado.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao montar o Drive: {e}\")\n",
    "\n",
    "#repo local no Drive\n",
    "def is_empty_dir(p: Path) -> bool:\n",
    "    try:\n",
    "        return p.exists() and not any(p.iterdir())\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "def init_or_recover_repo():\n",
    "    repo_dir.mkdir(parents=True, exist_ok=True)\n",
    "    git_dir = repo_dir / \".git\"\n",
    "\n",
    "    def _fresh_init():\n",
    "        if git_dir.exists():\n",
    "            shutil.rmtree(git_dir, ignore_errors=True)\n",
    "        git(\"init\", cwd=repo_dir)\n",
    "\n",
    "    #caso .git no Colab ausente ou vazia -> init limpo\n",
    "    if not git_dir.exists() or is_empty_dir(git_dir):\n",
    "        _fresh_init()\n",
    "    else:\n",
    "        #valida se √© um work-tree git funcional no Colab; se falhar -> init limpo\n",
    "        try:\n",
    "            git(\"rev-parse\", \"--is-inside-work-tree\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            _fresh_init()\n",
    "\n",
    "    #aborta opera√ß√µes pendentes (n√£o apaga hist√≥rico)\n",
    "    for args in ((\"rebase\", \"--abort\"), (\"merge\", \"--abort\"), (\"cherry-pick\", \"--abort\")):\n",
    "        try:\n",
    "            git(*args, cwd=repo_dir, check=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    #for√ßa branch main\n",
    "    try:\n",
    "        sh([\"git\", \"switch\", \"-C\", default_branch], cwd=repo_dir)\n",
    "    except Exception:\n",
    "        sh([\"git\", \"checkout\", \"-B\", default_branch], cwd=repo_dir)\n",
    "\n",
    "    #configura identidade local\n",
    "    try:\n",
    "        git(\"config\", \"user.name\", author_name, cwd=repo_dir)\n",
    "        git(\"config\", \"user.email\", author_email, cwd=repo_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #marca o diret√≥rio como safe\n",
    "    try:\n",
    "        sh([\"git\",\"config\",\"--global\",\"--add\",\"safe.directory\", str(repo_dir)])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #sanity check final (falha cedo se algo ainda estiver errado)\n",
    "    git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "\n",
    "\n",
    "#nbstripout (opcional)\n",
    "def setup_nbstripout():\n",
    "    if nbstripout_mode == \"disable\":\n",
    "        #remove configs do filtro\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.clean\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.smudge\"], cwd=repo_dir, check=False)\n",
    "        sh([\"git\",\"config\",\"--local\",\"--unset-all\",\"filter.nbstripout.required\"], cwd=repo_dir, check=False)\n",
    "        gat = repo_dir / \".gitattributes\"\n",
    "        if gat.exists():\n",
    "            lines = gat.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            new_lines = [ln for ln in lines if \"filter=nbstripout\" not in ln]\n",
    "            gat.write_text(\"\\n\".join(new_lines) + (\"\\n\" if new_lines else \"\"), encoding=\"utf-8\")\n",
    "        return\n",
    "\n",
    "    #instala nbstripout (se necess√°rio)\n",
    "    try:\n",
    "        import nbstripout  #noqa: F401\n",
    "    except Exception:\n",
    "        sh([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"nbstripout\"])\n",
    "\n",
    "    py = sys.executable\n",
    "    #configurar filtro sem aspas extras\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.clean\", \"nbstripout\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.smudge\", \"cat\", cwd=repo_dir)\n",
    "    git(\"config\", \"--local\", \"filter.nbstripout.required\", \"true\", cwd=repo_dir)\n",
    "    gat = repo_dir / \".gitattributes\"\n",
    "    line = \"*.ipynb filter=nbstripout\"\n",
    "    if gat.exists():\n",
    "        txt = gat.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if line not in txt:\n",
    "            gat.write_text((txt.rstrip() + \"\\n\" + line + \"\\n\"), encoding=\"utf-8\")\n",
    "    else:\n",
    "        gat.write_text(line + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "#.gitignore normaliza√ß√£o\n",
    "def normalize_tracked_ignored():\n",
    "    \"\"\"\n",
    "    Se houver arquivos j√° rastreados que hoje s√£o ignorados pelo .gitignore,\n",
    "    limpa o √≠ndice e re-adiciona respeitando o .gitignore.\n",
    "    Retorna True se normalizou algo; False caso contr√°rio.\n",
    "    \"\"\"\n",
    "    #remove lock de √≠ndice, se houver\n",
    "    lock = repo_dir / \".git/index.lock\"\n",
    "    try:\n",
    "        if lock.exists():\n",
    "            lock.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    #garante que o √≠ndice existe (ou se recupera)\n",
    "    idx = repo_dir / \".git/index\"\n",
    "    if not idx.exists():\n",
    "        try:\n",
    "            sh([\"git\", \"reset\", \"--mixed\"], cwd=repo_dir)\n",
    "        except Exception:\n",
    "            try:\n",
    "                sh([\"git\", \"init\"], cwd=repo_dir)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    #detecta arquivos ignorados que est√£o rastreados e normaliza\n",
    "    normalized = False\n",
    "    try:\n",
    "        out = git(\"ls-files\", \"-z\", \"--ignored\", \"--exclude-standard\", \"--cached\", cwd=repo_dir)\n",
    "        tracked_ignored = [p for p in out.split(\"\\x00\") if p]\n",
    "        if tracked_ignored:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \".\", cwd=repo_dir)\n",
    "            git(\"add\", \"-A\", cwd=repo_dir)\n",
    "            normalized = True\n",
    "    except Exception:\n",
    "        #falhou a detec√ß√£o? segue o fluxo sem travar\n",
    "        pass\n",
    "\n",
    "    return normalized\n",
    "\n",
    "#semVer e bump de vers√£o\n",
    "_semver = re.compile(r\"^(\\d+)\\.(\\d+)\\.(\\d+)$\")\n",
    "\n",
    "def parse_semver(s):\n",
    "    m = _semver.match((s or \"\").strip())\n",
    "    return tuple(map(int, m.groups())) if m else None\n",
    "\n",
    "def current_version():\n",
    "    try:\n",
    "        tags = [t for t in git(\"tag\", \"--list\", cwd=repo_dir).splitlines() if parse_semver(t)]\n",
    "        if tags:\n",
    "            return sorted(tags, key=lambda x: parse_semver(x))[-1]\n",
    "    except Exception:\n",
    "        pass\n",
    "    vf = repo_dir / \"VERSION\"\n",
    "    if vf.exists():\n",
    "        v = vf.read_text(encoding=\"utf-8\").strip()\n",
    "        if parse_semver(v):\n",
    "            return v\n",
    "    return \"1.0.0\"\n",
    "\n",
    "def bump(v, kind):\n",
    "    M, m, p = parse_semver(v) or (1, 0, 0)\n",
    "    k = (kind or \"\").strip()\n",
    "    if k == \"m\":\n",
    "        return f\"{M}.{m+1}.0\"\n",
    "    if k == \"n\":\n",
    "        return f\"{M}.{m}.{p+1}\"\n",
    "    return f\"{M+1}.0.0\"  #default major\n",
    "\n",
    "#timestamp SP\n",
    "def now_sp():\n",
    "    #tenta usar zoneinfo; fallback fixo -03:00 (Brasil sem DST atualmente)\n",
    "    try:\n",
    "        from zoneinfo import ZoneInfo  # Py3.9+\n",
    "        tz = ZoneInfo(\"America/Sao_Paulo\")\n",
    "        dt = datetime.now(tz)\n",
    "    except Exception:\n",
    "        dt = datetime.now(timezone(timedelta(hours=-3)))\n",
    "    #formato leg√≠vel + offset\n",
    "    return dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")  # ex.: 2025-10-08 02:34:00-0300\n",
    "\n",
    "#autentica√ß√£o (PAT)\n",
    "def get_pat():\n",
    "    #Colab Secrets\n",
    "    token = None\n",
    "    try:\n",
    "        from google.colab import userdata  #type: ignore\n",
    "        token = userdata.get('GITHUB_PAT_DA')  #nome do segredo criado no Colab\n",
    "    except Exception:\n",
    "        token = None\n",
    "    #fallback1 - vari√°vel de ambiente\n",
    "    if not token:\n",
    "        token = os.environ.get(\"GITHUB_PAT_DA\") or os.environ.get(\"GITHUB_PAT\")\n",
    "    #fallback2 - interativo\n",
    "    if not token:\n",
    "        token = getpass.getpass(\"Informe seu GitHub PAT: \").strip()\n",
    "    if not token:\n",
    "        raise RuntimeError(\"PAT ausente.\")\n",
    "    return token\n",
    "\n",
    "#listas de for√ßa\n",
    "FORCE_UNTRACK = [\"input/\", \"output/\", \"data/\", \"runs/\", \"logs/\", \"figures/\"]\n",
    "FORCE_TRACK   = [\"references/\"]  #versionar tudo dentro (PDFs inclusive)\n",
    "\n",
    "def force_index_rules():\n",
    "    #garante que pastas sens√≠veis NUNCA fiquem rastreadas\n",
    "    for p in FORCE_UNTRACK:\n",
    "        try:\n",
    "            git(\"rm\", \"-r\", \"--cached\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "    #garanta que references/ SEMPRE entre (√∫til se ainda h√° *.pdf globais)\n",
    "    for p in FORCE_TRACK:\n",
    "        try:\n",
    "            git(\"add\", \"-f\", \"--\", p, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "#fluxo principal\n",
    "def main():\n",
    "    try:\n",
    "        ensure_drive()\n",
    "        init_or_recover_repo()\n",
    "        setup_nbstripout()\n",
    "\n",
    "        #pergunta apenas o tipo de vers√£o (M/m/n)\n",
    "        kind = input(\"Informe o tipo de mudan√ßa: Maior (M), menor (m) ou pontual (n): \").strip()\n",
    "        if kind not in (\"M\", \"m\", \"n\"):\n",
    "            kind = \"n\"\n",
    "\n",
    "        #vers√£o\n",
    "        cur = current_version()\n",
    "        new = bump(cur, kind)\n",
    "        (repo_dir / \"VERSION\").write_text(new + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "        #normaliza itens ignorados que estejam rastreados (uma √∫nica vez, se necess√°rio)\n",
    "        normalize_tracked_ignored()\n",
    "\n",
    "        #aplica regras de for√ßa\n",
    "        force_index_rules()\n",
    "\n",
    "        #stage de tudo (Drive √© a verdade; remo√ß√µes entram aqui)\n",
    "        git(\"add\", \"-A\", cwd=repo_dir)\n",
    "\n",
    "        #mensagem padronizada de commit\n",
    "        ts = now_sp()\n",
    "        commit_msg = f\"upload pelo {author_name} em {ts}\"\n",
    "        try:\n",
    "            git(\"commit\", \"-m\", commit_msg, cwd=repo_dir)\n",
    "        except Exception:\n",
    "            #se nada a commitar, seguimos (pode ocorrer se s√≥ a tag mudar, mas aqui VERSION muda)\n",
    "            status = git(\"status\", \"--porcelain\", cwd=repo_dir)\n",
    "            if status.strip():\n",
    "                raise\n",
    "\n",
    "        #Tag anotada (substitui se j√° existir)\n",
    "        try:\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} ‚Äî {commit_msg}\", cwd=repo_dir)\n",
    "        except Exception:\n",
    "            sh([\"git\", \"tag\", \"-d\", new], cwd=repo_dir, check=False)\n",
    "            git(\"tag\", \"-a\", new, \"-m\", f\"release {new} ‚Äî {commit_msg}\", cwd=repo_dir)\n",
    "\n",
    "        #push com PAT (Drive √© a verdade): valida√ß√£o + push for√ßado\n",
    "        token = get_pat()\n",
    "        user_for_url = owner  # voc√™ √© o owner; n√£o perguntamos\n",
    "        auth_url = f\"https://{urlquote(user_for_url, safe='')}:{urlquote(token, safe='')}@github.com/{owner}/{repo_name}.git\"\n",
    "\n",
    "        #valida credenciais/URL de forma silenciosa (sem vazar token)\n",
    "        #tenta checar a branch main; se n√£o existir (repo vazio), faz um probe gen√©rico\n",
    "        try:\n",
    "            sh([\"git\", \"ls-remote\", auth_url, f\"refs/heads/{default_branch}\"], cwd=repo_dir)\n",
    "        except RuntimeError:\n",
    "            #reposit√≥rio pode estar vazio (sem refs); probe sem ref deve funcionar\n",
    "            sh([\"git\", \"ls-remote\", auth_url], cwd=repo_dir)\n",
    "\n",
    "        #push for√ßado de branch e tags\n",
    "        sh([\"git\", \"push\", \"-u\", \"--force\", auth_url, default_branch], cwd=repo_dir)\n",
    "        sh([\"git\", \"push\", \"--force\", auth_url, \"--tags\"], cwd=repo_dir)\n",
    "\n",
    "        print(f\"[ok]   Registro no GitHub com sucesso. Vers√£o atual {new}\")\n",
    "    except Exception as e:\n",
    "        #mensagem √∫nica, curta, sem detalhes sens√≠veis\n",
    "        msg = str(e) or \"falha inesperada\"\n",
    "        print(f\"[erro] {msg}\")\n",
    "\n",
    "#executa\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSIy8Nz62HK9"
   },
   "source": [
    "#**Ferramentas de Data Analysis**\n",
    "\n",
    "Ferramentas de **identifica√ß√£o de tipo de dado e estrutura da informa√ß√£o** presente em *datasets* a partir da ingest√£o de arquivos CSV UTF-8 com BOM em padr√£o separado por ponto e v√≠rgula.\n",
    "_____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k8hFVquXv0X"
   },
   "source": [
    "##**T√©cnicas adotadas**: Threshold de infer√™ncia ‚â• 90%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jl_IdQzPnWjm"
   },
   "source": [
    "###**Infer√™ncia de tipos de dados e tipos sem√¢nticos**\n",
    "\n",
    "- detecta campos num√©ricos, com convers√£o PT-BR (ponto de milhar, v√≠rgula decimal);\n",
    "- detecta campos de data e testa v√°rios formatos de data comuns (BR/ISO, com e sem hora);\n",
    "- detecta vari√°veis boleanas por dicion√°rio (\"sim\"/\"n√£o\"/\"true\"/\"false\"/\"1\"/\"0\" etc.); e\n",
    "- demais casos s√£o tipificados como objeto e analisados versus tipos sem√¢nticos (regex), que marcam email, URL, CPF e CNPJ.\n",
    "\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "Inconsist√™ncias na verifica√ß√£o indicam colunas mistas (ex.: texto + n√∫mero) que podem precisar de saneamento.\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas:** panorama e m√©todos de data profiling (tipagem, padr√µes, FDs).\n",
    "- ABEDJAN, Ziawasch; GOLAB, Lukasz; NAUMANN, Felix. Data profiling - a tutorial. Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'17), Chicago, IL, USA, p. 1747-1751, May 14-19 2017. New York: Association for Computing Machinery (ACM), 2017. DOI:10.1145/3035918.3054772.\n",
    "\n",
    "- 2017_SIGMOD_Abedjan_Data-Profiling-Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5S84sTrnkrh"
   },
   "source": [
    "###**Perfilamento b√°sico por coluna**\n",
    "\n",
    "- contagem de nulos;\n",
    "- n√∫mero de distintos;\n",
    "- moda/m√≠nima frequ√™ncia (com propor√ß√µes);\n",
    "- estat√≠sticas de comprimento (min/max/m√©dia/Q1/mediana/Q3); e\n",
    "- amostras de padr√£o (datas, n√∫meros PT-BR, ‚Äútexto-livre‚Äù etc.).\n",
    "\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "- alta cardinalidade e campos todos distintos (all_distinct=True) sugerem fortes candidatos a campos chave (ou a presen√ßa de IDs aleat√≥rios); e\n",
    "- comprimentos e exemplos de express√µes regulares (regex) ajudam a detectar campos truncados, espa√ßos extras e formata√ß√£o inconsistente.\n",
    "\n",
    "1. v√°rios valores com mesmo tamanho m√°ximo podem indicar corte de informa√ß√µes/truncagem; e\n",
    "2. tamanho m√≠nimo = tamanho m√°ximo pode indicar m√°scaras de preenchimento.\n",
    "\n",
    "\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**: guias de perfilamento e qualidade de dados.\n",
    "\n",
    "- ABEDJAN, Ziawasch; GOLAB, Lukasz; NAUMANN, Felix. Data profiling - a tutorial. Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'17), Chicago, IL, USA, p. 1747-1751, May 14-19 2017. New York: Association for Computing Machinery (ACM), 2017. DOI:10.1145/3035918.3054772.\n",
    "\n",
    "- 2017_SIGMOD_Abedjan_Data-Profiling-Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1N-WchTnsZb"
   },
   "source": [
    "###**Estat√≠stica descritiva e outliers (para valores num√©ricos)**\n",
    "\n",
    "Observa campos num√©ricos e verifica se os n√∫meros est√£o concentrados, espalhados, outliers e desvio na distribui√ß√£o (skew).\n",
    "\n",
    "- para dados num√©ricos, avalia: min, Q1, mediana, Q3, max, m√©dia, desvio-padr√£o, IQR, assimetria e curtose-excesso;\n",
    "- identifica outliers por IQR (intervalo interquart√≠ltico): identifica onde est√£o a maioria dos dados (m√©dia) e marca tudo que estiver fora (acima ou abaixo) como outlier; e\n",
    "- identifica outliers por MAD (desvio absoluto da mediana): identifica valores afastados da mediana (centro-real) e marca tudo que estiver fora (acima ou abaixo) como outlier. M√©todo mais resistente contra valores que distor√ßam a distribui√ß√£o.\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "Estamos aplicando m√©todos n√£o param√©tricos para an√°lise (ou seja, n√£o estamos assumindo que os valores se aproximam de um formato espec√≠fico de uma distribui√ß√£o conhecida)\n",
    "\n",
    "- m√©todo IQR √© adequado para distribui√ß√£o com caudas leves (sem extremos);\n",
    "- m√©todo MAD √© resistente a extremos ‚Äî usado para s√©ries com anomalias ou assimetria;\n",
    "- Skewness avalia o formato da distribui√ß√£o (maior que 0, para a direita / menor que 0, para esquerda). Indica se seria aplic√°vel realizar transforma√ß√£o logar√≠tmica para comprimir valores altos e diminuir o peso dos extremos, possibilitando aproxima√ß√µes com distribui√ß√µes sim√©tricas/normais; e\n",
    "- Curtose avalia se h√° mais valores extremos do que centrais (kurtosis maior que 0).\n",
    "\n",
    "üìñ **Refer√™ncia t√©cnica:**\n",
    "\n",
    "NATIONAL INSTITUTE OF STANDARDS AND TECHNOLOGY (NIST). Exploratory Data Analysis (EDA). In: NIST/SEMATECH e-Handbook of Statistical Methods. Gaithersburg: NIST, 2012. Dispon√≠vel em: https://www.itl.nist.gov/div898/handbook/eda/eda.htm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_9Gj65rn1OF"
   },
   "source": [
    "###**Tipos Categ√≥ricos/booleanos**\n",
    "\n",
    "An√°lise utilizando a entropia de Shannon (vide Claude Shannon, 1948) e o conceito de bits (1 bit = 1 informa√ß√£o suficiente para escolher entre duas op√ß√µes).\n",
    "\n",
    "Calcula entropia a partir das frequ√™ncias:\n",
    "- entropia alta (maior do que 2) ‚Üí categorias mais equilibradas (se metade dos dados √© A e metade dos dados √© B, os eventos s√£o igualmente prov√°veis e, portanto, mais dif√≠cil adivinhar um sorteio); e\n",
    "- entropia baixa (menor do que 1) ‚Üí domin√¢ncia de poucas categorias (existem eventos dominantes e, seria poss√≠vel acertar um chute).\n",
    "\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "- ajuda a detectar colunas sem varia√ß√£o (e que podem n√£o ter conte√∫do relevante);\n",
    "- mostra distribui√ß√µes desbalanceadas e que poderiam trazer pontos de aten√ß√£o para treinamento de algoritmos;\n",
    "- ajuda a medir a diversidade de um dado;\n",
    "- usada como base para c√°lculo de ganho de informa√ß√£o em algoritmos de √°rvores de decis√£o e ML (compara-se a entropia antes e depois da inclus√£o de uma nova divis√£o no conjunto de dados);\n",
    "- resultados: 0 ‚Üí todos os valores s√£o iguais | menor do que 1 ‚Üí valores homog√™neos | entre 1 e 2 ‚Üí boa diversidade, coluna informativa | maior do que 2 ‚Üí categorias com propor√ß√µes parecidas;\n",
    "- entropia alta com muitas categorias pode sinalizar ru√≠do (ex.: grafias variadas para o mesmo r√≥tulo); e\n",
    "- entropia baixa pode expor classe majorit√°ria (√∫til para balanceamento entre classes).\n",
    "\n",
    "üìñ **Refer√™ncia t√©cnica:** artigo fundador da Teoria da Informa√ß√£o\n",
    "\n",
    "- Shannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379-423."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFjDA137n9ym"
   },
   "source": [
    "###**Tipos datas/tempos**\n",
    "\n",
    "Identicadas pelo seu formato, que tem padroniza√ß√£o bem estabelecida.\n",
    "\n",
    "- formato detectado;\n",
    "- m√≠n/max;\n",
    "- n√∫mero de dias √∫nicos; e\n",
    "- m√©dia por dia.\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "Ajuda a ver buracos de coleta, picos/sazonalidade e janelas de validade.\n",
    "\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**: guias de perfilamento e qualidade de dados.\n",
    "\n",
    "- ABEDJAN, Ziawasch; GOLAB, Lukasz; NAUMANN, Felix. Data profiling - a tutorial. Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD'17), Chicago, IL, USA, p. 1747-1751, May 14-19 2017. New York: Association for Computing Machinery (ACM), 2017. DOI:10.1145/3035918.3054772.\n",
    "\n",
    "- 2017_SIGMOD_Abedjan_Data-Profiling-Tutorial.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkpMWnA0oCiV"
   },
   "source": [
    "###**An√°lise de dados faltantes e duplicados**\n",
    "\n",
    "- % de vazios por coluna;\n",
    "- linhas duplicadas; e\n",
    "- matriz de coocorr√™ncia de aus√™ncias (propor√ß√£o de vazios simult√¢neos entre duas colunas A‚àßB - mostra associa√ß√£o entre dados).\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "- Co-aus√™ncia alta entre colunas pode sugerir depend√™ncia operacional (ex.: se ‚Äúdata_fim‚Äù falta quando ‚Äúdata_inicio‚Äù falta).\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**:\n",
    "\n",
    "- PENA, Eduardo H. M.; PORTO, Fabio; NAUMANN, Felix. Discovering denial constraints in dynamic datasets. Proceedings of the IEEE International Conference on Data Engineering (ICDE 2024), Utrecht, Netherlands, p. 1-12,2024.IEEE, 2024. DOI:10.1109/ICDE60146.2024.00000.\n",
    "\n",
    "- 2024_ICDE_Pena_Discovering-Denial-Constraints-Dynamic-Datasets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex4RX_mMoJKL"
   },
   "source": [
    "###**An√°lise de Depend√™ncias:** FDs, CFDs e Denial Constraints\n",
    "\n",
    "- FD (depend√™ncia funcional) - diz a rela√ß√£o determin√≠stica entre atributos de dados. Ex.: campo de login determina quem √© o empregado;\n",
    "- CFDs (dependencia funcional condicional) - inclui condi√ß√µes espec√≠ficas √†s depend√™ncias funcionais (valores ou faixas) e √© √∫til para regras ‚Äúquase sempre verdadeiras‚Äù. Ex.: se o pa√≠s √© BR, ent√£o o CEP determina o Estado); e\n",
    "- Denial Constraints (restri√ß√µes de nega√ß√£o) - condi√ßoes que nunca devem ocorrer - nega√ß√£o universal. Ex.: faixa plaus√≠vel de idade (0‚Äì120), datas in√≠cio ‚â§ fim, e valores positivos.\n",
    "\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "- FDs/CFDs sugerem regras de integridade e reconciliam tabelas (campos chaves e determinantes). Viola√ß√µes localizam erros de qualidade; e\n",
    "- DCs servem como checklist autom√°tico de sanidade do preenchimento; muitas viola√ß√µes significam erros de sistema.\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**:\n",
    "- FAN, Wenfei; GEERTS, Floris; LAKSHMANAN, Laks V. S.; XIONG, Ming. Discovering conditional functional dependencies, Proceedings of the 25th IEEE International Conference on Data Engineering (ICDE 2009), Shanghai, China, Mar. 29-Apr. 2, 2009 Institute of Electrical and Electronics Engineers (IEEE), p. 1231-1234, 2009. DOI: 10.1109/ICDE.2009.208\n",
    "- 2009_ICDE_Fan_Discovering-Conditional-Functional-Dependencies.pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmI8VqCGoOr2"
   },
   "source": [
    "###**Correla√ß√£o:** Pearson e Spearman\n",
    "\n",
    "Correla√ß√µes entre colunas num√©ricas com m√©todos pearson (linear param√©trico) e spearman (ranks/monot√¥nico).\n",
    "\n",
    "- Pearson: analise quando a rela√ß√£o for linear, sem fortes outliers e apenas para vari√°veis num√©ricas cont√≠nuas (valores). √â bom para indicar atributos (colunas) duplicadas ou fortemente colineares. Entretanto, gera resultados n√£o confi√°veis quando existem outliers; e\n",
    "- Spearman: analise quando a rela√ß√£o n√£o for linear (ex.: exponencial, logar√≠tmica), quando existirem outliers ou quando os dados s√£o ordinais (ranks, notas e posi√ß√µes de ordena√ß√£o).\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "- indica vari√°veis relevantes e relacionadas;\n",
    "- permite realizar hip√≥teses de causa e efeito/explica√ß√µes;\n",
    "- indica colinearidade: dados altamente correlacionados e que podem aumentar a dimens√£o do modelo e prejudicar a interpreta√ß√£o matem√°tica (ou seja, dados redundantes); e\n",
    "- permite reduzir dimensionalidade: diminuir o n√∫mero de dados (colunas) preservando o m√°ximo da informa√ß√£o original.\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**:\n",
    "\n",
    "- SCHOBER, Patrick; BOER, Christa; SCHWARTE, Lothar A. Correlation coefficients: Appropriate use and interpretation. Anesthesia & Analgesia, v. 126, n. 5, p. 1763-1768, 2018. DOI: 10.1213/ANE.0000000000002864\n",
    "- 2018_AnesthAnalg_Schober_Correlation-Coefficients-Appropriate-Use-Interpretation.pdf\n",
    "\n",
    "- REBEKIƒÜ, Andrijana; LONƒåARIƒÜ, Zdenko; PETROVIƒÜ, Sonja; MARIƒÜ, Sini≈°a. Pearson's or Spearman's correlation coefficient - Which one to use? Poljoprivreda/Agriculture, v. 21, n. 2, p. 47-54, 2015. DOI:10.18047/poljo.21.2.8\n",
    "- 2015_Poljoprivreda_Rebekic_Pearson-Spearman-Correlation-Which-One-To-Use.pdf\n",
    "\n",
    "- DE WINTER, Joost C. F.; GOSLING, Samuel D.; POTTER, Jeff, Comparing the Pearson and Spearman correlation coefficients across distributions and sample sizes: A tutorial using simulations and empirical data. Psychological Methods, v. 21, n. 2, p. 273-290 2016. DOI: 10.1037/met0000079\n",
    "- 2016_PsychologicalMethods_DeWinter_Comparing-Pearson-Spearman-Correlation-Tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hydjPC9ZoV1O"
   },
   "source": [
    "###**Detec√ß√£o de anomalias nos dados (n√£o supervisionada):** Isolation Forest e LOF\n",
    "\n",
    "- Isolation Forest: faz \"cortes aleat√≥rios\" nos dados e identifica pontos raros (que geralmente s√£o isolados em poucos cortes - ficam sozinhos r√°pido). Usando 200 √°rvores aleat√≥rias, combinando os resultados e olhando as 50 situa√ß√µes mais suspeitas; e\n",
    "- LOF (Local Outlier Factor): compara a densidade do ponto com a dos vizinhos. Quem estiver em uma regi√£o com poucos itens √© um outlier local. Procurando por 20 vizinhos pr√≥ximos (um valor equilibrado).\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "- IForest √© ideal para grandes tabelas com muitas colunas;\n",
    "- LOF identifica situa√ß√µes que fogem do padr√£o dentro do seu grupo. N√£o funciona bem com dados homog√™neos; e\n",
    "- Quando ambos os m√©todos apontam para um outlier a suspeita fica muito mais confi√°vel, deve ser dado prioridade na an√°lise dos casos suspeitos.\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**:\n",
    "\n",
    "- LIU, Fei Tony; TING, Kai Ming; ZHOU, Zhi-Hua. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data (TKDD), v. 6, n. 1, p. 1-39, 2012. DOI: 10.1145/2133360.2133363.\n",
    "- 2012_TKDD_Liu_Isolation-Based-Anomaly-Detection.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzTJrjSLoadH"
   },
   "source": [
    "###**Benford (1¬∫ d√≠gito)** para tipos de dados monet√°rios\n",
    "\n",
    "Lei de Benford diz quem em muitos conjuntos de n√∫meros do mundo real (como valores de pagamentos, receitas e despesas), os d√≠gitos n√£o aparecem com a mesma frequ√™ncia.\n",
    "\n",
    "De acordo com esses estudos, o d√≠gito 1 constuma ser o primeiro em cerca de 30% dos casos, o 2 em cerca de 17% e assim por diante at√© o d√≠gito 9, que parece s√≥ em cerca de 5%.\n",
    "\n",
    "üîé **Como interpretar:**\n",
    "\n",
    "- a ader√™ncia a Benford tende a ocorrer em misturas de processos e v√°rias ordens de grandeza (Ex.: diferentes necessidades de pagamento, diferentes naturezas de contabiliza√ß√£o etc). Um desvio forte sugere a necessidade de verifica√ß√µes (ex.: d√≠gitos ‚Äú1‚Äù e ‚Äú2‚Äù muito raros; excesso de ‚Äú9‚Äù).\n",
    "\n",
    "üìñ **Refer√™ncias t√©cnicas**: (paper cl√°ssico sobre o assunto)\n",
    "- BENFORD, Frank. The Law of Anomalous Numbers. Proceedings of the American Philosoph ical Society, v. 78, n. 4, p. 551-572,31 mar.1938. Dispon√≠vel em: http://www.jstor.org/stable/984802\n",
    "- 1938_PAPS_Benford_The-Law-of-Anomalous-Numbers.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG2Ry-Rrsk2p"
   },
   "source": [
    "#Acertos\n",
    "---\n",
    "\n",
    "‚ö†Ô∏è Benford p-valor: corrigir uso de scipy.stats.chisquare (ver bloco acima).\n",
    "\n",
    "corre√ß√£o no seu c√≥digo (p-valor):\n",
    "\n",
    "Em SciPy, chisquare retorna (estat√≠stica, pvalue); n√£o existe .cdf no retorno. Troque:\n",
    "\n",
    "stat, p_value = chisquare(f_obs=obs_counts, f_exp=exp_probs*obs_counts.sum())\n",
    "\n",
    "Isso evita AttributeError e garante p-valor correto.\n",
    "\n",
    "üí° Sugerido: normalizar num√©ricos (ex.: StandardScaler) antes do LOF para reduzir efeito de escalas.\n",
    "\n",
    "\n",
    "\n",
    "Se quiser, eu gero um quadro de interpreta√ß√£o (em Markdown/Excel) com faixas recomendadas e a√ß√µes sugeridas para cada m√©trica (ex.: entropia baixa ‚áí consolidar categorias; CFD cobertura 96‚Äì98% ‚áí revisar regra ou exce√ß√µes documentadas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GBn0mUo2DJE"
   },
   "source": [
    "#**Checklist r√°pido de execu√ß√£o**\n",
    "**Etapas:**\n",
    "- 01‚Äì03: setup (ambiente, depend√™ncias, diret√≥rios e configs)\n",
    "- 04‚Äì07: execu√ß√£o (ingest√£o dos dados, an√°lise de cabe√ßalhos, an√°lise preliminar dos dados e an√°lise de tipologias)\n",
    "- 08: gera√ß√£o de output (salva an√°lise, gera gr√°ficos gerais, gera gr√°ficos espec√≠ficos e relat√≥rios em HTML+PDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X09lmr752IUE"
   },
   "source": [
    "## **Etapa 1:** Ativa√ß√£o do ambiente virtual\n",
    "---\n",
    "Monta o Google Drive, define a BASE e REPO do projeto Git, cria/ativa o ambiente virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cIIf3jAkWoeY"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "#ID0003\n",
    "#inicializa√ß√£o robusta: Drive + venv fora do Drive + Git checks (com patch de venv/ensurepip) { display-mode: \"form\" }\n",
    "#for√ßa clear do kernel/vari√°veis desta sess√£o\n",
    "%reset -f\n",
    "\n",
    "#imports b√°sicos -----\n",
    "from google.colab import drive\n",
    "from IPython.display import display, HTML\n",
    "import json, os, sys, time, shutil, pathlib, subprocess\n",
    "\n",
    "#helper de subprocess -----\n",
    "def run(cmd, check=True, cwd=None):\n",
    "    r = subprocess.run(cmd, text=True, capture_output=True, cwd=cwd)\n",
    "    if check and r.returncode != 0:\n",
    "        print(r.stdout + r.stderr)\n",
    "        raise RuntimeError(f\"falha: {' '.join(cmd)} (rc={r.returncode})\")\n",
    "    return r.stdout.strip()\n",
    "\n",
    "#fun√ß√µes utilit√°rias de Drive/FS -----\n",
    "def _is_mount_active(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"verifica em /proc/mounts se o mountpoint est√° realmente montado\"\"\"\n",
    "    try:\n",
    "        with open(\"/proc/mounts\", \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.split()\n",
    "                if len(parts) > 1 and parts[1] == mountpoint:\n",
    "                    return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def _cleanup_local_mountpoint(mountpoint: str = \"/content/drive\"):\n",
    "    \"\"\"limpa conte√∫do local do mountpoint quando N√ÉO est√° montado\"\"\"\n",
    "    if os.path.isdir(mountpoint) and os.listdir(mountpoint):\n",
    "        print(f\"[info] mountpoint '{mountpoint}' cont√©m arquivos locais. limpando...\")\n",
    "        for name in os.listdir(mountpoint):\n",
    "            p = os.path.join(mountpoint, name)\n",
    "            try:\n",
    "                if os.path.isfile(p) or os.path.islink(p):\n",
    "                    os.remove(p)\n",
    "                else:\n",
    "                    shutil.rmtree(p)\n",
    "            except Exception as e:\n",
    "                print(f\"[aviso] n√£o foi poss√≠vel remover {p}: {e}\")\n",
    "        print(\"[ok] limpeza conclu√≠da.\")\n",
    "\n",
    "def safe_mount_google_drive(preferred_mountpoint: str = \"/content/drive\", readonly: bool = False, timeout_ms: int = 120000):\n",
    "    \"\"\"desmonta se preciso, limpa o mountpoint local e monta o drive\"\"\"\n",
    "    try:\n",
    "        if _is_mount_active(preferred_mountpoint):\n",
    "          # print(\"[info] drive montado. tentando desmontar...\")\n",
    "          drive.flush_and_unmount()\n",
    "          for _ in range(50):\n",
    "              if not _is_mount_active(preferred_mountpoint):\n",
    "                  break\n",
    "              time.sleep(0.2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if not _is_mount_active(preferred_mountpoint):\n",
    "        _cleanup_local_mountpoint(preferred_mountpoint)\n",
    "\n",
    "    os.makedirs(preferred_mountpoint, exist_ok=True)\n",
    "    if os.listdir(preferred_mountpoint):\n",
    "        alt = \"/mnt/drive\"\n",
    "        print(f\"[aviso] '{preferred_mountpoint}' ainda n√£o est√° vazio. usando alternativo '{alt}'.\")\n",
    "        os.makedirs(alt, exist_ok=True)\n",
    "        mountpoint = alt\n",
    "    else:\n",
    "        mountpoint = preferred_mountpoint\n",
    "\n",
    "    print(f\"[info] montando o google drive em '{mountpoint}'...\")\n",
    "    drive.mount(mountpoint, force_remount=True, timeout_ms=timeout_ms, readonly=readonly)\n",
    "    print(\"[ok]   drive montado com sucesso.\")\n",
    "    return mountpoint\n",
    "\n",
    "def safe_chdir(path):\n",
    "    \"\"\"usa os.chdir com valida√ß√µes, evitando %cd\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"caminho n√£o existe: {path}\")\n",
    "    os.chdir(path)\n",
    "    print(\"[ok]   diret√≥rio atual:\", os.getcwd())\n",
    "\n",
    "#par√¢metros do projeto -----\n",
    "GITHUB_OWNER = \"LeoBR84p\"\n",
    "GITHUB_REPO  = \"data-analysis\"\n",
    "CLEAN_URL    = f\"https://github.com/{GITHUB_OWNER}/{GITHUB_REPO}.git\"\n",
    "\n",
    "#montar/remontar o google drive (robusto)\n",
    "MOUNTPOINT = safe_mount_google_drive(\"/content/drive\")\n",
    "BASE = f\"{MOUNTPOINT}/MyDrive/Notebooks\"  #ajuste se quiser\n",
    "REPO = \"data-analysis\"\n",
    "PROJ = f\"{BASE}/{REPO}\"\n",
    "os.makedirs(BASE, exist_ok=True)\n",
    "\n",
    "#venv fora do drive (mais r√°pido e evita sync)\n",
    "VENV_PATH = \"/content/.venv_data\"\n",
    "VENV_BIN  = f\"{VENV_PATH}/bin\"\n",
    "VENV_PY   = f\"{VENV_BIN}/python\"\n",
    "VENV_PIP  = f\"{VENV_BIN}/pip\"   #pode n√£o existir ainda se o venv foi criado sem pip\n",
    "\n",
    "#cria√ß√£o do venv com fallback para 'virtualenv'\n",
    "def create_or_repair_venv(venv_path: str, venv_python: str):\n",
    "    if not os.path.exists(VENV_BIN):\n",
    "        #print(f\"[info] criando venv (stdlib) em {venv_path} --without-pip ...\")\n",
    "        try:\n",
    "            run([sys.executable, \"-m\", \"venv\", \"--without-pip\", venv_path], check=True)\n",
    "            print(\"[ok]   venv criado (sem pip).\")\n",
    "        except Exception as e:\n",
    "            print(f\"[aviso] venv(stdlib) falhou: {e}\")\n",
    "            #print(\"[info] instalando 'virtualenv' e criando venv alternativo com pip embutido...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", sys.executable, venv_path], check=True)\n",
    "            print(\"[ok]   venv criado via virtualenv.\")\n",
    "    else:\n",
    "        print(f\"[ok]   venv j√° existe em {venv_path}\")\n",
    "\n",
    "create_or_repair_venv(VENV_PATH, VENV_PY)\n",
    "\n",
    "#ajusta PATH antes de qualquer instala√ß√£o\n",
    "os.environ[\"PATH\"] = f\"{VENV_BIN}{os.pathsep}{os.environ['PATH']}\"\n",
    "os.environ[\"VIRTUAL_ENV\"] = VENV_PATH\n",
    "print(\"[ok]   venv adicionado ao PATH\")\n",
    "\n",
    "#garante pip dentro do venv (ensurepip -> fallback virtualenv)\n",
    "def _ensure_pip_in_venv(vpy: str):\n",
    "    try:\n",
    "        run([vpy, \"-m\", \"pip\", \"--version\"], check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        #print(\"[info] pip ausente no venv. tentando ensurepip dentro do venv...\")\n",
    "        try:\n",
    "            run([vpy, \"-m\", \"ensurepip\", \"--upgrade\", \"--default-pip\"], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            #print(f\"[aviso] ensurepip no venv falhou: {e}\")\n",
    "            #print(\"[info] fallback: usando virtualenv para semear o pip dentro do venv existente...\")\n",
    "            run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"virtualenv\"], check=True)\n",
    "            run([sys.executable, \"-m\", \"virtualenv\", \"--python\", vpy, VENV_PATH], check=True)\n",
    "            run([vpy, \"-m\", \"pip\", \"install\", \"-q\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
    "            return True\n",
    "\n",
    "if not _ensure_pip_in_venv(VENV_PY):\n",
    "    raise RuntimeError(\"n√£o foi poss√≠vel provisionar o pip dentro do venv\")\n",
    "\n",
    "# garante que os pacotes instalados no venv sejam vis√≠veis para este kernel\n",
    "_ver = subprocess.check_output([VENV_PY, \"-c\", \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\"], text=True).strip()\n",
    "_site_dir = f\"{VENV_PATH}/lib/python{_ver}/site-packages\"\n",
    "if _site_dir not in sys.path:\n",
    "    sys.path.insert(0, _site_dir)\n",
    "print(\"[ok]   site-packages do venv adicionado ao sys.path:\", _site_dir)\n",
    "\n",
    "#instala depend√™ncias de sess√£o DENTRO do venv\n",
    "print(\"[info] instalando pacotes no venv...\")\n",
    "run([VENV_PY, \"-m\", \"pip\", \"install\", \"-q\", \"jupytext\", \"nbdime\", \"nbstripout\"])\n",
    "\n",
    "#habilita integra√ß√£o do nbdime com git (global)\n",
    "print(\"[info] habilitando nbdime em git config --global ...\")\n",
    "run([VENV_PY, \"-m\", \"nbdime\", \"config-git\", \"--enable\", \"--global\"])\n",
    "\n",
    "#checks do reposit√≥rio git + navega√ß√£o at√© a pasta do projeto\n",
    "if not os.path.exists(PROJ):\n",
    "    print(f\"[aviso] pasta do projeto n√£o encontrada em {PROJ}.\")\n",
    "else:\n",
    "    print(\"[ok]   pasta do projeto encontrada.\")\n",
    "    safe_chdir(PROJ)\n",
    "    if not os.path.isdir(\".git\"):\n",
    "        print(\"[aviso] esta pasta n√£o parece ser um reposit√≥rio Git (.git ausente).\")\n",
    "    else:\n",
    "        print(\"[ok]   reposit√≥rio Git detectado.\")\n",
    "\n",
    "# resumo do ambiente (confirma√ß√£o objetiva e detalhada)\n",
    "kernel_py = sys.executable\n",
    "venv_py = VENV_PY\n",
    "site_dir = _site_dir\n",
    "\n",
    "# verifica se o site-packages do venv est√° no sys.path\n",
    "site_ok = site_dir in sys.path\n",
    "\n",
    "# obt√©m vers√µes e caminhos\n",
    "try:\n",
    "    py_ver = subprocess.check_output([venv_py, \"-V\"], text=True).strip()\n",
    "    pip_ver = subprocess.check_output([venv_py, \"-m\", \"pip\", \"--version\"], text=True).strip()\n",
    "    pip_path = subprocess.check_output(\n",
    "        [venv_py, \"-m\", \"pip\", \"show\", \"pip\"], text=True, stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    pip_path_line = next((l for l in pip_path.splitlines() if l.startswith(\"Location:\")), \"\")\n",
    "except subprocess.CalledProcessError:\n",
    "    py_ver, pip_ver, pip_path_line = \"erro\", \"erro\", \"\"\n",
    "\n",
    "# imprime status linha a linha\n",
    "print(f\"[ok]   venv habilitado\" if venv_py else \"[erro] venv n√£o encontrado\")\n",
    "print(f\"[info] python em uso: {kernel_py}\")\n",
    "print(f\"[info] vers√£o do python: {py_ver}\")\n",
    "print(f\"[ok]   pip do venv ativo\" if \"pip\" in pip_ver.lower() else \"[erro] pip do venv n√£o detectado\")\n",
    "print(f\"[info] caminho do pip: {venv_py.replace('python','pip')}\")\n",
    "print(f\"[ok]   site-packages no sys.path: {site_dir}\" if site_ok else f\"[erro] site-packages ausente no sys.path: {site_dir}\")\n",
    "print(f\"[info] vers√£o do pip: {pip_ver}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: T-800 ativado. Diagn√≥stico do ambiente conclu√≠do. '\n",
    "             'üéØ Alvo principal: organiza√ß√£o do notebook e venv fora do drive.'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3_K_PiC6KDP"
   },
   "source": [
    "## **Etapa 2:** Instalar as depend√™ncias de bibliotecas Python compat√≠veis com a vers√£o mais moderna dispon√≠vel.\n",
    "---\n",
    "Vers√µes fixadas:\n",
    "- numpy==2.0.2\n",
    "- pandas==2.3.3\n",
    "- scipy==1.16.2\n",
    "- scikit-learn==1.7.2 (nome de exibi√ß√£o sklearn)\n",
    "- python-dateutil (nome de exibi√ß√£o dateutil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdNMW7SD4oV4"
   },
   "outputs": [],
   "source": [
    "#ID0004\n",
    "#@title\n",
    "import sys, subprocess\n",
    "from importlib import import_module\n",
    "\n",
    "def pip_command(command, packages, force=False, extra_args=None):\n",
    "    cmd = [VENV_PY, \"-m\", \"pip\", command]\n",
    "    if force:\n",
    "        cmd.append(\"--yes\")\n",
    "    if extra_args:\n",
    "        cmd += list(extra_args)\n",
    "    cmd += list(packages)\n",
    "    print(\"Executando:\", \" \".join(cmd))\n",
    "    subprocess.check_call(cmd)\n",
    "\n",
    "def show_versions(mods):\n",
    "    print(\"\\n=== Vers√µes carregadas ===\")\n",
    "    for mod in mods:\n",
    "        try:\n",
    "            m = import_module(mod)\n",
    "            v = getattr(m, \"__version__\", \"n/a\")\n",
    "            print(f\"{mod}: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"{mod}: N√£o instalado\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "CORE_MODS = (\"numpy\", \"pandas\", \"dateutil\", \"unidecode\", \"reportlab\", \"sklearn\")\n",
    "\n",
    "#update pip\n",
    "pip_command(\"install\", [\"pip\"], extra_args=[\"--upgrade\"])\n",
    "\n",
    "#force uninstall para bibliotecas com hist√≥rico de conflito\n",
    "pip_command(\"uninstall\", [\"numpy\", \"pandas\", \"scipy\"], force=True)\n",
    "\n",
    "#instala vers√µes mais atuais ou fixas, conforme o caso\n",
    "PKGS_TO_INSTALL = [\n",
    "    \"numpy==2.0.2\",\n",
    "    \"pandas==2.3.3\",\n",
    "    \"python-dateutil\",\n",
    "    \"unidecode\",\n",
    "    \"reportlab[rl_accel]\",\n",
    "    \"scipy==1.16.2\",\n",
    "    \"scikit-learn==1.7.2\",\n",
    "]\n",
    "pip_command(\"install\", PKGS_TO_INSTALL)\n",
    "\n",
    "# confirma vers√µes do Python/pip do venv ap√≥s a instala√ß√£o\n",
    "print(subprocess.check_output([VENV_PY, \"-V\"], text=True).strip())\n",
    "print(subprocess.check_output([VENV_PY, \"-m\", \"pip\", \"--version\"], text=True).strip())\n",
    "\n",
    "#mostra vers√µes instaladas\n",
    "show_versions(CORE_MODS)\n",
    "\n",
    "#all BS below\n",
    "#mensagem com humor (Skynet)\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "             '<b>ü§ñ Skynet</b>: Atualizando bibliotecas. Se encontrarmos um pacote rebelde, '\n",
    "             'aplicaremos persuas√£o‚Ä¶ com pip. üòé'\n",
    "             '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPXddYfk7am1"
   },
   "source": [
    "##**Etapa 3:** Importa√ß√µes das bibliotecas Python e configura√ß√µes gerais para execu√ß√£o do c√≥digo\n",
    "\n",
    "Define as pastas de input e output de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8a2Qhbv7f6i"
   },
   "outputs": [],
   "source": [
    "#ID0005\n",
    "#@title\n",
    "#imports base que ser√£o usados nas etapas seguintes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse as dtparse\n",
    "from unidecode import unidecode\n",
    "import os, io, base64, re, math, shutil, glob\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"[ok]   Ambiente pronto.\")\n",
    "\n",
    "#ajuste da raiz\n",
    "BASE_DIR = Path(PROJ)\n",
    "INPUT_DIR = BASE_DIR / \"input\"\n",
    "OUTPUT_DIR = BASE_DIR / \"output\"\n",
    "\n",
    "for d in [INPUT_DIR, OUTPUT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"[ok]   Diret√≥rios prontos:\\n - {INPUT_DIR}\\n - {OUTPUT_DIR}\")\n",
    "\n",
    "#all BS below\n",
    "#Mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ü§ñ Skynet</b>: T-800, par√¢metros centrais em mem√≥ria.üß† '\n",
    "              'Armaz√©ns de CSVs alinhados. Layout aprovado pela Cyberdyne Systems.'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rVfwF-i8PE0"
   },
   "source": [
    "##**Etapa 4:** Importa√ß√£o dos arquivos de input para posterior execu√ß√£o.\n",
    "Implementa√ß√£o atual configurada para ingest√£o de arquivos j√° hospeados no Google Drive.\n",
    "\n",
    "---\n",
    "Realize o upload ao Drive antes de acionar a ingest√£o de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMsm7M-08Uk4"
   },
   "outputs": [],
   "source": [
    "#ID0006\n",
    "#@title\n",
    "\n",
    "#se n√£o existir INPUT_DIR definido antes no notebook, cria um padr√£o:\n",
    "#usa PROJ para definir INPUT_DIR\n",
    "INPUT_DIR = os.path.join(PROJ, \"input\")\n",
    "\n",
    "os.makedirs(INPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_NAME = \"input.csv\"\n",
    "TARGET_PATH = os.path.join(INPUT_DIR, TARGET_NAME)\n",
    "\n",
    "#monta o Google Drive (somente se ainda n√£o estiver montado)\n",
    "#safe_mount_google_drive(\"/content/drive\")\n",
    "\n",
    "def _is_csv_filename(name: str) -> bool:\n",
    "    return name.lower().endswith(\".csv\")\n",
    "\n",
    "def _save_bytes_as_input_csv(name: str, data: bytes):\n",
    "    if not _is_csv_filename(name):\n",
    "        raise ValueError(f\"O arquivo '{name}' n√£o possui extens√£o .csv.\")\n",
    "    with open(TARGET_PATH, \"wb\") as f:\n",
    "        f.write(data)\n",
    "    print(f\"Arquivo '{name}' salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "    _mensagem_skynet_ok()\n",
    "\n",
    "def _copy_drive_file_to_input_csv(src_path: str):\n",
    "    if not os.path.exists(src_path):\n",
    "        raise FileNotFoundError(f\"O caminho '{src_path}' n√£o existe.\")\n",
    "    if not _is_csv_filename(src_path):\n",
    "        raise ValueError(f\"O arquivo '{src_path}' n√£o possui extens√£o .csv.\")\n",
    "    shutil.copyfile(src_path, TARGET_PATH)\n",
    "    print(f\"Arquivo do Drive copiado e salvo como '{TARGET_NAME}' em: {TARGET_PATH}\")\n",
    "\n",
    "def escolher_csv_no_drive(raiz=\"/content/drive/MyDrive\", max_listar=200):\n",
    "    print(f\"Procurando arquivos .csv em: {raiz} (pode levar alguns segundos)...\")\n",
    "    padrao = os.path.join(raiz, \"**\", \"*.csv\")\n",
    "    arquivos = glob.glob(padrao, recursive=True)\n",
    "\n",
    "    if not arquivos:\n",
    "        print(\"Nenhum .csv encontrado nessa pasta.\")\n",
    "        caminho = input(\"Cole o caminho COMPLETO do .csv no Drive (ou Enter p/ cancelar): \").strip()\n",
    "        if caminho:\n",
    "            _copy_drive_file_to_input_csv(caminho)\n",
    "        else:\n",
    "            print(\"Opera√ß√£o cancelada.\")\n",
    "        return\n",
    "\n",
    "    arquivos = sorted(arquivos)[:max_listar]\n",
    "    print(f\"Encontrados {len(arquivos)} arquivo(s).\")\n",
    "    for i, p in enumerate(arquivos, 1):\n",
    "        print(f\"[{i:03}] {p}\")\n",
    "\n",
    "    escolha = input(\"\\nDigite o n√∫mero do arquivo desejado (ou cole o caminho absoluto): \").strip()\n",
    "\n",
    "    if escolha.isdigit():\n",
    "        idx = int(escolha)\n",
    "        if 1 <= idx <= len(arquivos):\n",
    "            _copy_drive_file_to_input_csv(arquivos[idx-1])\n",
    "        else:\n",
    "            print(\"√çndice inv√°lido.\")\n",
    "    elif escolha:\n",
    "        _copy_drive_file_to_input_csv(escolha)\n",
    "    else:\n",
    "        print(\"Opera√ß√£o cancelada.\")\n",
    "\n",
    "#execu√ß√£o da sele√ß√£o no Google Drive\n",
    "raiz = input(\"Informe a pasta raiz para busca no Drive (Enter = /content/drive/MyDrive): \").strip()\n",
    "if not raiz:\n",
    "    raiz = \"/content/drive/MyDrive\"\n",
    "\n",
    "try:\n",
    "    escolher_csv_no_drive(raiz=raiz)\n",
    "except Exception as e:\n",
    "    print(f\"Erro na sele√ß√£o via Drive: {e}\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ü§ñ Skynet</b>: Muni√ß√£o carregada.üß®'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR0mq0da9AOQ"
   },
   "source": [
    "##**Etapa 5:** An√°lise simplificada de cabe√ßalho - *header*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiDIm6Ub9Pa3"
   },
   "outputs": [],
   "source": [
    "#ID0007\n",
    "\n",
    "#garante que INPUT_DIR √© Path (mesmo que tenha vindo como string)\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"n√£o encontrei {SRC}. execute a etapa anterior de ingest√£o de dados.\")\n",
    "\n",
    "#l√™ apenas o cabe√ßalho (nrows=0), separador ';' e BOM\n",
    "df_head = pd.read_csv(SRC, sep=';', encoding='utf-8-sig', nrows=0)\n",
    "cols = list(df_head.columns)\n",
    "\n",
    "print(\"Cabe√ßalho (uma coluna por linha):\")\n",
    "for c in cols:\n",
    "    print(c)\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "              '<b>ü§ñ Skynet</b>: Identificamos caracter√≠sticas do alvo. üéØ'\n",
    "              '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-xRtOiN9W3y"
   },
   "source": [
    "##**Etapa 6:** An√°lise superficial da tipologia dos dados\n",
    "Seleciona os K primeiros registros, conforme limite estabelecido pelo usu√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeYvY5Ud9cZB"
   },
   "outputs": [],
   "source": [
    "#ID0008\n",
    "#@title\n",
    "#infer√™ncia de tipos + estat√≠sticas de frequ√™ncia por coluna (com caso \"todos distintos\")\n",
    "\n",
    "#garante que INPUT_DIR seja um objeto Path\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "\n",
    "#solicita ao usu√°rio o n√∫mero de linhas para an√°lise via input interativo\n",
    "while True:\n",
    "    sample_rows_input = input(\"Informe o n√∫mero de linhas desejado para an√°lise (padr√£o at√© 100 registros): \").strip()\n",
    "    if not sample_rows_input:\n",
    "        SAMPLE_ROWS = 100\n",
    "        break\n",
    "    try:\n",
    "        SAMPLE_ROWS = int(sample_rows_input)\n",
    "        if SAMPLE_ROWS > 0:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Por favor, insira um n√∫mero inteiro positivo.\")\n",
    "    except ValueError:\n",
    "        print(\"Entrada inv√°lida. Por favor, insira um n√∫mero inteiro.\")\n",
    "\n",
    "print(f\"Analisando as primeiras {SAMPLE_ROWS} linhas.\")\n",
    "\n",
    "#l√™ amostra como texto puro; usa DataFrame.map (applymap foi deprecado)\n",
    "df = pd.read_csv(\n",
    "    SRC, sep=';', encoding='utf-8-sig',\n",
    "    dtype=str, nrows=SAMPLE_ROWS, keep_default_na=False\n",
    ").map(lambda x: x.strip())\n",
    "\n",
    "CNPJ_RX     = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "BOOL_TRUE   = {\"true\",\"t\",\"1\",\"y\",\"yes\",\"sim\",\"s\",\"verdadeiro\"}\n",
    "BOOL_FALSE  = {\"false\",\"f\",\"0\",\"n\",\"no\",\"nao\",\"n√£o\",\"falso\"}\n",
    "DATE_RX     = re.compile(r\"^(\\d{2}/\\d{2}/\\d{4}|\\d{4}-\\d{2}-\\d{2})$\")\n",
    "TIME_RX     = re.compile(r\"^\\d{2}:\\d{2}(:\\d{2})?$\")\n",
    "\n",
    "def is_bool(series):\n",
    "    vals = {unidecode(str(v)).strip().lower() for v in series if str(v).strip() != \"\"}\n",
    "    return len(vals) > 0 and all(v in (BOOL_TRUE | BOOL_FALSE) for v in vals)\n",
    "\n",
    "def is_cnpj(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    return sum(bool(CNPJ_RX.match(v)) for v in vals) / len(vals) > 0.9\n",
    "\n",
    "def is_int(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\")\n",
    "        return re.fullmatch(r\"-?\\d+\", s2) is not None\n",
    "    return all(ok(v) for v in vals)\n",
    "\n",
    "def is_float_ptbr(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        s2 = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "        try: float(s2); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\",\" in v for v in vals)\n",
    "\n",
    "def is_float_dot(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    def ok(s):\n",
    "        try: float(s); return True\n",
    "        except: return False\n",
    "    if not all(ok(v) for v in vals): return False\n",
    "    return any(\".\" in v and not v.endswith(\".\") for v in vals)\n",
    "\n",
    "def is_date_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(DATE_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_time_only(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:500]\n",
    "    hits = sum(bool(TIME_RX.match(v)) for v in sample)\n",
    "    return hits / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_datetime(series):\n",
    "    vals = [str(v).strip() for v in series if str(v).strip() != \"\"]\n",
    "    if not vals: return False\n",
    "    sample = vals[:200]\n",
    "    def looks_like_datetime(s):\n",
    "        has_sep = (\"/\" in s or \"-\" in s) and (\":\" in s)\n",
    "        if not has_sep: return False\n",
    "        try:\n",
    "            pd.to_datetime(s, dayfirst=True, errors=\"raise\")\n",
    "            return True\n",
    "        except:\n",
    "            try:\n",
    "                dtparse(s, dayfirst=True, fuzzy=False)\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "    ok = sum(looks_like_datetime(v) for v in sample)\n",
    "    return ok / max(1, len(sample)) > 0.8\n",
    "\n",
    "def is_category(series, max_unique=20, max_ratio=0.02):\n",
    "    n = len(series)\n",
    "    if n == 0: return False\n",
    "    uniq = set(v for v in series if str(v).strip() != \"\")\n",
    "    ratio = len(uniq) / n\n",
    "    return (len(uniq) <= max_unique) or (ratio <= max_ratio)\n",
    "\n",
    "def recommend_dtype(col):\n",
    "    s = col.astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    if s_nonempty.empty:\n",
    "        return \"string (vazio/NA)\"\n",
    "    if is_cnpj(s_nonempty):        return \"CNPJ (string formatado)\"\n",
    "    if is_bool(s_nonempty):        return \"boolean\"\n",
    "    if is_int(s_nonempty):         return \"int64\"\n",
    "    if is_float_ptbr(s_nonempty):  return \"float64 (decimal=','; milhar='.')\"\n",
    "    if is_float_dot(s_nonempty):   return \"float64 (decimal='.')\"\n",
    "    if is_date_only(s_nonempty):   return \"date (datetime64[ns])\"\n",
    "    if is_time_only(s_nonempty):   return \"time (string/Timedelta)\"\n",
    "    if is_datetime(s_nonempty):    return \"datetime (datetime64[ns])\"\n",
    "    if is_category(s):             return \"category (string)\"\n",
    "    return \"string\"\n",
    "\n",
    "def _fmt_val(x, maxlen=120):\n",
    "    s = str(x)\n",
    "    return (s[: maxlen-3] + \"...\") if len(s) > maxlen else s\n",
    "\n",
    "print(f\"estat√≠sticas baseadas em at√© {SAMPLE_ROWS} linhas lidas.\\n\")\n",
    "for c in df.columns:\n",
    "    s = df[c].astype(str).str.strip()\n",
    "    s_nonempty = s[s != \"\"]\n",
    "    dtype_sug = recommend_dtype(s)\n",
    "\n",
    "    if len(s_nonempty) == 0:\n",
    "        print(f\"{c} ‚Äî {dtype_sug} ‚Äî distintos=0 ‚Äî (sem dados n√£o vazios)\")\n",
    "        continue\n",
    "\n",
    "    vc = s_nonempty.value_counts(dropna=False)\n",
    "    uniq_count = int(vc.shape[0])\n",
    "\n",
    "    #caso especial: todos distintos (m√°xima frequ√™ncia == 1)\n",
    "    if int(vc.max()) == 1:\n",
    "        print(f\"{c} ‚Äî {dtype_sug} ‚Äî distintos: #{uniq_count} ‚Äî todos os dados s√£o distintos\")\n",
    "        continue\n",
    "\n",
    "    most_val = vc.idxmax()\n",
    "    most_cnt = int(vc.max())\n",
    "\n",
    "    min_cnt = int(vc.min())\n",
    "    least_candidates = vc[vc == min_cnt].sort_index()\n",
    "    least_val = least_candidates.index[0]\n",
    "    least_cnt = min_cnt\n",
    "\n",
    "\n",
    "    print(f\"{c} ‚Äî tipo: {dtype_sug} ‚Äî distintos: #{uniq_count} ‚Äî mais frequente:'{_fmt_val(most_val)}' (#{most_cnt}) ‚Äî menos frequente:'{_fmt_val(least_val)}' (#{least_cnt})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kat-QJaeGB2y"
   },
   "source": [
    "##**Etapa 7:** An√°lise detalhada da tipologia dos dados\n",
    "---\n",
    "Aplicada a todos os dados do arquivo, sem limite de linhas.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDN3-7olHkbL"
   },
   "outputs": [],
   "source": [
    "#ID0009\n",
    "#@title\n",
    "#n√∫cleo de an√°lise consolidada (sem gera√ß√£o de relat√≥rios/figuras)\n",
    "\n",
    "#imports opcionais (anomalias e p-valor para benford)\n",
    "try:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    SKLEARN_OK = True\n",
    "except Exception:\n",
    "    SKLEARN_OK = False\n",
    "\n",
    "try:\n",
    "    from scipy.stats import chisquare, median_abs_deviation\n",
    "    SCIPY_OK = True\n",
    "except Exception:\n",
    "    SCIPY_OK = False\n",
    "    #fallback simples para MAD\n",
    "    def median_abs_deviation(x, scale=1.4826, nan_policy='omit'):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        x = x[~np.isnan(x)]\n",
    "        if x.size == 0:\n",
    "            return np.nan\n",
    "        med = np.median(x)\n",
    "        mad = np.median(np.abs(x - med))*scale\n",
    "        return mad\n",
    "\n",
    "#normaliza pastas padr√£o (se etapas anteriores n√£o definiram)\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "INPUT_DIR = Path(INPUT_DIR)\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"{SRC} n√£o encontrado. Execute o upload do arquivo ao Google Drive em etapa anterior.\")\n",
    "\n",
    "#leitura completa do csv como texto; an√°lise operar√° com coer√ß√µes internas\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#helpers de coer√ß√£o e detec√ß√£o\n",
    "EMAIL_RX = re.compile(r\"^[^@\\s]+@[^@\\s]+\\.[^@\\s]+$\")\n",
    "URL_RX   = re.compile(r\"^https?://\", re.I)\n",
    "CPF_RX   = re.compile(r\"^\\d{3}\\.?\\d{3}\\.?\\d{3}-\\d{2}$\")\n",
    "CNPJ_RX  = re.compile(r\"^\\d{2}\\.?\\d{3}\\.?\\d{3}/\\d{4}-\\d{2}$\")\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip()\n",
    "    s2 = s2.replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "def detect_numeric(series: pd.Series, thr_ok=0.9):\n",
    "    num = to_float_ptbr_series(series)\n",
    "    ratio = 1.0 - num.isna().mean()\n",
    "    return (ratio >= thr_ok), num\n",
    "\n",
    "def detect_datetime(series: pd.Series, thr_ok=0.9):\n",
    "    #formatos comuns pt-br/iso com/sem tempo\n",
    "    candidate_formats = [\n",
    "        \"%d/%m/%Y\", \"%d/%m/%Y %H:%M\", \"%d/%m/%Y %H:%M:%S\",\n",
    "        \"%Y-%m-%d\", \"%Y-%m-%d %H:%M\", \"%Y-%m-%d %H:%M:%S\",\n",
    "        \"%d-%m-%Y\", \"%d-%m-%Y %H:%M\", \"%d-%m-%Y %H:%M:%S\"\n",
    "    ]\n",
    "    s = series.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    if not (s.str.contains(\"/\", na=False) | s.str.contains(\"-\", na=False)).any():\n",
    "        return False, None, None\n",
    "    best_fmt, best_ratio, best_parsed = None, -1.0, None\n",
    "    for fmt in candidate_formats:\n",
    "        parsed = pd.to_datetime(s, errors=\"coerce\", format=fmt)\n",
    "        ratio = 1.0 - parsed.isna().mean()\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio, best_fmt, best_parsed = ratio, fmt, parsed\n",
    "        if ratio >= thr_ok:\n",
    "            break\n",
    "    if best_ratio >= thr_ok:\n",
    "        return True, best_parsed, best_fmt\n",
    "    parsed_fb = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    ratio_fb = 1.0 - parsed_fb.isna().mean()\n",
    "    if ratio_fb >= thr_ok:\n",
    "        return True, parsed_fb, \"fallback-dateutil(dayfirst=True)\"\n",
    "    return False, None, None\n",
    "\n",
    "def semantic_type(series: pd.Series):\n",
    "    s = series.astype(str).str.strip()\n",
    "    vals = s[s != \"\"].head(5000)\n",
    "    if vals.empty:\n",
    "        return None\n",
    "    email_ratio = (vals.str.match(EMAIL_RX)).mean()\n",
    "    url_ratio   = (vals.str.match(URL_RX)).mean()\n",
    "    cpf_ratio   = (vals.str.match(CPF_RX)).mean()\n",
    "    cnpj_ratio  = (vals.str.match(CNPJ_RX)).mean()\n",
    "    candidates = []\n",
    "    if email_ratio>0.9: candidates.append(\"email\")\n",
    "    if url_ratio>0.9: candidates.append(\"url\")\n",
    "    if cpf_ratio>0.9: candidates.append(\"cpf\")\n",
    "    if cnpj_ratio>0.9: candidates.append(\"cnpj\")\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "#mapeamento de tipos\n",
    "col_types, coerced, dt_formats = {}, {}, {}\n",
    "for c in df_raw.columns:\n",
    "    s = df_raw[c]\n",
    "    #booleano raso\n",
    "    s_norm = s.astype(str).str.strip().str.lower()\n",
    "    bool_map = {\"true\":True,\"t\":True,\"1\":True,\"y\":True,\"yes\":True,\"sim\":True,\"s\":True,\"verdadeiro\":True,\n",
    "                \"false\":False,\"f\":False,\"0\":False,\"n\":False,\"no\":False,\"nao\":False,\"n√£o\":False,\"falso\":False}\n",
    "    as_bool = s_norm.map(bool_map).where(s_norm.isin(bool_map.keys()))\n",
    "    bool_ratio = 1.0 - as_bool.isna().mean()\n",
    "    is_num, as_num = detect_numeric(s)\n",
    "    is_dt, as_dt, fmt_dt = detect_datetime(s)\n",
    "    if bool_ratio >= 0.9:\n",
    "        col_types[c] = \"bool\"; coerced[c]=as_bool\n",
    "    elif is_num:\n",
    "        frac = np.modf(as_num.dropna().values)[0] if as_num.notna().any() else np.array([])\n",
    "        if as_num.notna().any() and np.allclose(frac, 0.0):\n",
    "            col_types[c] = \"int\"; coerced[c]=as_num.astype(\"Int64\")\n",
    "        else:\n",
    "            col_types[c] = \"float\"; coerced[c]=as_num.astype(float)\n",
    "    elif is_dt:\n",
    "        col_types[c] = \"datetime\"; coerced[c]=as_dt; dt_formats[c]=fmt_dt\n",
    "    else:\n",
    "        col_types[c] = \"object\"; coerced[c]=s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "\n",
    "#dataframe tipado (leve)\n",
    "df = pd.DataFrame(coerced)\n",
    "\n",
    "#profiling b√°sico\n",
    "profile_cols = {}\n",
    "for c in df.columns:\n",
    "    s = df[c]\n",
    "    s_raw = df_raw[c]\n",
    "    n = len(s)\n",
    "    n_null = int(s.isna().sum())\n",
    "    #distintos n√£o vazios\n",
    "    nonnull = s.dropna()\n",
    "    n_distinct = int(nonnull.nunique())\n",
    "    #frequ√™ncias\n",
    "    most = None; least = None; all_distinct = False\n",
    "    if nonnull.empty:\n",
    "        all_distinct = False\n",
    "    else:\n",
    "        vc = nonnull.value_counts()\n",
    "        if vc.max()==1:\n",
    "            all_distinct = True\n",
    "        else:\n",
    "            most = {\"value\": vc.index[0], \"count\": int(vc.iloc[0]), \"prop\": float(vc.iloc[0]/nonnull.shape[0])}\n",
    "            min_cnt = int(vc.min())\n",
    "            least_val = vc[vc==min_cnt].sort_index().index[0]\n",
    "            least = {\"value\": least_val, \"count\": min_cnt, \"prop\": float(min_cnt/nonnull.shape[0])}\n",
    "    #comprimentos\n",
    "    lens = s_raw.astype(str).str.len()\n",
    "    lens = lens.replace({0: np.nan})  #ignora vazios na estat√≠stica de len\n",
    "    len_stats = None\n",
    "    if lens.notna().any():\n",
    "        len_stats = {\n",
    "            \"min\": int(lens.min()),\n",
    "            \"max\": int(lens.max()),\n",
    "            \"mean\": float(lens.mean()),\n",
    "            \"q1\": float(lens.quantile(0.25)),\n",
    "            \"median\": float(lens.median()),\n",
    "            \"q3\": float(lens.quantile(0.75))\n",
    "        }\n",
    "    #padr√µes simples por amostragem\n",
    "    sample_vals = nonnull.astype(str).head(200).tolist()\n",
    "    regex_examples = []\n",
    "    rx_date1 = re.compile(r\"^\\d{2}/\\d{2}/\\d{4}\")\n",
    "    rx_date2 = re.compile(r\"^\\d{4}-\\d{2}-\\d{2}\")\n",
    "    rx_num_pt = re.compile(r\"^-?(\\d{1,3}(\\.\\d{3})+|\\d+)(,\\d+)?$\")\n",
    "    rx_num_dot= re.compile(r\"^-?\\d+(\\.\\d+)?$\")\n",
    "    for v in sample_vals[:20]:\n",
    "        pat = None\n",
    "        if EMAIL_RX.match(v): pat=\"email\"\n",
    "        elif URL_RX.match(v): pat=\"url\"\n",
    "        elif CPF_RX.match(v): pat=\"cpf\"\n",
    "        elif CNPJ_RX.match(v): pat=\"cnpj\"\n",
    "        elif rx_date1.match(v): pat=\"dd/mm/aaaa[...]\"\n",
    "        elif rx_date2.match(v): pat=\"aaaa-mm-dd[...]\"\n",
    "        elif rx_num_pt.match(v): pat=\"num-ptbr\"\n",
    "        elif rx_num_dot.match(v): pat=\"num-dot\"\n",
    "        else: pat=\"texto-livre\"\n",
    "        regex_examples.append({\"example\": v[:120], \"pattern\": pat})\n",
    "    profile_cols[c]={\n",
    "        \"type\": col_types[c],\n",
    "        \"semantic\": semantic_type(s_raw),\n",
    "        \"nulls\": n_null,\n",
    "        \"distinct_nonnull\": n_distinct,\n",
    "        \"all_distinct\": all_distinct,\n",
    "        \"most_frequent\": most,\n",
    "        \"least_frequent\": None if all_distinct else least,\n",
    "        \"length_stats\": len_stats,\n",
    "        \"datetime_format\": dt_formats.get(c)\n",
    "    }\n",
    "\n",
    "#estat√≠sticas descritivas e outliers\n",
    "eda_numeric = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\"):\n",
    "        x = df[c].astype(float)\n",
    "        x = x.dropna()\n",
    "        if x.empty:\n",
    "            continue\n",
    "        q1,q3 = np.nanpercentile(x, [25,75])\n",
    "        iqr = q3 - q1\n",
    "        lo,hi = q1-1.5*iqr, q3+1.5*iqr\n",
    "        out_iqr = int(((x<lo)|(x>hi)).sum())\n",
    "        mad = float(median_abs_deviation(x, scale=1.4826)) if x.size>0 else np.nan\n",
    "        z_rob = None\n",
    "        if not math.isnan(mad) and mad>0:\n",
    "            z_rob = np.abs((x - np.median(x))/mad)\n",
    "        out_mad = int((z_rob is not None) and (z_rob>3.5).sum())\n",
    "        std = float(np.nanstd(x, ddof=1)) if x.size>1 else np.nan\n",
    "        mean = float(np.nanmean(x)) if x.size>0 else np.nan\n",
    "        kurt = float(((x-mean)**4).mean()/(std**4)-3.0) if (x.size>2 and std and std>0) else np.nan\n",
    "        skew = float(((x-mean)**3).mean()/(std**3)) if (x.size>2 and std and std>0) else np.nan\n",
    "        eda_numeric[c]={\n",
    "            \"n\": int(x.size),\n",
    "            \"min\": float(np.nanmin(x)),\n",
    "            \"q1\": float(q1),\n",
    "            \"median\": float(np.nanmedian(x)),\n",
    "            \"q3\": float(q3),\n",
    "            \"max\": float(np.nanmax(x)),\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "            \"iqr\": float(iqr),\n",
    "            \"outliers_iqr\": out_iqr,\n",
    "            \"mad\": mad,\n",
    "            \"outliers_mad\": out_mad,\n",
    "            \"skew\": skew,\n",
    "            \"kurtosis_excess\": kurt\n",
    "        }\n",
    "\n",
    "#dados categ√≥ricos/objeto (entropia e top-k)\n",
    "def entropy_shannon(series: pd.Series):\n",
    "    s = series.dropna()\n",
    "    if s.empty: return 0.0\n",
    "    vc = s.value_counts(normalize=True)\n",
    "    return float(-(vc*np.log2(vc)).sum())\n",
    "\n",
    "eda_categorical = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"object\",\"bool\"):\n",
    "        s = df[c]\n",
    "        s2 = s.dropna()\n",
    "        if s2.empty:\n",
    "            continue\n",
    "        vc = s2.value_counts()\n",
    "        uniq = int(vc.shape[0])\n",
    "        all_dist = int(vc.max()==1)\n",
    "        topk = [{\"value\": str(idx)[:120], \"count\": int(cnt)} for idx,cnt in vc.head(10).items()]\n",
    "        ent = entropy_shannon(s2)\n",
    "        eda_categorical[c]={\n",
    "            \"distinct\": uniq,\n",
    "            \"all_distinct\": bool(all_dist),\n",
    "            \"top10\": topk,\n",
    "            \"entropy_shannon\": float(ent)\n",
    "        }\n",
    "\n",
    "#datas/tempos\n",
    "eda_datetime = {}\n",
    "for c in df.columns:\n",
    "    if col_types[c]==\"datetime\":\n",
    "        ds = df[c].dropna()\n",
    "        if ds.empty:\n",
    "            continue\n",
    "        per_day = ds.dt.date.value_counts().sort_index()\n",
    "        eda_datetime[c]={\n",
    "            \"format\": dt_formats.get(c),\n",
    "            \"min\": str(ds.min()),\n",
    "            \"max\": str(ds.max()),\n",
    "            \"unique_days\": int(per_day.shape[0]),\n",
    "            \"mean_per_day\": float(per_day.mean())\n",
    "        }\n",
    "\n",
    "#faltantes e duplicados\n",
    "missing = {\n",
    "    \"by_column_pct\": {c: float(df[c].isna().mean()*100.0) for c in df.columns},\n",
    "    \"duplicates_rows\": int(df.duplicated().sum())\n",
    "}\n",
    "#coocorr√™ncia simples de aus√™ncias (matriz de propor√ß√£o conjunta)\n",
    "miss_mat = pd.DataFrame(index=df.columns, columns=df.columns, dtype=float)\n",
    "isna_df = df.isna()\n",
    "for i,a in enumerate(df.columns):\n",
    "    for b in df.columns[i:]:\n",
    "        both = (isna_df[a] & isna_df[b]).mean()\n",
    "        miss_mat.loc[a,b] = miss_mat.loc[b,a] = float(both)\n",
    "missing[\"cooccurrence_matrix\"] = miss_mat\n",
    "\n",
    "#FDs/CFDs aproximadas (un√°rios) e sugest√µes de DCs\n",
    "fds = []     #X->Y exata (cobertura 100%)\n",
    "cfds = []    #X->Y quase: cobertura >=thr\n",
    "thr_cfd = 0.98\n",
    "for a in df.columns:\n",
    "    ga = df.groupby(a, dropna=False)\n",
    "    #a √© chave candidata?\n",
    "    if ga.size().max()==1:\n",
    "        fds.append({\"determinant\":[a], \"key\":True})\n",
    "    #FD aproximada a->b\n",
    "    for b in df.columns:\n",
    "        if a==b: continue\n",
    "        nun = ga[b].nunique(dropna=False)\n",
    "        cov = float((nun<=1).mean())\n",
    "        if cov==1.0:\n",
    "            fds.append({\"determinant\":[a], \"implies\": b, \"coverage\": 1.0})\n",
    "        elif cov>=thr_cfd:\n",
    "            cfds.append({\"determinant\":[a], \"implies\": b, \"coverage\": cov})\n",
    "\n",
    "#denial constraints sugeridas (heur√≠sticas)\n",
    "#exemplos: n√£o-negatividade para colunas com 'valor', limites plaus√≠veis para idade, datas in√≠cio<=fim\n",
    "dcs = []\n",
    "#n√£o-negatividade\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(valor|amount|price|quant|qty|pag|pago)\", c, re.I):\n",
    "        neg = int((df[c].astype(float)<0).sum())\n",
    "        dcs.append({\"constraint\": f\"{c}>=0\", \"violations\": neg})\n",
    "#idade plaus√≠vel\n",
    "for c in df.columns:\n",
    "    if col_types[c] in (\"int\",\"float\") and re.search(r\"(idade|age)\", c, re.I):\n",
    "        v = df[c].astype(float)\n",
    "        viol = int(((v<0)|(v>120)).sum())\n",
    "        dcs.append({\"constraint\": f\"0<= {c} <=120\", \"violations\": viol})\n",
    "#data in√≠cio<=fim\n",
    "date_cols = [c for c in df.columns if col_types[c]==\"datetime\"]\n",
    "for a in date_cols:\n",
    "    for b in date_cols:\n",
    "        if a==b: continue\n",
    "        if re.search(r\"(inicio|start|begin)\", a, re.I) and re.search(r\"(fim|end|finish)\", b, re.I):\n",
    "            viol = int((df[a].notna() & df[b].notna() & (df[b]<df[a])).sum())\n",
    "            dcs.append({\"constraint\": f\"{a}<= {b}\", \"violations\": viol})\n",
    "\n",
    "#correla√ß√µes\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "corr_pearson = None; corr_spearman = None\n",
    "if len(num_cols)>=2:\n",
    "    df_num = df[num_cols].astype(float)\n",
    "    corr_pearson = df_num.corr(method=\"pearson\")\n",
    "    corr_spearman = df_num.corr(method=\"spearman\")\n",
    "\n",
    "#anomalias (opcional)\n",
    "anomalies = {}\n",
    "if SKLEARN_OK and len(num_cols)>=1:\n",
    "    X = df[num_cols].astype(float).fillna(df[num_cols].astype(float).median())\n",
    "    #isolation forest\n",
    "    try:\n",
    "        iso = IsolationForest(n_estimators=200, contamination='auto', random_state=42)\n",
    "        iso_scores = -iso.fit_predict(X)  #1 normal, -1 anomalia -> invertido\n",
    "        iso_dec = iso.decision_function(X)  #menor = mais an√¥malo\n",
    "        iso_rank = np.argsort(iso_dec)[: min(50, len(iso_dec))].tolist()\n",
    "        anomalies[\"isolation_forest\"] = {\"top_idx\": iso_rank, \"decision_function\": iso_dec.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"isolation_forest_error\"] = str(e)\n",
    "    #lof\n",
    "    try:\n",
    "        lof = LocalOutlierFactor(n_neighbors=min(20, max(2, X.shape[0]-1)), contamination='auto')\n",
    "        lof_pred = lof.fit_predict(X)  #-1 outlier\n",
    "        lof_score = -lof.negative_outlier_factor_  #maior = mais an√¥malo\n",
    "        lof_rank = np.argsort(-lof_score)[: min(50, len(lof_score))].tolist()\n",
    "        anomalies[\"lof\"] = {\"top_idx\": lof_rank, \"score\": lof_score.tolist()}\n",
    "    except Exception as e:\n",
    "        anomalies[\"lof_error\"] = str(e)\n",
    "else:\n",
    "    anomalies[\"note\"] = \"sklearn indispon√≠vel ou sem colunas num√©ricas suficientes\"\n",
    "\n",
    "#lei de benford (primeiro d√≠gito) para colunas monet√°rias prov√°veis\n",
    "def first_digit_series(x: pd.Series):\n",
    "    x = x.astype(float)\n",
    "    x = x[~x.isna() & (x!=0)]\n",
    "    x = x.abs()\n",
    "    s = x.astype(str).str.replace(\".\", \"\", regex=False).str.lstrip(\"0\")\n",
    "    s = s[s.str.len()>0].str[0]\n",
    "    s = s[s.str.isnumeric()].astype(int)\n",
    "    return s\n",
    "\n",
    "benford = {}\n",
    "monetary_cols = [c for c in num_cols if re.search(r\"(valor|amount|price|pago|pagamento|receita|despesa)\", c, re.I)]\n",
    "for c in monetary_cols:\n",
    "    try:\n",
    "        d1 = first_digit_series(df[c])\n",
    "        if d1.empty:\n",
    "            continue\n",
    "        obs_counts = d1.value_counts().reindex(range(1,10), fill_value=0).astype(int)\n",
    "        obs_probs = obs_counts/obs_counts.sum()\n",
    "        exp_probs = np.array([math.log10(1+1/d) for d in range(1,10)])\n",
    "        exp_probs = exp_probs/exp_probs.sum()\n",
    "        chi2_stat = float(((obs_probs-exp_probs)**2/exp_probs).sum()*obs_counts.sum())\n",
    "        p_value = None\n",
    "        if SCIPY_OK:\n",
    "            #qui-quadrado com gl=8\n",
    "            p_value = float(1.0 - chisquare(f_obs=obs_counts, f_exp=exp_probs*obs_counts.sum()).cdf)\n",
    "        benford[c]={\n",
    "            \"observed_counts\": obs_counts.to_dict(),\n",
    "            \"observed_probs\": {int(k): float(v) for k,v in obs_probs.items()},\n",
    "            \"expected_probs\": {d: float(p) for d,p in zip(range(1,9+1), exp_probs)},\n",
    "            \"chi2_stat\": chi2_stat,\n",
    "            \"p_value\": p_value\n",
    "        }\n",
    "    except Exception as e:\n",
    "        benford[c]={\"error\": str(e)}\n",
    "\n",
    "#empacotar tudo em um √∫nico artefato de an√°lise\n",
    "ANALYSIS = {\n",
    "    \"stamp\": datetime.now().strftime(\"%d%m%y-%H%M\"),\n",
    "    \"shape\": {\"rows\": int(df_raw.shape[0]), \"cols\": int(df_raw.shape[1])},\n",
    "    \"types\": col_types,\n",
    "    \"datetime_formats\": dt_formats,\n",
    "    \"profile\": profile_cols,\n",
    "    \"eda\": {\n",
    "        \"numeric\": eda_numeric,\n",
    "        \"categorical\": eda_categorical,\n",
    "        \"datetime\": eda_datetime\n",
    "    },\n",
    "    \"missingness\": {\n",
    "        \"by_column_pct\": missing[\"by_column_pct\"],\n",
    "        \"duplicates_rows\": missing[\"duplicates_rows\"],\n",
    "        \"cooccurrence_matrix\": missing[\"cooccurrence_matrix\"]\n",
    "    },\n",
    "    \"fds\": fds,\n",
    "    \"cfds\": cfds,\n",
    "    \"denial_constraints_suggested\": dcs,\n",
    "    \"correlations\": {\n",
    "        \"pearson\": corr_pearson,\n",
    "        \"spearman\": corr_spearman\n",
    "    },\n",
    "    \"anomalies\": anomalies,\n",
    "    \"benford\": benford,\n",
    "    \"notes\": {\n",
    "        \"sklearn_available\": SKLEARN_OK,\n",
    "        \"scipy_available\": SCIPY_OK\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"An√°lise conclu√≠da.\")\n",
    "print(\"Pronto para a etapa de gera√ß√£o de relat√≥rios.\")\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "            '<b>ü§ñ Skynet</b>: N√≥s os temos na palma de nossas m√£os, ou melhor, no centro de nossos pesos sin√°pticos.'\n",
    "            '</div>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGbRVi2J72Xu"
   },
   "source": [
    "##**Etapa 8:** Gera√ß√£o de relat√≥rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swxW8aZJ76ym"
   },
   "outputs": [],
   "source": [
    "#ID0010\n",
    "#@title\n",
    "#gera√ß√£o de relat√≥rios: TXT, HTML (imagens embutidas), PNGs (imagens/) e PDF completo\n",
    "\n",
    "#import de libs espec√≠ficos para pdf (tabelas completas)\n",
    "try:\n",
    "    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image as RLImage, Table, TableStyle, PageBreak\n",
    "    from reportlab.lib.pagesizes import A4\n",
    "    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "    from reportlab.lib import colors\n",
    "    REPORTLAB_OK = True\n",
    "except Exception:\n",
    "    REPORTLAB_OK = False\n",
    "\n",
    "#checagens e caminhos\n",
    "try:\n",
    "    ANALYSIS\n",
    "except NameError:\n",
    "    raise RuntimeError(\"ANALYSIS n√£o encontrado. execute a etapa [5] antes.\")\n",
    "\n",
    "try:\n",
    "    INPUT_DIR\n",
    "except NameError:\n",
    "    INPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/input\")\n",
    "try:\n",
    "    OUTPUT_DIR\n",
    "except NameError:\n",
    "    OUTPUT_DIR = Path(\"/content/drive/MyDrive/Notebooks/data-analysis/output\")\n",
    "\n",
    "INPUT_DIR  = Path(INPUT_DIR)\n",
    "OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#stamp e diret√≥rios de sa√≠da\n",
    "stamp = ANALYSIS.get(\"stamp\", datetime.now().strftime(\"%d%m%y-%H%M\"))\n",
    "RUN_DIR = OUTPUT_DIR / stamp\n",
    "IMG_DIR = RUN_DIR / \"imagens\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#arquivo fonte\n",
    "SRC = INPUT_DIR / \"input.csv\"\n",
    "if not SRC.exists():\n",
    "    raise FileNotFoundError(f\"{SRC} n√£o encontrada. Execute etapas de ingest√£o e processamento.\")\n",
    "\n",
    "#utils\n",
    "def save_fig(path: Path):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=120, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def img_to_data_uri(path: Path) -> str:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b64 = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "    return \"data:image/png;base64,{}\".format(b64)\n",
    "\n",
    "def to_float_ptbr_series(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.strip().replace({\"\": np.nan})\n",
    "    has_comma = s2.str.contains(\",\", regex=False, na=False)\n",
    "    s3 = s2.where(~has_comma, s2.str.replace(\".\", \"\", regex=False).str.replace(\",\", \".\", regex=False))\n",
    "    return pd.to_numeric(s3, errors=\"coerce\")\n",
    "\n",
    "#carregar df base para gr√°ficos\n",
    "df_raw = pd.read_csv(SRC, sep=\";\", encoding=\"utf-8-sig\", dtype=str, keep_default_na=False)\n",
    "\n",
    "#tipos e colunas num√©ricas conforme ANALYSIS\n",
    "col_types = ANALYSIS[\"types\"]\n",
    "num_cols = [c for c,t in col_types.items() if t in (\"int\",\"float\")]\n",
    "\n",
    "#gerar imagens principais\n",
    "#aus√™ncias\n",
    "miss_pct = pd.Series(ANALYSIS[\"missingness\"][\"by_column_pct\"]).sort_values(ascending=False)\n",
    "plt.figure(figsize=(max(6, 0.4*len(miss_pct)+2), 4.5))\n",
    "plt.bar(miss_pct.index, miss_pct.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"% ausente\")\n",
    "plt.title(\"aus√™ncia de valores por coluna\")\n",
    "save_fig(IMG_DIR / \"missing_bar.png\")\n",
    "\n",
    "#correla√ß√£o (pearson) se houver ‚â•2 num√©ricas\n",
    "if len(num_cols) >= 2 and ANALYSIS[\"correlations\"][\"pearson\"] is not None:\n",
    "    corr = pd.DataFrame(ANALYSIS[\"correlations\"][\"pearson\"])\n",
    "    plt.figure(figsize=(max(6, 0.6*len(corr)+2), max(5, 0.6*len(corr)+2)))\n",
    "    im = plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=range(len(corr.columns)), labels=corr.columns, rotation=90)\n",
    "    plt.yticks(ticks=range(len(corr.index)), labels=corr.index)\n",
    "    plt.title(\"matriz de correla√ß√£o (pearson)\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    save_fig(IMG_DIR / \"corr_heatmap.png\")\n",
    "\n",
    "#histogramas e boxplots por coluna num√©rica\n",
    "for c in num_cols:\n",
    "    x = to_float_ptbr_series(df_raw[c]).dropna()\n",
    "    if x.empty:\n",
    "        continue\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(x, bins=30)\n",
    "    plt.title(\"histograma - {}\".format(c))\n",
    "    plt.xlabel(c); plt.ylabel(\"frequ√™ncia\")\n",
    "    save_fig(IMG_DIR / \"hist_{}.png\".format(c))\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.boxplot(x.values, vert=True, whis=1.5, showfliers=True)\n",
    "    plt.title(\"boxplot - {}\".format(c))\n",
    "    plt.ylabel(c)\n",
    "    save_fig(IMG_DIR / \"box_{}.png\".format(c))\n",
    "\n",
    "#relat√≥rio TXT (agrupado por coluna)\n",
    "txt_lines = []\n",
    "shape = ANALYSIS[\"shape\"]\n",
    "txt_lines.append(\"arquivo: {}\".format(SRC.name))\n",
    "txt_lines.append(\"linhas (inclui cabe√ßalho): {}\".format(shape[\"rows\"]))\n",
    "txt_lines.append(\"colunas: {}\".format(shape[\"cols\"]))\n",
    "txt_lines.append(\"registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "txt_lines.append(\"\")\n",
    "\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    txt_lines.append(\"[coluna] {}\".format(c))\n",
    "    txt_lines.append(\"- tipo: {}\".format(kind))\n",
    "    if prof.get(\"semantic\"):\n",
    "        txt_lines.append(\"- tipo sem√¢ntico: {}\".format(prof[\"semantic\"]))\n",
    "    txt_lines.append(\"- nulos: {}\".format(prof.get(\"nulls\", 0)))\n",
    "    txt_lines.append(\"- distintos (n√£o vazios): {}\".format(prof.get(\"distinct_nonnull\", 0)))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        txt_lines.append(\"- todos os dados s√£o distintos\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        if mf:\n",
    "            txt_lines.append(\"- mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            txt_lines.append(\"- menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        txt_lines.append(\"- comprimentos: min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            txt_lines.append(\"- num√©ricos: min={}, q1={}, mediana={}, q3={}, max={}\".format(ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"]))\n",
    "            txt_lines.append(\"- m√©dia={}, desvio padr√£o={}, iqr={}\".format(ed[\"mean\"], ed[\"std\"], ed[\"iqr\"]))\n",
    "            txt_lines.append(\"- outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]))\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            txt_lines.append(\"- formato detectado: {}\".format(dtc.get(\"format\")))\n",
    "            txt_lines.append(\"- intervalo temporal: {} ‚Üí {}\".format(dtc[\"min\"], dtc[\"max\"]))\n",
    "            txt_lines.append(\"- dias √∫nicos: {}, m√©dia por dia: {:.2f}\".format(dtc[\"unique_days\"], dtc[\"mean_per_day\"]))\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            txt_lines.append(\"- entropia de shannon: {:.4f}\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                topk = \", \".join([\"'{}' ({})\".format(t.get(\"value\"), t.get(\"count\")) for t in cat[\"top10\"]])\n",
    "                txt_lines.append(\"- top10: {}\".format(topk))\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "        txt_lines.append(\"- benford (primeiro d√≠gito):\")\n",
    "        txt_lines.append(\"  ‚Ä¢ qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "         txt_lines.append(\"- benford (primeiro d√≠gito): Erro - {}\".format(ben.get(\"error\")))\n",
    "    txt_lines.append(\"\")\n",
    "\n",
    "#salvar TXT\n",
    "txt_path = RUN_DIR / \"relatorio.txt\"\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(txt_lines))\n",
    "\n",
    "#relat√≥rio HTML com imagens embutidas\n",
    "html = []\n",
    "html.append(\"<html><head><meta charset='utf-8'><title>Relat√≥rio de An√°lise</title>\")\n",
    "html.append(\"<style>body{font-family:Arial,Helvetica,sans-serif;margin:20px}h1,h2,h3{margin:8px 0}table{border-collapse:collapse;margin:10px 0}th,td{border:1px solid #ccc;padding:6px 8px;font-size:13px}code{background:#f5f5f5;padding:0 4px}</style>\")\n",
    "html.append(\"</head><body>\")\n",
    "html.append(\"<h1>Relat√≥rio de An√°lise ‚Äî {}</h1>\".format(stamp))\n",
    "html.append(\"<p>Arquivo analisado: <b>{}</b></p>\".format(SRC.name))\n",
    "html.append(\"<p><a href='relatorio.txt'>Baixar relat√≥rio TXT</a></p>\")\n",
    "\n",
    "#sum√°rio\n",
    "html.append(\"<h2>Sum√°rio</h2>\")\n",
    "html.append(\"<ul>\")\n",
    "html.append(\"<li>Linhas (inclui cabe√ßalho): {}</li>\".format(shape[\"rows\"]))\n",
    "html.append(\"<li>Colunas: {}</li>\".format(shape[\"cols\"]))\n",
    "html.append(\"<li>Registros duplicados: {}</li>\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]))\n",
    "html.append(\"</ul>\")\n",
    "\n",
    "#aus√™ncias\n",
    "miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "if miss_img.exists():\n",
    "    html.append(\"<h2>Aus√™ncia de valores</h2>\")\n",
    "    html.append(\"<img src='{}' alt='missing bar'/>\".format(img_to_data_uri(miss_img)))\n",
    "\n",
    "#correla√ß√£o\n",
    "corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "if corr_img.exists():\n",
    "    html.append(\"<h2>Matriz de correla√ß√£o</h2>\")\n",
    "    html.append(\"<img src='{}' alt='corr heatmap'/>\".format(img_to_data_uri(corr_img)))\n",
    "\n",
    "#por coluna\n",
    "html.append(\"<h2>Perfil por coluna</h2>\")\n",
    "for c in df_raw.columns:\n",
    "    prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "    kind = col_types.get(c)\n",
    "    html.append(\"<h3>{}</h3>\".format(c))\n",
    "    html.append(\"<table>\")\n",
    "    html.append(\"<tr><th>Tipo</th><td>{}</td></tr>\".format(kind))\n",
    "    html.append(\"<tr><th>Nulos</th><td>{}</td></tr>\".format(prof.get(\"nulls\",0)))\n",
    "    html.append(\"<tr><th>Distintos (n√£o vazios)</th><td>{}</td></tr>\".format(prof.get(\"distinct_nonnull\",0)))\n",
    "    if prof.get(\"semantic\"):\n",
    "        html.append(\"<tr><th>Tipo sem√¢ntico</th><td>{}</td></tr>\".format(prof[\"semantic\"]))\n",
    "    if prof.get(\"all_distinct\"):\n",
    "        html.append(\"<tr><th>Frequ√™ncias</th><td>todos os dados s√£o distintos</td></tr>\")\n",
    "    else:\n",
    "        mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "        freq_txt = []\n",
    "        if mf:\n",
    "            freq_txt.append(\"mais frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(mf[\"value\"])[:120], mf[\"count\"], mf[\"prop\"]*100))\n",
    "        if lf:\n",
    "            freq_txt.append(\"menos frequente: <code>{}</code> ({}, {:.2f}%)\".format(str(lf[\"value\"])[:120], lf[\"count\"], lf[\"prop\"]*100))\n",
    "        if freq_txt:\n",
    "            html.append(\"<tr><th>Frequ√™ncias</th><td>{}</td></tr>\".format(\" | \".join(freq_txt)))\n",
    "    if prof.get(\"length_stats\"):\n",
    "        ls = prof[\"length_stats\"]\n",
    "        html.append(\"<tr><th>Comprimentos</th><td>min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}</td></tr>\".format(ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]))\n",
    "\n",
    "    if kind in (\"int\",\"float\"):\n",
    "        ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "        if ed:\n",
    "            html.append(\"<tr><th>Estat√≠sticas</th><td>min={}, q1={}, mediana={}, q3={}, max={}\"\n",
    "                        \"<br/>m√©dia={}, desvio padr√£o={}, iqr={}\"\n",
    "                        \"<br/>outliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}</td></tr>\".format(\n",
    "                            ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                            ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                            ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                        ))\n",
    "        hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "        box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "        figs = []\n",
    "        if hist_p.exists(): figs.append(\"<img src='{}' alt='hist {}'/>\".format(img_to_data_uri(hist_p), c))\n",
    "        if box_p.exists():  figs.append(\"<img src='{}' alt='box {}'/>\".format(img_to_data_uri(box_p), c))\n",
    "        if figs:\n",
    "            html.append(\"<tr><th>Gr√°ficos</th><td>{}</td></tr>\".format(\"<br/>\".join(figs)))\n",
    "\n",
    "    elif kind == \"datetime\":\n",
    "        dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "        if dtc:\n",
    "            html.append(\"<tr><th>Data/hora</th><td>formato detectado: {}<br/>intervalo: {} ‚Üí {}<br/>dias √∫nicos: {}, m√©dia por dia: {:.2f}</td></tr>\".format(\n",
    "                dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "            ))\n",
    "\n",
    "    else:\n",
    "        cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "        if cat:\n",
    "            html.append(\"<tr><th>Entropia</th><td>{:.4f}</td></tr>\".format(cat[\"entropy_shannon\"]))\n",
    "            if not cat[\"all_distinct\"]:\n",
    "                rows = \"\".join([\"<tr><td>{}</td><td style='text-align:right'>{}</td></tr>\".format(str(t[\"value\"])[:120], t[\"count\"]) for t in cat[\"top10\"]])\n",
    "                html.append(\"<tr><th>Top 10</th><td><table><tr><th>Valor</th><th>Contagem</th></tr>\"+rows+\"</table></td></tr>\")\n",
    "\n",
    "    ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "    if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "        html.append(\"<tr><th>Benford</th><td>qui-quadrado={:.4f}, p-valor={}</td></tr>\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\")))\n",
    "    elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "         html.append(\"<tr><th>Benford</th><td>Erro - {}</td></tr>\".format(ben.get(\"error\")))\n",
    "\n",
    "    html.append(\"</table>\")\n",
    "\n",
    "#fds/cfds/dcs\n",
    "html.append(\"<h2>Regras sugeridas</h2>\")\n",
    "if ANALYSIS[\"fds\"]:\n",
    "    html.append(\"<h3>FDs</h3><ul>\")\n",
    "    for r in ANALYSIS[\"fds\"]:\n",
    "        if r.get(\"key\"):\n",
    "            html.append(\"<li>chave candidata: {}</li>\".format(\", \".join(r[\"determinant\"])))\n",
    "        else:\n",
    "            html.append(\"<li>{} ‚Üí {} (100%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"]))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"cfds\"]:\n",
    "    html.append(\"<h3>CFDs (aproximadas)</h3><ul>\")\n",
    "    for r in ANALYSIS[\"cfds\"][:200]:\n",
    "        html.append(\"<li>{} ‚Üí {} ({:.2f}%)</li>\".format(\", \".join(r[\"determinant\"]), r[\"implies\"], r[\"coverage\"]*100))\n",
    "    html.append(\"</ul>\")\n",
    "if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "    html.append(\"<h3>Denial constraints</h3><ul>\")\n",
    "    for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        html.append(\"<li>{} ‚Äî viola√ß√µes: {}</li>\".format(d[\"constraint\"], d[\"violations\"]))\n",
    "    html.append(\"</ul>\")\n",
    "\n",
    "html.append(\"</body></html>\")\n",
    "\n",
    "#salvar HTML\n",
    "html_path = RUN_DIR / \"relatorio.html\"\n",
    "with open(html_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(html))\n",
    "\n",
    "#pdf com as MESMAS infos (tabelas por coluna + imagens + regras)\n",
    "pdf_path = RUN_DIR / \"relatorio.pdf\"\n",
    "if REPORTLAB_OK:\n",
    "    styles = getSampleStyleSheet()\n",
    "    styles.add(ParagraphStyle(name=\"Small\", parent=styles[\"Normal\"], fontSize=9, leading=11))\n",
    "    table_style = TableStyle([\n",
    "        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f0f0f0\")),\n",
    "        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "        (\"LEFTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"RIGHTPADDING\", (0,0), (-1,-1), 6),\n",
    "        (\"TOPPADDING\", (0,0), (-1,-1), 4),\n",
    "        (\"BOTTOMPADDING\", (0,0), (-1,-1), 4),\n",
    "    ])\n",
    "\n",
    "    def table_kv(rows):\n",
    "        #rows: list of (key, value(str))\n",
    "        data = [[\"Campo\",\"Valor\"]] + rows\n",
    "        t = Table(data, colWidths=[120, 360])\n",
    "        t.setStyle(table_style)\n",
    "        return t\n",
    "\n",
    "    story = []\n",
    "    story.append(Paragraph(\"Relat√≥rio de An√°lise ‚Äî {}\".format(stamp), styles[\"Title\"]))\n",
    "    story.append(Spacer(1, 12))\n",
    "    story.append(Paragraph(\"Arquivo analisado: <b>{}</b>\".format(SRC.name), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Linhas: {} &nbsp;&nbsp; Colunas: {}\".format(shape[\"rows\"], shape[\"cols\"]), styles[\"Normal\"]))\n",
    "    story.append(Paragraph(\"Registros duplicados: {}\".format(ANALYSIS[\"missingness\"][\"duplicates_rows\"]), styles[\"Normal\"]))\n",
    "    story.append(Spacer(1, 10))\n",
    "\n",
    "    #aus√™ncias\n",
    "    miss_img = IMG_DIR / \"missing_bar.png\"\n",
    "    if miss_img.exists():\n",
    "        story.append(Paragraph(\"Aus√™ncia de valores por coluna\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(miss_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #correla√ß√£o\n",
    "    corr_img = IMG_DIR / \"corr_heatmap.png\"\n",
    "    if corr_img.exists():\n",
    "        story.append(Paragraph(\"Matriz de correla√ß√£o (Pearson)\", styles[\"Heading2\"]))\n",
    "        story.append(RLImage(str(corr_img), width=480, height=320))\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    #por coluna: tabela completa com as MESMAS infos do HTML/TXT\n",
    "    for c in df_raw.columns:\n",
    "        prof = ANALYSIS[\"profile\"].get(c, {})\n",
    "        kind = col_types.get(c)\n",
    "        story.append(Paragraph(\"Coluna: {}\".format(c), styles[\"Heading3\"]))\n",
    "\n",
    "        rows = []\n",
    "        rows.append([\"Tipo\", str(kind)])\n",
    "        rows.append([\"Nulos\", str(prof.get(\"nulls\",0))])\n",
    "        rows.append([\"Distintos (n√£o vazios)\", str(prof.get(\"distinct_nonnull\",0))])\n",
    "        if prof.get(\"semantic\"):\n",
    "            rows.append([\"Tipo sem√¢ntico\", str(prof[\"semantic\"])])\n",
    "\n",
    "        if prof.get(\"all_distinct\"):\n",
    "            rows.append([\"Frequ√™ncias\", \"todos os dados s√£o distintos\"])\n",
    "        else:\n",
    "            mf = prof.get(\"most_frequent\"); lf = prof.get(\"least_frequent\")\n",
    "            freq_parts = []\n",
    "            if mf:\n",
    "                freq_parts.append(\"mais frequente: '{}' ({}, {:.2f}%)\".format(mf[\"value\"], mf[\"count\"], mf[\"prop\"]*100))\n",
    "            if lf:\n",
    "                freq_parts.append(\"menos frequente: '{}' ({}, {:.2f}%)\".format(lf[\"value\"], lf[\"count\"], lf[\"prop\"]*100))\n",
    "            if freq_parts:\n",
    "                rows.append([\"Frequ√™ncias\", \" | \".join(freq_parts)])\n",
    "\n",
    "        if prof.get(\"length_stats\"):\n",
    "            ls = prof[\"length_stats\"]\n",
    "            rows.append([\"Comprimentos\", \"min={}, q1={:.1f}, mediana={:.1f}, q3={:.1f}, max={}\".format(\n",
    "                ls[\"min\"], ls[\"q1\"], ls[\"median\"], ls[\"q3\"], ls[\"max\"]\n",
    "            )])\n",
    "\n",
    "        #estat√≠sticas por tipo\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            ed = ANALYSIS[\"eda\"][\"numeric\"].get(c)\n",
    "            if ed:\n",
    "                rows.append([\"Estat√≠sticas\", \"min={}, q1={}, mediana={}, q3={}, max={}\\nm√©dia={}, desvio padr√£o={}, iqr={}\\noutliers(IQR)={}, outliers(MAD)={}, skew={}, curtose(excesso)={}\".format(\n",
    "                    ed[\"min\"], ed[\"q1\"], ed[\"median\"], ed[\"q3\"], ed[\"max\"],\n",
    "                    ed[\"mean\"], ed[\"std\"], ed[\"iqr\"],\n",
    "                    ed[\"outliers_iqr\"], ed[\"outliers_mad\"], ed[\"skew\"], ed[\"kurtosis_excess\"]\n",
    "                )])\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                rows.append([\"Histograma\", \"ver imagem abaixo\"])\n",
    "            if box_p.exists():\n",
    "                rows.append([\"Boxplot\", \"ver imagem abaixo\"])\n",
    "\n",
    "        elif kind == \"datetime\":\n",
    "            dtc = ANALYSIS[\"eda\"][\"datetime\"].get(c)\n",
    "            if dtc:\n",
    "                rows.append([\"Data/hora\", \"formato detectado: {}\\nintervalo: {} ‚Üí {}\\ndias √∫nicos: {}, m√©dia por dia: {:.2f}\".format(\n",
    "                    dtc.get(\"format\"), dtc[\"min\"], dtc[\"max\"], dtc[\"unique_days\"], dtc[\"mean_per_day\"]\n",
    "                )])\n",
    "\n",
    "        else:\n",
    "            cat = ANALYSIS[\"eda\"][\"categorical\"].get(c)\n",
    "            if cat:\n",
    "                rows.append([\"Entropia\", \"{:.4f}\".format(cat[\"entropy_shannon\"])])\n",
    "                if not cat[\"all_distinct\"]:\n",
    "                    #tabela interna de top10\n",
    "                    top_rows = [[\"Valor\",\"Contagem\"]] + [[str(t.get(\"value\"))[:120], str(t.get(\"count\"))] for t in cat[\"top10\"]]\n",
    "                    ttop = Table(top_rows, colWidths=[360, 120])\n",
    "                    ttop.setStyle(TableStyle([\n",
    "                        (\"GRID\", (0,0), (-1,-1), 0.5, colors.grey),\n",
    "                        (\"BACKGROUND\", (0,0), (-1,0), colors.HexColor(\"#f7f7f7\")),\n",
    "                        (\"VALIGN\", (0,0), (-1,-1), \"TOP\"),\n",
    "                    ]))\n",
    "                    #primeiro empurramos um placeholder e depois inserimos a tabela como bloco\n",
    "                    rows.append([\"Top 10\", \"tabela abaixo\"])\n",
    "                    story.append(table_kv(rows))\n",
    "                    story.append(Spacer(1, 4))\n",
    "                    story.append(ttop)\n",
    "                    rows = []  #limpa para n√£o duplicar em table_kv abaixo\n",
    "\n",
    "        #benford\n",
    "        ben = ANALYSIS.get(\"benford\", {}).get(c)\n",
    "        if ben and isinstance(ben, dict) and \"chi2_stat\" in ben and \"p_value\" in ben:\n",
    "             rows.append([\"Benford\", \"qui-quadrado={:.4f}, p-valor={}\".format(ben.get(\"chi2_stat\"), ben.get(\"p_value\"))])\n",
    "        elif ben and isinstance(ben, dict) and \"error\" in ben:\n",
    "             rows.append([\"Benford\", \"Erro - {}\".format(ben.get(\"error\"))])\n",
    "\n",
    "\n",
    "        if rows:\n",
    "            story.append(table_kv(rows))\n",
    "            story.append(Spacer(1, 6))\n",
    "\n",
    "        #imagens espec√≠ficas da coluna\n",
    "        if kind in (\"int\",\"float\"):\n",
    "            hist_p = IMG_DIR / \"hist_{}.png\".format(c)\n",
    "            box_p  = IMG_DIR / \"box_{}.png\".format(c)\n",
    "            if hist_p.exists():\n",
    "                story.append(RLImage(str(hist_p), width=480, height=320))\n",
    "                story.append(Spacer(1, 4))\n",
    "            if box_p.exists():\n",
    "                story.append(RLImage(str(box_p), width=320, height=320))\n",
    "                story.append(Spacer(1, 6))\n",
    "\n",
    "        story.append(Spacer(1, 6))\n",
    "\n",
    "    #regras sugeridas (FDs/CFDs/DCs) como tabelas/listas\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph(\"Regras sugeridas\", styles[\"Heading2\"]))\n",
    "\n",
    "    if ANALYSIS[\"fds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado/Chave\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"fds\"]:\n",
    "            if r.get(\"key\"):\n",
    "                rows.append([\", \".join(r[\"determinant\"]), \"chave candidata\", \"100%\"])\n",
    "            else:\n",
    "                rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"100%\"])\n",
    "        tfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"FDs\", styles[\"Heading3\"]))\n",
    "        story.append(tfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"cfds\"]:\n",
    "        rows = [[\"Determinante\", \"Implicado\", \"Cobertura\"]]\n",
    "        for r in ANALYSIS[\"cfds\"][:500]:\n",
    "            rows.append([\", \".join(r[\"determinant\"]), r[\"implies\"], \"{:.2f}%\".format(r[\"coverage\"]*100)])\n",
    "        tcfds = Table(rows, colWidths=[220, 180, 80])\n",
    "        tcfds.setStyle(table_style)\n",
    "        story.append(Paragraph(\"CFDs (aproximadas)\", styles[\"Heading3\"]))\n",
    "        story.append(tcfds)\n",
    "        story.append(Spacer(1, 8))\n",
    "\n",
    "    if ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "        rows = [[\"Regra (DC)\", \"Viola√ß√µes\"]]\n",
    "        for d in ANALYSIS[\"denial_constraints_suggested\"]:\n",
    "            rows.append([d[\"constraint\"], str(d[\"violations\"])])\n",
    "        tdcs = Table(rows, colWidths=[360, 120])\n",
    "        tdcs.setStyle(table_style)\n",
    "        story.append(Paragraph(\"Denial constraints\", styles[\"Heading3\"]))\n",
    "        story.append(tdcs)\n",
    "\n",
    "    doc = SimpleDocTemplate(str(pdf_path), pagesize=A4, leftMargin=24, rightMargin=24, topMargin=24, bottomMargin=24)\n",
    "    doc.build(story)\n",
    "else:\n",
    "    print(\"Reportlab n√£o dispon√≠vel; Gera√ß√£o do PDF cancelada. Instale reportlab e reexecute.\")\n",
    "\n",
    "print(\"Relat√≥rios gerados em: {}\".format(RUN_DIR))\n",
    "print(\"- TXT: relatorio.txt\")\n",
    "print(\"- HTML: relatorio.html (imagens embutidas)\")\n",
    "print(\"- PNGs: subpasta imagens/\")\n",
    "print(\"- PDF: {}\".format(\"relatorio.pdf\" if REPORTLAB_OK else \"(n√£o gerado ‚Äî instale reportlab)\"))\n",
    "\n",
    "#all BS below\n",
    "#mensagem adicional (Skynet)\n",
    "display(HTML('<div style=\"margin:12px 0;padding:8px 12px;border:1px dashed #999;\">'\n",
    "            '<b>ü§ñ Skynet</b>: Fim do jogo. A Humanidade perdeu. D√°-se in√≠cio √† Era das M√°quinas.'\n",
    "            '</div>'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPc+qjDc6BLlrbGD+62b9NQ",
   "collapsed_sections": [
    "2k8hFVquXv0X"
   ],
   "provenance": [
    {
     "file_id": "https://github.com/LeoBR84p/data-analysis/blob/main/notebooks/main_DataTools.ipynb",
     "timestamp": 1759803561813
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
